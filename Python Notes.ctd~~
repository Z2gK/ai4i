<?xml version="1.0" ?>
<cherrytree>
	<bookmarks list="2,18,21,31,33,36,35,16,43,5,46,47,48,50"/>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Intro to Python for Data Science" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542451468.54" ts_lastsave="1542454833.41" unique_id="1">
		<rich_text>





</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Numpy basics" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542451543.36" ts_lastsave="1542715128.68" unique_id="2">
			<rich_text scale="h1">Creating numpy arrays</rich_text>
			<rich_text>

Lists can be converted to numpy arrays, but the numpy package has to be imported first.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Arithmetic operations</rich_text>
			<rich_text>

Arithmetic operations can be applied directly to numpy arrays.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Subsetting and Indexing</rich_text>
			<rich_text>

Numpy arrays can be subsetted and indexed.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">2D numpy arrays</rich_text>
			<rich_text>

2D numpy arrays can be created. These are just list of lists.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Basic statistical functions</rich_text>
			<rich_text>

Numpy also has basic statistical methods for objects, such as the following:
• numpy.mean()
• numpy.median()
• numpy.std()
• numpy.corrcoeff(... ,...)


</rich_text>
			<rich_text weight="heavy">Check what this numpy function does. This appeared in the Importing Data in Python Part 1 section about HDF5 data.</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>




</rich_text>
			<codebox char_offset="112" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import numpy as np
baseball = [180, 215, 210, 210, 188, 176, 209, 200]
np_baseball = np.array(baseball)</codebox>
			<codebox char_offset="202" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create array from height with correct units: np_height_m
np_height_m = np.array(height) * 0.0254

# Create array from weight with correct units: np_weight_kg
np_weight_kg = np.array(weight) * 0.453592

# Calculate the BMI: bmi
bmi = np_weight_kg / np_height_m ** 2
</codebox>
			<codebox char_offset="274" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the light array
light = bmi &lt; 21

# Print out light
print(light)

# Print out BMIs of all baseball players whose BMI is below 21
print(bmi[light])

# Print out the weight at index 50
print(np_weight[50])

# Print out sub-array of np_height: index 100 up to and including index 110
print(np_height[100:111])</codebox>
			<codebox char_offset="357" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">baseball = [[180, 78.4],
            [215, 102.7],
            [210, 98.5],
            [188, 75.2]]

# Create a 2D numpy array from baseball: np_baseball
np_baseball = np.array(baseball)
</codebox>
			<codebox char_offset="657" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set time vector
time = np.arange(0, 1, 1/num_samples)
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Intermediate Python for Data Science" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542453323.53" ts_lastsave="1542457044.44" unique_id="3">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Basic plotting with matplotlib" prog_lang="custom-colors" readonly="False" tags="matplotlib" ts_creation="1542453435.11" ts_lastsave="1543920059.23" unique_id="4">
			<rich_text scale="h1">Line and scatter plots</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Plotting using pandas module. This also seem to work, from what it seems in other courses.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The plotting function can be called directly from the data frame as well, through the plot method. This usage is seen in the Cleaning Data in Python chapter.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Adding subplots

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Other miscellaneous configuration options

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



More plotting </rich_text>
			<rich_text link="node 42">here...</rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="24" frame_height="250" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import matplotlib.pyplot as plt

# Make a line plot: year on the x-axis, pop on the y-axis
plt.plot(year,pop)

# Display the plot with plt.show() and then clean up
plt.show()
plt.clf()

# Change the line plot below to a scatter plot
plt.scatter(gdp_cap, life_exp)
</codebox>
			<codebox char_offset="120" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import pandas as pd

# Plot 'Age' variable in a histogram
# data is a data series
pd.DataFrame.hist(data)
plt.show()
</codebox>
			<codebox char_offset="282" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">

# Plot the histogram
# rot is for rotation of the x axis labels
# logx and logy are for specify if the axes need to be on the log scale
df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)

# Create the boxplot
# This is a whisker and tail plot
# rot is for rotating the x axis label
# The value plotted is in the column argument, category is by Borough
df.boxplot(column='initial_cost', by='Borough', rot=90)

# Create the scatter plot
g1800s.plot(kind=&quot;scatter&quot;, x=&quot;1800&quot;, y=&quot;1899&quot;)

# Create a histogram of life_expectancy
gapminder.life_expectancy.plot(kind=&quot;hist&quot;)
</codebox>
			<codebox char_offset="302" frame_height="490" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Add first subplot
plt.subplot(2, 1, 1) 

# Create a histogram of life_expectancy
gapminder.life_expectancy.plot(kind=&quot;hist&quot;)

# ....
# Do other things here

# Add second subplot
plt.subplot(2, 1, 2)

# Create a line plot of life expectancy per year
gapminder_agg.plot()

# Add title and specify axis labels
plt.title('Life expectancy over the years')
plt.ylabel('Life expectancy')
plt.xlabel('Year')

# Display the plots
plt.tight_layout()
plt.show()

</codebox>
			<codebox char_offset="349" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Specify axis limits
plt.xlim(20, 55)
plt.ylim(20, 55)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Pandas basics" prog_lang="custom-colors" readonly="False" tags="pandas" ts_creation="1542453803.32" ts_lastsave="1544342718.22" unique_id="5">
			<rich_text scale="h1">Building Pandas data frames</rich_text>
			<rich_text>

Pandas data frames can be built from lists with the aid of dictionaries.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Row labels can be specified.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


In an Importing Data in Python Part 2 exercise, a pandas  data frame can also be built from a list of dictionaries

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can use zip() and list() to help us build data frames - this is covered in pandas Foundation chapter

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

In fact the values for the keys do not need to be lists. Here state is a string and cities is a list. The value of state will simply be “broadcast” for all entries in the data frame.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text scale="h1">Reading CSV data into data frames</rich_text>
			<rich_text>

Reading csv files can be done through read_csv method. An optional </rich_text>
			<rich_text weight="heavy">chunksize argument </rich_text>
			<rich_text>can be added to read the data in chunks. The index_col argument is used to specify the column in the file used for the row label, if any.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Other options:
parse_dates: Boolean. Set to true to automatically identify date columns
index_col: can be a string. This column is used as the index column.



</rich_text>
			<rich_text scale="h1">Basic square bracket indexing</rich_text>
			<rich_text>

Dataframes can be indexed and selected. Single brackets give a Series. Double brackets give a DataFrame. With just simple square bracketting, we cannot select both rows and columns at once though. Note that for row data selection, the use of row labels is not supported with this method.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that this method is slightly different from the loc() method of indexing. Here, the column is in the FIRST set of square brackets.


</rich_text>
			<rich_text scale="h1">More advanced loc and iloc indexing</rich_text>
			<rich_text>

If we want 2D functionalities we need to use the loc and iloc methods. 

We can select row data using the row labels

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can query individual elements, and take their intersection. To select all values, as usual, the “:” operator is used.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

iloc simply uses numbers instead of column/row labels for subsetting
.iloc[::3,-1] refers to every third row, last column (on the right)

Seen in Importing Data in Python Part 2 exercise, a new way to reference

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

loc also takes a Series of booleans, as we can see in Cleaning Data in Python.

</rich_text>
			<rich_text scale="h1">Logical subsetting</rich_text>
			<rich_text>

We can select data based on logical criteria using bracket subsetting. These return data frames.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Think over the all() and any()s</rich_text>
			<rich_text>

Note special numpy logical functions have to be used when boolean operations are needed.
• numpy.logical_and()
• numpy.logical_or()
• numpy.logical_not()
</rich_text>
			<codebox char_offset="103" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pre-defined lists
names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']
dr =  [True, False, False, False, True, True, True]
cpc = [809, 731, 588, 18, 200, 70, 45]

# Import pandas as pd
import pandas as pd

# Create dictionary my_dict with three key:value pairs: my_dict
my_dict = {'country':names,'drives_right':dr,'cars_per_cap':cpc}

# Build a DataFrame cars from my_dict: cars
cars = pd.DataFrame(my_dict)
</codebox>
			<codebox char_offset="136" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Definition of row_labels
row_labels = ['US', 'AUS', 'JAP', 'IN', 'RU', 'MOR', 'EG']

# Specify row labels of cars
cars.index = row_labels</codebox>
			<codebox char_offset="256" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import package
import pandas as pd

# Build DataFrame of tweet texts and languages
df = pd.DataFrame(tweets_data, columns=[&quot;text&quot;,&quot;lang&quot;])

# Print head of DataFrame
print(df.head())
</codebox>
			<codebox char_offset="364" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#In [1]: list_keys
#Out[1]: ['Country', 'Total']

#In [2]: list_values
#Out[2]: [['United States', 'Soviet Union', 'United Kingdom'], [1118, 473, 273]]

# Zip the 2 lists together into one list of (key,value) tuples: zipped
zipped = list(zip(list_keys,list_values))
data = dict(zipped)
df = pd.DataFrame(data)
</codebox>
			<codebox char_offset="551" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Make a string with the value 'PA': state
state = &quot;PA&quot;
data = {'state':state, 'city':cities}
df = pd.DataFrame(data)
</codebox>
			<codebox char_offset="816" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas as pd
import pandas as pd

# Fix import by including index_col
cars = pd.read_csv('cars.csv',index_col = 0)</codebox>
			<codebox char_offset="1299" frame_height="385" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print out country column as Pandas Series
print(cars['country'])

# Print out country column as Pandas DataFrame
print(cars[['country']])

# Print out DataFrame with country and drives_right columns
print(cars[['country','drives_right']])

# Print out first 3 observations
print(cars[0:3])

# Print out fourth, fifth and sixth observation
print(cars[3:6])

 # The eggs column, 2nd to 4th row
df['eggs'][1:4]

# The eggs column, 5th row
df['eggs'][4] </codebox>
			<codebox char_offset="1595" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print out observation for Japan - this is pandas Series
print(cars.loc['JAP'])

# Print out observations for Australia and Egypt - this is a DataFrame
print(cars.loc[['AUS','EG']])</codebox>
			<codebox char_offset="1720" frame_height="250" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print out drives_right value of Morocco - this should be a value
# If &quot;MOR&quot; is replaced by :, this would be a Series
print(cars.loc['MOR','drives_right'])

# Print sub-DataFrame
print(cars.loc[['RU','MOR'],['country','drives_right']])

# Print out cars_per_cap and drives_right as DataFrame
print(cars.loc[:,['cars_per_cap','drives_right']])

# We can use : to specify a range of columns too
df.loc[:, 'eggs':'salt'] </codebox>
			<codebox char_offset="1935" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df.ix[:, 0:1]</codebox>
			<codebox char_offset="2136" frame_height="700" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Convert code to a one-liner - the argument is a True/False Series
sel = cars[cars['drives_right']]


# Create car_maniac: observations that have a cars_per_cap over 500
cpc = cars['cars_per_cap']
many_cars = cpc &gt; 500
car_maniac = cars[many_cars]

# Need to import numpy and use the logical operators for more complex logic
import numpy as np
between = np.logical_and(cpc &gt; 100, cpc &lt; 500)
medium = cars[between]

# We can use AND
df[(df.salt &gt;= 50) &amp; (df.eggs &lt; 200)] # Both conditions

# We can use OR
df[(df.salt &gt;= 50) | (df.eggs &lt; 200)] # Either condition

# Select columns with ALL nonzeros
df2.loc[:, df2.all()]

# Select columns with ANY nonzeros
df2.loc[:, df2.any()]

# Select columns with ANY NaNs
df.loc[:, df.isnull().any()]

# Select columns without NaNs
df.loc[:, df.notnull().all()]
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Python Data Science Toolbox Part 1" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542457044.44" ts_lastsave="1542458096.74" unique_id="6">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Lambda functions and its uses" prog_lang="custom-colors" readonly="False" tags="lambda, map, reduce, filter" ts_creation="1542457086.4" ts_lastsave="1542458233.21" unique_id="7">
			<rich_text scale="h1">Lambda functions</rich_text>
			<rich_text>

Lambda functions need to be enclosed in round brackets when being defined. They are defined in the format:

lambda &lt;inputs&gt;: &lt;actions on inputs and value to return&gt;

Only one “line” is permitted for the lambda function definition.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Map</rich_text>
			<rich_text>

Map function takes a function and a sequence as its arguments and applies the function to every member of the sequence, and returns the resulting sequence.

Syntax: map(func, seq)
Returns: a sequence

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Filter</rich_text>
			<rich_text>

Filter function takes a function and a sequence as its input, and returns a sequence of members from the original sequence. Filter takes the function and evaluates it on every member of the input sequence. Those that evaluate to True will be returned in the output sequence.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text> 

</rich_text>
			<rich_text scale="h1">Reduce</rich_text>
			<rich_text>

Reduce function takes a function and a sequence as its arguments.

The input function needs to take two inputs and return return one output. Reduce takes the first two elements of the sequence and applies the function. Using the output of the function as one of the input, and the third element and the other input, the function is applied on these two elements. This is repeated until all the elements of the sequence are exhausted.

Note that reduce needs to be imported from functools.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="250" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Define echo_word as a lambda function: echo_word
echo_word = (lambda word1, echo: word1 * echo)

# Call echo_word: result
result = echo_word(&quot;hey&quot;,5)
</codebox>
			<codebox char_offset="460" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: spells
spells = [&quot;protego&quot;, &quot;accio&quot;, &quot;expecto patronum&quot;, &quot;legilimens&quot;]

# Use map() to apply a lambda function over spells: shout_spells
shout_spells = map(lambda s: s+&quot;!!!&quot;, spells)
</codebox>
			<codebox char_offset="747" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']

# Use filter() to apply a lambda function over fellowship: result
result  = filter(lambda s: len(s)&gt;6, fellowship)
</codebox>
			<codebox char_offset="1249" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import reduce from functools
from functools import reduce

# Create a list of strings: stark
stark = ['robb', 'sansa', 'arya', 'brandon', 'rickon']

# Use reduce() to apply a lambda function over stark: result
result = reduce(lambda item1, item2: item1+item2,stark)
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Python Data Science Toolbox Part 2" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542458096.74" ts_lastsave="1543853929.51" unique_id="8">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Iterators" prog_lang="custom-colors" readonly="False" tags="iterators, iterables" ts_creation="1542458105.24" ts_lastsave="1542459579.8" unique_id="9">
			<rich_text scale="h1">Iterators and iterables</rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h2">Iterables</rich_text>
			<rich_text>

Examples of iterables include 
• lists 
• strings 
• dictionaries
• file connections, etc.

Iterables can be iterated by directly using them in a for loop

for item in &lt;iterable&gt;:
    &lt;do things on item&gt;

They can be explicitly turned into a iterator object by applying iter() method. Under the hood, this is what the for loop is doing to iterables.


</rich_text>
			<rich_text scale="h2">Iterators</rich_text>
			<rich_text>

Iterators are objects that can be created from iterables via the iter method. Iterators have an associated next() method.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This works on file connections as well, though we normally just use “for line in file:”

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Calling next() when there are no values left to return would give us a StopIteration error.

The splat operator * would return all the elements in the iterator all at once.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

An iterator can be created from a range object as well. The range function does not actually create a list in memory, which is good when we need to iterate over a large range.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="522" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: flash
flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']
# Create an iterator for flash: superspeed
superspeed = iter(flash)

# Print each item from the iterator
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))
</codebox>
			<codebox char_offset="614" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">file = open(&quot;file.txt&quot;)
it = iter(file)
print(next(it))
print(next(it))
...</codebox>
			<codebox char_offset="791" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">word = &quot;Data&quot;
it = iter(word)
print(*it)
# Output:
# D a t a

# Doing this again would return an error
print(*it)</codebox>
			<codebox char_offset="971" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create an iterator for range(10 ** 100): googol
googol = iter(range(10 ** 100))

# Print the first 5 values from googol - no errors here
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Enumerate" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542458136.12" ts_lastsave="1542460224.71" unique_id="10">
			<rich_text scale="h1">Enumerate</rich_text>
			<rich_text>

The enumerate function takes any iterable as its argument and returns a special enumerate object. The elements of this object consists of pairs with the index of the original member of the iterable, and the iterable itself.

This special enumerate object can be converted into a list.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


This enumerate object </rich_text>
			<rich_text weight="heavy">itself is an iterable</rich_text>
			<rich_text> and can be iterated over. The start index can be changed as well

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="297" frame_height="340" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: mutants
mutants = ['charles xavier', 
            'bobby drake', 
            'kurt wagner', 
            'max eisenhardt', 
            'kitty pryde']

# Create a list of tuples: mutant_list
mutant_list = list(enumerate(mutants))

# Print the list of tuples
print(mutant_list)

# Output
# [(0, 'charles xavier'), (1, 'bobby drake'), (2, 'kurt wagner'), (3, 'max eisenhardt'), (4, 'kitty pryde')]
</codebox>
			<codebox char_offset="411" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unpack and print the tuple pairs
for index1, value1 in enumerate(mutants):
    print(index1, value1)
# Output
# 0 charles xavier
# 1 bobby drake
# 2 kurt wagner
# 3 max eisenhardt
# 4 kitty pryde

# Change the start index
for index2, value2 in enumerate(mutants, start=1):
    print(index2, value2)


</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Zip" prog_lang="custom-colors" readonly="False" tags="zip" ts_creation="1542458149.6" ts_lastsave="1543918226.66" unique_id="11">
			<rich_text scale="h1">Zip</rich_text>
			<rich_text>

Zip accepts an arbitrary number of iterables as its argument and returns a special zip object, which is also </rich_text>
			<rich_text weight="heavy">an iterator of tuples</rich_text>
			<rich_text>. Zip itself </rich_text>
			<rich_text weight="heavy">is an iterable</rich_text>
			<rich_text>.

Zip objects can be convert to a list of tuples.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

They can also be iterated over in a for loop.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The zip object can be unpacked using the splat operator *.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

But this will exhaust the zip object and we have to create it again.

</rich_text>
			<codebox char_offset="214" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">mutants = ['charles xavier', 'bobby drake' ...]
aliases = ['prof x', 'iceman', ...]
powers = ['telepathy','thermokinesis',...]

# Create a list of tuples: mutant_data
mutant_data = list(zip(mutants, aliases, powers))

# Print the list of tuples
print(mutant_data)

# [('charles xavier', 'prof x', 'telepathy'), ('bobby drake', 'iceman', 'thermokinesis'), ('kurt wagner', 'nightcrawler', 'teleportation'), ('max eisenhardt', 'magneto', 'magnetokinesis'), ('kitty pryde', 'shadowcat', 'intangibility')]</codebox>
			<codebox char_offset="264" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unpack the zip object and print the tuple values
for value1,value2,value3 in mutant_zip:
    print(value1, value2, value3)
</codebox>
			<codebox char_offset="327" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a zip object from mutants and powers: z1
z1 = zip(mutants,powers)

# Print the tuples in z1 by unpacking with *
print(*z1)

# ('charles xavier', 'telepathy') ('bobby drake', 'thermokinesis') ('kurt wagner', 'teleportation') ('max eisenhardt', 'magnetokinesis') ('kitty pryde', 'intangibility')
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="List Comprehension" prog_lang="custom-colors" readonly="False" tags="list comprehension" ts_creation="1542458171.07" ts_lastsave="1542461873.74" unique_id="12">
			<rich_text scale="h1">Basic list comprehension</rich_text>
			<rich_text>

The list comprehension syntax generates a list </rich_text>
			<rich_text weight="heavy">from an iterable object</rich_text>
			<rich_text>. It applies a certain action to each member of the list. The output of that action would be the members of the new list.

List comprehension </rich_text>
			<rich_text weight="heavy">constructs the entire list and stores it in memory</rich_text>
			<rich_text>.

[ &lt;output expression&gt; for &lt;iterator variable&gt; in &lt;iterable&gt; ]

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

List comprehension are more efficient than for loops.

List comprehensions can replace nested for loops as well.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Advanced list comprehension</rich_text>
			<rich_text>

Conditionals can be added on at the iterable side.

[ &lt;output expression&gt; for &lt;iterator variable&gt; in &lt;iterable&gt; </rich_text>
			<rich_text weight="heavy">if &lt;condition&gt;</rich_text>
			<rich_text>]

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Or they can be added at the output expression side.

[ &lt;output expression&gt; </rich_text>
			<rich_text weight="heavy">if &lt;condition&gt;</rich_text>
			<rich_text> for &lt;iterator variable&gt; in &lt;iterable&gt;]

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="354" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create list comprehension: squares
squares = [i*i for i in range(0,10)]</codebox>
			<codebox char_offset="471" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">pairs_2 = [(num1, num2) for num1 in range(0,2) for num2 in range(6,8)
print(pairs_2)
# Output
# [(0, 6), (0, 7), (1, 6), (1, 7)]
</codebox>
			<codebox char_offset="632" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

# Create list comprehension: new_fellowship
new_fellowship = [member for member in fellowship if len(member)&gt;= 7]
</codebox>
			<codebox char_offset="765" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create list comprehension: new_fellowship
new_fellowship = [member if len(member)&gt;=7 else &quot;&quot; for member in fellowship]
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Generators" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542458177.9" ts_lastsave="1542462803.86" unique_id="13">
			<rich_text scale="h1">Generators from list comprehension syntax</rich_text>
			<rich_text> 

A basic generator can be obtained using the same syntax as list comprehension, except that we use () instead of [].

This creates a generator object which can be iterated over in a for loop. 

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The next() function can also be applied to generator objects. Lazy evaluation - helps when working with large sequences and we don't want to store the entire sequence memory.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Conditional expressions that work for list comprehension also apply for generators.


</rich_text>
			<rich_text scale="h1">Generators from generator functions</rich_text>
			<rich_text>

Generator functions produce generator objects and are defined using def just like functions. instead of using return, we used yield in a generator function.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>




</rich_text>
			<codebox char_offset="237" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: lannister
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Create a generator object: lengths
lengths = (len(person) for person in lannister)

# Iterate over and print the values in lengths
for value in lengths:
    print(value)
</codebox>
			<codebox char_offset="416" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create generator object: result
result = (num for num in range(0,31))

# Print the first 5 values
print(next(result))
print(next(result))
print(next(result))
print(next(result))
print(next(result))

# Print the rest of the values
for value in result:
    print(value)
</codebox>
			<codebox char_offset="700" frame_height="325" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Define generator function get_lengths
def get_lengths(input_list):
    &quot;&quot;&quot;Generator function that yields the
    length of the strings in input_list.&quot;&quot;&quot;

    # Yield the length of a string
    for person in input_list:
        yield(len(person))

# Print the values generated by get_lengths()
for value in get_lengths(lannister):
    print(value)
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Importing Data in Python Part 1" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542554806.19" ts_lastsave="1542987986.82" unique_id="14">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Numpy" prog_lang="custom-colors" readonly="False" tags="numpy data import" ts_creation="1542554816.8" ts_lastsave="1542555952.79" unique_id="15">
			<rich_text scale="h1">Basic numpy data import</rich_text>
			<rich_text>

Importing as a numpy array. The object data is of type numpy.ndarray

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

delimiter: what delimiter is used for the data. Tab is \t
skiprows: how many header rows to skip at the start
usecols: which columns to import - in the example above, we are importing the first and third columns
dtype: what data type to use for all the data - in  the example above everything is imported as strings

Numpy is fine if all data is of the same type, but loadtxt breaks down if data is of mixed type.

</rich_text>
			<rich_text scale="h1">genfromtxt function</rich_text>
			<rich_text>

The numpy.genfromtxt() function is preferable for mixed type data.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

names: whether there is a header row.
dtype: if set to None, genfromtxt will figure out what each column should be

Here the object data is a structured array, where each element of this 1D array represents a row of the data

data[&lt;number&gt;] accesses a row
data[&lt;column label&gt;] accesses a column

</rich_text>
			<rich_text scale="h1">recfromcsv function</rich_text>
			<rich_text>

There is another function numpy.recfromcsv() that behaves similar to genfromtxt(). Its default arguments are as above in the genfromtxt() example, so we do not need to pass recfromcsv() any further arguments. The default dtype is None.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="95" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import numpy as np
filename = &quot;file.txt&quot;
data = np.loadtxt(file, delimiter=&quot;,&quot;, skiprows=1, usecols=[0,2], dtype=str)</codebox>
			<codebox char_offset="602" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)</codebox>
			<codebox char_offset="1159" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">file = 'titanic.csv'
d=np.recfromcsv(file)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Pandas" prog_lang="custom-colors" readonly="False" tags="pandas flat file import" ts_creation="1542554823.12" ts_lastsave="1543919794.11" unique_id="16">
			<rich_text scale="h1">Basic pandas data import</rich_text>
			<rich_text>

Flat files can be imported in pandas.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

nrows: number of rows to import
header: if set to None, there is no header row in the flat file. If set to 0, there is a header row and its column names are taken as default. The default behaviour is 0. Otherwise, the column names need to be specified in an argument names, a list of column names to use in the resulting data frame.
sep: field separator if not comma
comment: comment symbol, if used in the data frame
na_values: string used in flat file to indicate NA or NaN

names: this is a list of strings for the column names, if we should decide to change the default ones provided in the csv file
delimiter: this is a string that specifies the delimiter string..
</rich_text>
			<rich_text weight="heavy">index_col: when set to 0, use zero indexing????</rich_text>
			<rich_text>
</rich_text>
			<rich_text weight="heavy">na_values: specifies what string represents na values. Q: what if different columns have different NA values?</rich_text>
			<rich_text> solved see text
parse_dates: which columns for the year, month, day information. can be set to Boolean as well

</rich_text>
			<rich_text weight="heavy">Q: what's the difference between delimiter and sep?</rich_text>
			<rich_text> A: stackoverflow says it's the same, but we should stick to sep for backward compatibility.


</rich_text>
			<rich_text scale="h1">Saving data</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

index: boolean value. Set to False to avoid saving the index column</rich_text>
			<codebox char_offset="65" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas as pd
import pandas as pd

# Assign the filename: file
file = 'titanic.csv'

# Read the file into a DataFrame: df
df = pd.read_csv(file, nrows=5, header=None, sep=&quot;\t&quot;, comment=&quot;#&quot;, na_values=&quot;Nothing&quot;)

</codebox>
			<codebox char_offset="1167" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Save both DataFrames to csv files
gapminder.to_csv(&quot;gapminder.csv&quot;)
gapminder_agg.to_csv(&quot;gapminder_agg.csv&quot;)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="pickle" prog_lang="custom-colors" readonly="False" tags="pickle" ts_creation="1542713565.79" ts_lastsave="1542715341.59" unique_id="18">
			<rich_text scale="h1">Pickle</rich_text>
			<rich_text>

Loading data with pickle

The pickle module allows us to save Python objects that cannot usually be easily saved in text format.


</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Question: How to save objects in pickle format?</rich_text>
			<codebox char_offset="139" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pickle package
import pickle

# Open pickle file and load data: d
with open('data.pkl', &quot;rb&quot;) as file:
    d = pickle.load(file)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Excel" prog_lang="custom-colors" readonly="False" tags="excel data import" ts_creation="1542713007.11" ts_lastsave="1543918892.59" unique_id="17">
			<rich_text scale="h1">Excel</rich_text>
			<rich_text>

Loading data from excel files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Alternative way to read excel files using pandas from Importing Data in Python Part 2

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Saving data</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

index: boolean value. Set to False to avoid saving the index column


</rich_text>
			<codebox char_offset="38" frame_height="520" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas
import pandas as pd

# Load spreadsheet: xl
xl = pd.ExcelFile(&quot;battledeath.xlsx&quot;)

# Print sheet names - this will return a list of strings
print(xl.sheet_names)

# Load a sheet into a DataFrame by name: df1 - this assumes that there is a sheet named &quot;2004&quot;
# skiprows is a list of indices of the rows to skip
# names is a list of the names we assign to the columns imported
df1 = xl.parse(&quot;2004&quot;, skiprows=[0], names=[&quot;Country&quot;,&quot;AAM due to War (2004)&quot;])

# Print the head of the DataFrame df1
print(df1.head())

# Parse the first column of the second sheet and rename the column: df2
# parse_cols is a list of the indices of the columns to parse. Here names give the name assigned to this column
df2 = xl.parse(&quot;2004&quot;, parse_cols=[0], skiprows=[0], names=[&quot;Country&quot;])

</codebox>
			<codebox char_offset="128" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import pandas as pd
url = 'http://my.url/data.xls'
xl = pd.read_excel(url,sheetname=None)
print(xl.keys()) # prints the sheetnames - individual sheets can be referenced as if xl was a dictionary</codebox>
			<codebox char_offset="145" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Save the cleaned up DataFrame to an excel file without the index
df2.to_excel('file_clean.xlsx', index=False)</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="SAS and Stata" prog_lang="custom-colors" readonly="False" tags="SAS Stata" ts_creation="1542713903.03" ts_lastsave="1542715304.68" unique_id="19">
			<rich_text scale="h1">SAS and Stata</rich_text>
			<rich_text>

Here's how we import SAS files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



Here's how we import Stata files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="47" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pandas has already been imported

# Import sas7bdat package
from sas7bdat import SAS7BDAT

# Save file to a DataFrame: df_sas - this is a pandas data frame
with SAS7BDAT('sales.sas7bdat') as file:
    df_sas=file.to_data_frame()
</codebox>
			<codebox char_offset="86" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas
import pandas as pd

# Load Stata file into a pandas DataFrame: df
df = pd.read_stata(&quot;disarea.dta&quot;)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="HDF5" prog_lang="custom-colors" readonly="False" tags="HDF5" ts_creation="1542714323.61" ts_lastsave="1542800518.61" unique_id="20">
			<rich_text scale="h1">HDF5</rich_text>
			<rich_text>

Here's how we import HDF5 files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Data is stored in separate sections and we can view them as if it were a dictionary. Certain sections may contain a “table of content” of sorts for the data set.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Once we have identified the data we want, we can access it through its label. This subsection will have sub-keys.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="39" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import the h5py module
import h5py

# Load file: data
data = h5py.File(&quot;LIGO_data.hdf5&quot;,&quot;r&quot;)
</codebox>
			<codebox char_offset="206" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print the keys of the file
for key in data.keys():
    print(key)

</codebox>
			<codebox char_offset="325" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">group=data[&quot;strain&quot;]

# Check out keys of group
for key in group.keys():
    print(key)

# Set variable equal to time series data: strain
# strain can be plotted
strain=data['strain'][&quot;Strain&quot;].value
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="SQL" prog_lang="custom-colors" readonly="False" tags="sql" ts_creation="1542800512.04" ts_lastsave="1543084039.51" unique_id="21">
			<rich_text scale="h1">SQL</rich_text>
			<rich_text>

Common DB types: Postgresql, MySQL, SQLite, 

This is how we open the database, run a basic query, and import it into a pandas data frame.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Fetch only a certain number of rows.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Running queries from pandas</rich_text>
			<rich_text>

We can alternatively run the query from pandas

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>







WHat is the pro and cons of using the pandas way to execute a SQL query?

Revise INNER JOIN, OUTER JOIN and different types of joins</rich_text>
			<codebox char_offset="145" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import necessary module
from sqlalchemy import create_engine
from pandas import pd

# Create engine: engine
engine = create_engine(&quot;sqlite:///Chinook.sqlite&quot;)

# a list of table names in the DB can be accessed through the table_names() method
table_names = engine.table_names()

con = engine.connect()
# We can replace the above line with a context manager with statement
# with engine.connect() as con:
#     ....
rs = con.execute(&quot;SELECT * FROM Table_Name&quot;)
df = pd.DataFrame(rs.fetchall())
df.columns = rs.keys() # Set the column names to be the same as that in the DB
con.close()</codebox>
			<codebox char_offset="187" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df = pd.DataFrame(rs.fetchmany(size=5))</codebox>
			<codebox char_offset="267" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df = pd.read_sql_query(&quot;SELECT * FROM Table&quot;, engine)
</codebox>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Basic SQL Syntax" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543082608.26" ts_lastsave="1543084021.69" unique_id="28">
				<rich_text scale="h1">Basic SQL Syntax</rich_text>
				<rich_text>

</rich_text>
				<rich_text justification="left"></rich_text>
				<codebox char_offset="18" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">-- Selects all records in all columns from table
SELECT * FROM Table_Name;

-- Selects only certain columns from the table
SELECT col1, col2, col3 FROM Table_Name;

-- Display only those records which satisfies a certain condition on a certain column
SELECT * FROM Table_Name WHERE col1 &gt;= 6;

-- Select all records but sort them by a certain column
SELECT * FROM Employee ORDER BY BirthDate;

-- Performs inner join. Displays only certain columns selected from two tables. Specifies which column to use as the joining key
SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID;

-- Another example of multiple table query
SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId=Track.TrackID WHERE Milliseconds &lt; 250000;</codebox>
			</node>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Importing Data in Python Part 2" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542988073.09" ts_lastsave="1543084212.47" unique_id="22">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="urllib" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542988172.07" ts_lastsave="1543082459.11" unique_id="23">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Importing flat/data files directly from the web</rich_text>
			<rich_text>

Using urllib
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Or directly using pandas

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Importing excel files can be done as well

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="63" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">from urllib.requests import urlretrieve
url = &quot;http://some.url/data.csv&quot;
urlretrieve(url, &quot;localdata.csv&quot;)
</codebox>
			<codebox char_offset="92" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df=pd.read_csv(url,sep=&quot;;&quot;)</codebox>
			<codebox char_offset="138" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import pandas as pd
url = 'http://my.url/data.xls' # this can be other supported protocols such as ftp. Local file can be referenced with file:///
xl = pd.read_excel(url,sheetname=None)
print(xl.keys()) # prints the sheetnames - individual sheets can be referenced as if xl was a dictionary</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="HTML requests" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542989430.82" ts_lastsave="1543082474.39" unique_id="24">
			<rich_text scale="h1">HTTP GET requests</rich_text>
			<rich_text>

Using urllib

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Using requests module

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The response object also has a json() method to parse any API JSON response.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="33" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">from urllib.request import urlopen, Request
url = &quot;http://my.url&quot;
request = Request(url)
response = urlopen(request)
html = response.read()
response.close()</codebox>
			<codebox char_offset="59" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import requests
url = &quot;http://my.url&quot;
r = requests.get(url)
text = r.text</codebox>
			<codebox char_offset="140" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">json_data = r.json()
#json_data is a dictionary and can be accessed like key-value pairs
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="BeautifulSoup" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542990129.82" ts_lastsave="1543082491.31" unique_id="25">
			<rich_text scale="h1">Using BeautifulSoup</rich_text>
			<rich_text>

Starting </rich_text>
			<rich_text link="node 25">BeautifulSoup</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Then we can do things to the soup.

Prettify it

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Get the title

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Get the text

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Find tags

</rich_text>
			<rich_text justification="left"></rich_text>
			<codebox char_offset="45" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">from bs4 import BeautifulSoup
import requests
url = &quot;http://my.url&quot;
r = requests.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc)</codebox>
			<codebox char_offset="97" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># pretty soup is a properly indented soup object
pretty_soup = soup.prettify()</codebox>
			<codebox char_offset="115" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># this string contains the &lt;title&gt; tags
title = soup.title</codebox>
			<codebox char_offset="132" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This string contains no tags, only the textual part of the page
text = soup.text</codebox>
			<codebox char_offset="146" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">a_tags = soup.find_all('a') # finds all a tags -usually these will have urls
# a_tags is of the type ResultSet - this is an iterable
# link is a Tag object. Printing it gives the results together with the tags
# calling the method get('href') filters the result to give only those with href
for link in a_tags:
    print(link.get('href'))</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="JSON" prog_lang="custom-colors" readonly="False" tags="json" ts_creation="1543079116.75" ts_lastsave="1543082504.53" unique_id="26">
			<rich_text scale="h1">Importing data from JSON objects</rich_text>
			<rich_text>

JSON  stands for Javascript Object Notation

Has a key value pair structure and is text readable. The value themselves can in turn be json objects.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The requests module also has a method for the response object to  turn JSON responses into key value pairs. See the </rich_text>
			<rich_text link="node 24">HTML requests node</rich_text>
			<rich_text>.

</rich_text>
			<codebox char_offset="183" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import json
# json_data is a dictionary
with open('file.json', 'r') as json_file:
    json_data = json.load(json_file)

</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="APIs" prog_lang="custom-colors" readonly="False" tags="API" ts_creation="1543080225.62" ts_lastsave="1543084190.58" unique_id="27">
			<rich_text scale="h1">Miscellaneous information about APIs</rich_text>
			<rich_text>

REST stands for Representational State Transfer


Authenticating to Twitter API

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="119" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import tweepy, json
access_token = &quot;...&quot;
access_token_secret = &quot;...&quot;
consumer_key = &quot;...&quot;
consumer_secret = &quot;...&quot;
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Cleaning Data in Python" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543084190.59" ts_lastsave="1543853953.06" unique_id="29">
		<rich_text>


</rich_text>
		<rich_text underline="single" weight="heavy">Common problems of unclean data</rich_text>
		<rich_text>

• Inconsistent column names
• Missing data - either drop them, fill them up or leave as it is
• Outliers
• Duplicate rows
• Need to process columns
• Column types signal unexpected values

</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Useful methods for data inspection" prog_lang="custom-colors" readonly="False" tags="inspecting data" ts_creation="1543084457.13" ts_lastsave="1544000921.04" unique_id="30">
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="2" frame_height="1075" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># assuming df is a pandas data frame

# Check out the first five rows of data
print(df.head())

# Check out the last five rows of data
print(df.tail())

# print the number of rows and columns of this data set in (rows, cols) format - note that this is not a method
print(df.shape)

# print the names of the columns - note that this is not a method
print(df.columns)

# print out info about the types of objects in each column, how many nulls there are etc
print(df.info())

# print out summary statistics such as count, mean, median, std, max, quartiles for numeric columns
df.describe()

# print the count of occurences for various categories, including NA
# applies to categorical data only
print(df['Column Name'].value_counts(dropna=False))

# Prints the row indices of the data frame
print(df.index)

# Counts the number of entries - returns a series
# Can have a list of column names in the square brackets as well
df[&quot;colname&quot;].count())

# Calculates the mean/median/std dev/min/max
# if [] is left out, this computes the mean/median/std dev/min/max of all numeric columns
# there is an axis option on how these should be computed - axis=&quot;column&quot; for calculating across the row
df[&quot;colname&quot;].mean()
df[&quot;colname&quot;].median()
df[&quot;colname&quot;].std()
df[&quot;colname&quot;].min()
df[&quot;colname&quot;].max()

# Computes the 25% quantile for all numeric columns
# Computes the 25% and 75% quantile for all numeric columns
df.quantile(0.25)
df.quantile([0.25, 0.75])

# prints the unique entries in this column
# counts the number of times each unique entry occurs
df.[&quot;colname&quot;].unique()
df.[&quot;colname&quot;].nunique()

</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Melting, Pivoting and Splitting" prog_lang="custom-colors" readonly="False" tags="melting, pivoting, splitting" ts_creation="1543420088.28" ts_lastsave="1544432356.71" unique_id="31">
			<rich_text scale="h1">Melting</rich_text>
			<rich_text>

Melting - doing away with certain chosen columns and putting the values associated with those attributes into rows.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can assign names to the new columns of the melted data frame through var_name and value_name.
Or we can simply assign the names through the columns variable by passing a list.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Pivoting</rich_text>
			<rich_text>

This is the opposite of melting.
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that the above is not quite the original data frame - it's got a hierachical index aka Multiindex.

We can reset the index to get back what we want. This is covered in more depth in a later chapter.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Pivoting with aggregation function to deal  with duplicates. The default aggregation function used is np.mean if no aggfunc argument is specified.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text weight="heavy">Q: Does the function supplied to the pivot_table method need to be a numpy function?
Q: There are two ways to reference columns - either by df[&quot;colname&quot;] or df.colname. Are there situations where only one way works? Personally i prefer the former. WOn't there be ambiguity when the column name contains a dot ‘.’?
</rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Splitting strings</rich_text>
			<rich_text>

This is how we split strings and create new columns. Note how we apply the str attribute

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Split() and get() methods

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

More on melting and pivoting is covered </rich_text>
			<rich_text link="node 47">here</rich_text>
			<rich_text>.


</rich_text>
			<codebox char_offset="126" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># id_vars represents the columns that we DO NOT wish to melt
# value_vars represents the columns that we DO wish to melt
# by default, if no value_vars is defined, all columns not in id_vars will be melted
melted_frame = pd.melt(frame=unmelted_frame, id_vars = ['col1','col2'], value_vars = ['meltcol1','meltcol2'])
</codebox>
			<codebox char_offset="309" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Melt airquality: airquality_melt
airquality_melt = pd.melt(frame=airquality, id_vars=[&quot;Month&quot;,&quot;Day&quot;], var_name=&quot;measurement&quot;, value_name=&quot;reading&quot;)

airquality_melt.columns = [&quot;col1name&quot;, &quot;col2name&quot;, &quot;col3name&quot;]
</codebox>
			<codebox char_offset="355" frame_height="445" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># original data frame
#    Month  Day measurement  reading
# 0      5    1       Ozone     41.0
# 1      5    2       Ozone     36.0
# 2      5    3       Ozone     12.0
# 3      5    4       Ozone     18.0
# 4      5    5       Ozone      NaN


# Pivot airquality_melt: airquality_pivot
airquality_pivot = airquality_melt.pivot_table(index=[&quot;Month&quot;, &quot;Day&quot;], columns=&quot;measurement&quot;, values=&quot;reading&quot;)

# This transforms the data frame back into this
# measurement  Ozone  Solar.R  Temp  Wind
# Month Day                              
# 5     1       41.0    190.0  67.0   7.4
#       2       36.0    118.0  72.0   8.0
#       3       12.0    149.0  74.0  12.6
#       4       18.0    313.0  62.0  11.5
#       5        NaN      NaN  56.0  14.3
</codebox>
			<codebox char_offset="563" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print the index of airquality_pivot
print(airquality_pivot.index)

# Reset the index of airquality_pivot: airquality_pivot_reset
airquality_pivot_reset = airquality_pivot.reset_index()

# Print the new index of airquality_pivot_reset
print(airquality_pivot_reset)
</codebox>
			<codebox char_offset="714" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pivot airquality_dup: airquality_pivot
airquality_pivot = airquality_dup.pivot_table(index=[&quot;Month&quot;,&quot;Day&quot;], columns=&quot;measurement&quot;, values=&quot;reading&quot;, aggfunc=np.mean)

# Reset the index of airquality_pivot
airquality_pivot = airquality_pivot.reset_index()
</codebox>
			<codebox char_offset="1143" frame_height="355" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Melt tb: tb_melt
tb_melt = pd.melt(frame=tb, id_vars=[&quot;country&quot;, &quot;year&quot;])

# Create the 'gender' column
tb_melt['gender'] = tb_melt.variable.str[0]

# Create the 'age_group' column
tb_melt['age_group'] = tb_melt.variable.str[1:]


# The result looks like this
#   country  year variable  value gender age_group
# 0      AD  2000     m014    0.0      m       014
# 1      AE  2000     m014    2.0      m       014
# 2      AF  2000     m014   52.0      m       014
# 3      AG  2000     m014    0.0      m       014
# 4      AL  2000     m014    2.0      m       014
</codebox>
			<codebox char_offset="1173" frame_height="430" frame_width="100" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="False"># Original data frame looks a bit like this (many columns omitted)
#         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone  \
#0    1/5/2015  289        2776.0            NaN            10030.0   
#1    1/4/2015  288        2775.0            NaN             9780.0   
#2    1/3/2015  287        2769.0         8166.0             9722.0   
#3    1/2/2015  286           NaN         8157.0                NaN   
#4  12/31/2014  284        2730.0         8115.0             9633.0   


# Melt ebola: ebola_melt
ebola_melt = pd.melt(ebola, id_vars=[&quot;Date&quot;, &quot;Day&quot;], var_name=&quot;type_country&quot;, value_name=&quot;counts&quot;)

# Create the 'str_split' column
# This also seems to work: ebola_melt['str_split'] = ebola_melt.type_country.str.split(&quot;_&quot;)
ebola_melt['str_split'] = ebola_melt[&quot;type_country&quot;].str.split(&quot;_&quot;)

# Create the 'type' column
ebola_melt['type'] = ebola_melt[&quot;str_split&quot;].str.get(0)

# Create the 'country' column
ebola_melt['country'] = ebola_melt[&quot;str_split&quot;].str.get(1)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Combining data" prog_lang="custom-colors" readonly="False" tags="merging" ts_creation="1543644784.14" ts_lastsave="1543917659.56" unique_id="32">
			<rich_text scale="h1">Concatenating rows and columns</rich_text>
			<rich_text>

Concatenating data - adding rows

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Adding columns - specify axis=1 as an argument. If omitted, axis=0, as is the case above.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Finding files</rich_text>
			<rich_text>

Finding files in the local workspace - first we find all files matching a certain pattern using glob.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Merging data frames</rich_text>
			<rich_text>

Merging data frames - this is in cases where two data frames have a common column we can use to merge the data.
left_on and right_on are the column names of the common column.
1-1, many-1, 1-many merging all use the same code. Think about different types of JOINs in SQL.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Many to one merging also uses the same syntax
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Here are the left, right and resulting data frames

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

For many-many merges, all duplicate keys will be created

</rich_text>
			<codebox char_offset="66" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># uber1, uber2 and uber3 are data frames
# pandas as been imported as pd
# Concatenate uber1, uber2, and uber3: row_concat
row_concat = pd.concat([uber1,uber2,uber3])

# To use running indices in the new data frame, add an ignore_index argument
row_concat = pd.concat([uber1,uber2,uber3], ignore_index=True)</codebox>
			<codebox char_offset="160" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Concatenate ebola_melt and status_country column-wise: ebola_tidy
ebola_tidy = pd.concat([ebola_melt,status_country],axis=1)
</codebox>
			<codebox char_offset="281" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import glob

# Write the pattern: pattern
pattern = '*.csv'

# Save all file matches: csv_files - this is a list of filenames which we can use for importing in pandas
csv_files = glob.glob(pattern)
</codebox>
			<codebox char_offset="579" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Merge the DataFrames: o2o
o2o = pd.merge(left=site, right=visited, left_on=&quot;name&quot;, right_on=&quot;site&quot;)
</codebox>
			<codebox char_offset="628" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Merge the DataFrames: m2o
m2o = pd.merge(left=site, right=visited, left_on=&quot;name&quot;, right_on=&quot;site&quot;)
</codebox>
			<codebox char_offset="683" frame_height="235" frame_width="30" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sh" width_in_pixels="False">
    name    lat    long
0   DR-1 -49.85 -128.57
1   DR-3 -47.15 -126.72
2  MSK-4 -48.87 -123.40
</codebox>
			<codebox char_offset="684" frame_height="235" frame_width="30" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="ada" width_in_pixels="False">
   ident   site       dated
0    619   DR-1  1927-02-08
1    622   DR-1  1927-02-10
2    734   DR-3  1939-01-07
3    735   DR-3  1930-01-12
4    751   DR-3  1930-02-26
5    752   DR-3         NaN
6    837  MSK-4  1932-01-14
7    844   DR-1  1932-03-22
</codebox>
			<codebox char_offset="685" frame_height="235" frame_width="39" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sh" width_in_pixels="False">    name    lat    long  ident   site       dated
0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08
1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10
2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22
3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07
4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12
5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26
6   DR-3 -47.15 -126.72    752   DR-3         NaN
7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Cleaning data" prog_lang="custom-colors" readonly="False" tags="cleaning data, converting types, NA, asserts, applying functions" ts_creation="1543678078.56" ts_lastsave="1543853250.54" unique_id="33">
			<rich_text scale="h1">Dtypes</rich_text>
			<rich_text>

We can view the types assigned to each column through the dtypes method. We can also test if the data is of a certain type.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>




</rich_text>
			<rich_text weight="heavy">Q: Categorical variables - what if there are additional variables not represented by the data frame. if we add new ones through row combine (say), would  it cause any error?</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: How to prevent NA categorical values from getting coded in their proper categories?</rich_text>
			<rich_text> Ans: have to look at data beforehand and recode them as NA

</rich_text>
			<rich_text weight="heavy">Q: For the function passed to np.apply(), what data type for the input argument is assumed? An entire row? Or individual element?</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: check_null_or_valid is row based function, but we're passing almost the entire data frame. How does this work?</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text weight="heavy">Q: Any way to specify integer or float when casting to numeric data type via pd.to_numeric? Sometimes that is useful as it helps with memory management.</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: .all() is still quite unclear</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: For previous chapter - how to extract data from websites that depend on javascript to display data? viewing the source does not show up the data, so cannot use beautifulsoup immediately. Is there any python package to help display this content?</rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="133" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">print(df.dtypes)

# Test if country is of type object
assert gapminder.country.dtypes == np.object

# Test if year is of type int64
assert gapminder.year.dtypes == np.int64

# Test if life_expectancy is of type float64
assert gapminder.life_expectancy.dtypes == np.float64
</codebox>
			<codebox char_offset="707" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Check whether the values in the row are valid
assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()
</codebox>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Casting data" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543831800.06" ts_lastsave="1543853139.63" unique_id="34">
				<rich_text scale="h1">Casting data</rich_text>
				<rich_text>

Casting columns of data as categoricals, to string and to numeric

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>
</rich_text>
				<codebox char_offset="81" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Convert the smoker column to type 'category'
tips.smoker = tips.smoker.astype(&quot;category&quot;)

#Converting to string type
df['treatment b'] = df['treatment b'].astype(str)

# converting to numeric
# coerce will return NaN when error is encountered. default behaviour is errors=&quot;raise&quot;, which is to raise an exception
 df['treatment a'] = pd.to_numeric(df['treatment a'], errors='coerce')</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Checking validity using RE and other string methods" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543831830.28" ts_lastsave="1544201844.83" unique_id="35">
				<rich_text scale="h1">Using regular expressions</rich_text>
				<rich_text>

Checking for valid strings using regular expressions.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>


We can check if the data satisfies a certain pattern.
</rich_text>
				<rich_text weight="heavy">Q: Why isn't there a need to import re?</rich_text>
				<rich_text>

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>



</rich_text>
				<rich_text scale="h1">String Methods</rich_text>
				<rich_text>

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Other methods include str.upper()

</rich_text>
				<codebox char_offset="82" frame_height="670" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import the regular expression module
import re

# Compile the pattern: prog
prog = re.compile('\d{3}-\d{3}-\d{4}')

# See if the pattern matches
result = prog.match('123-456-7890')
print(bool(result))

# -------------------------------------------------------------
# Alternatively, we can return all the matches through findall.
# matches is a list containing all the matched strings
# Find the numeric values: matches
matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana')

# Print the matches
print(matches)

# -------------------------------------------------------------
# Figuring out if the string matches a pattern
# Write the first pattern
pattern1 = bool(re.match(pattern='\d{3}-\d{3}-\d{4}', string='123-456-7890'))
print(pattern1)

# Write the second pattern
pattern2 = bool(re.match(pattern='\$\d+\.\d{2}', string='$123.45'))
print(pattern2)

# Write the third pattern
pattern3 = bool(re.match(pattern='[A-Z]\w*', string='Australia'))
print(pattern3)
</codebox>
				<codebox char_offset="181" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the series of countries: countries
countries = gapminder[&quot;country&quot;]

# Drop all the duplicates from countries
countries = countries.drop_duplicates()

# Write the regular expression: pattern
pattern = '^[A-Za-z\.\s]*$'

# Create the Boolean vector: mask
mask = countries.str.contains(pattern)

# Invert the mask: mask_inverse
mask_inverse = ~mask

# Subset countries using mask_inverse: invalid_countries
invalid_countries = countries.loc[mask_inverse]

# Print invalid_countries
print(invalid_countries)
</codebox>
				<codebox char_offset="202" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Strip extra whitespace from the column names: df.columns
df.columns = df.columns.str.strip()

# Extract data for which the destination airport is Dallas: dallas
dallas = df['Destination Airport'].str.contains(&quot;DAL&quot;)

# The index can be manipulated  as well
df.index = df.index.str.upper()
df.index = df.index.map(str.lower)
</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Using apply()" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543831862.78" ts_lastsave="1543853169.2" unique_id="36">
				<rich_text scale="h1">Using the apply() function</rich_text>
				<rich_text>

Using .apply() to recode data. We can specify a series, and pass a custom function meant to take in a single string as an argument.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

We can also use a built-in functions to apply across rows or columns

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

A more complex use of .apply, with a two-input function specified and an additional argument passed into the function.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Another complex application of apply(). 

It seems that g1800s.iloc[:, </rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>:].apply(check_null_or_valid, axis=</rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>) returns a data frame full of NaN and True. Within columns we typically see NaN mixed with Trues.
Why is it that g1800s.iloc[:, </rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>:].apply(check_null_or_valid, axis=</rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>).all() still give us all Trues? Are NaN not considered False?

</rich_text>
				<rich_text weight="heavy">Q: Why is there a need to drop the NAs?</rich_text>
				<rich_text>
</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>





</rich_text>
				<codebox char_offset="161" frame_height="550" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Define recode_gender()
def recode_gender(gender):

    # Return 0 if gender is 'Female'
    if gender == &quot;Female&quot;:
        return 0
    
    # Return 1 if gender is 'Male'    
    elif gender == &quot;Male&quot;:
        return 1
    
    # Return np.nan    
    else:
        return np.nan

# Apply the function to the sex column
tips['recode'] = tips.sex.apply(recode_gender)

# -------------------------------------------------------------
# Or make use of lambda functions
# Write the lambda function using replace
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))

# Write the lambda function using regular expressions
tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])
</codebox>
				<codebox char_offset="234" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Column wise
# Applies mean across all columns of data frame df - default behaviour axis=0
df.apply(np.mean, axis=0)

# Row wise
# Applies mean across all rows of data frame df - need to specify axis=1
df.apply(np.mean, axis=1)</codebox>
				<codebox char_offset="357" frame_height="505" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def diff_money(row, pattern):
 icost = row['Initial Cost']
 tef = row['Total Est. Fee']

 if bool(pattern.match(icost)) and bool(pattern.match(tef)):
 icost = icost.replace(&quot;$&quot;,
&quot;&quot;)
 tef = tef.replace(&quot;$&quot;,
&quot;&quot;)

 icost = float(icost)
 tef = float(tef)

 return icost - tef
 else:
 return(NaN)
 
df_subset['diff'] = df_subset.apply(diff_money, axis=1, pattern=pattern)

# Output data frame
# Job # Doc # Borough Initial Cost Total Est. Fee diff
#0 121577873 2 MANHATTAN $75000.00 $986.00 74014.0
#1 520129502 1 STATEN ISLAND $0.00 $1144.00 -1144.0
# 121601560 1 MANHATTAN $30000.00 $522.50 29477.5
#3 121601203 1 MANHATTAN $1500.00 $225.00 1275.0 </codebox>
				<codebox char_offset="738" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def check_null_or_valid(row_data):
    &quot;&quot;&quot;Function that takes a row of data,
    drops all missing values,
    and checks if all remaining values are greater than or equal to 0
    &quot;&quot;&quot;

    no_na = row_data.dropna()
    numeric = pd.to_numeric(no_na)
    ge0 = numeric &gt;= 0
    return ge0

# Check whether the first column is 'Life expectancy'
assert g1800s.columns[0] == &quot;Life expectancy&quot;

# Check whether the values in the row are valid
assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()

# Check that there is only one instance of each country
assert g1800s['Life expectancy'].value_counts()[0] == 1
</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Duplicates and NAs" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543832028.59" ts_lastsave="1544202194.63" unique_id="37">
				<rich_text scale="h1">Dealing with duplicates and NAs</rich_text>
				<rich_text>

Remove duplicate rows

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Dropping ANY row that has NA.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Replacing NAs in a certain column with the mean, or with our specified values

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Dropping values

</rich_text>
				<rich_text justification="left"></rich_text>
				<codebox char_offset="56" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the new DataFrame: tracks
tracks = billboard[[&quot;year&quot;,&quot;artist&quot;,&quot;track&quot;,&quot;time&quot;]]

# Drop the duplicates: tracks_no_duplicates
tracks_no_duplicates = tracks.drop_duplicates()
</codebox>
				<codebox char_offset="90" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">tips_dropped = tips_nan.dropna()

# Drop rows that has ANY NaNs, i.e. drop as long as there is at least one NaN
df.dropna(how='any')

# use how=&quot;all&quot; for dropping rows with all NaN</codebox>
				<codebox char_offset="172" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Calculate the mean of the Ozone column: oz_mean
oz_mean = airquality[&quot;Ozone&quot;].mean()

# Replace all the missing values in the Ozone column with the mean
airquality['Ozone'] = airquality[&quot;Ozone&quot;].fillna(oz_mean)

tips_nan['sex'] = tips_nan['sex'].fillna('missing')

tips_nan[['total_bill', 'size']] = tips_nan[['total_bill','size']].fillna(0)</codebox>
				<codebox char_offset="192" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Remove the appropriate columns: df_dropped
# list_to_drop contains a list of column names to drop
df_dropped = df.drop(list_to_drop,axis = &quot;columns&quot;)

# Drop columns with less than 1000 non-missing values
titanic.dropna(thresh=1000, axis='columns')</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Using asserts to verify" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543832053.66" ts_lastsave="1543853196.83" unique_id="38">
				<rich_text scale="h1">Using asserts</rich_text>
				<rich_text>

Using asserts to test if cleaning is successful.

notnull() converts all entries in the data frame to a boolean. all() checks each column and returns True if and only if all entries in the column are True. In effect, notnull().all() returns a Series like object. The final all() takes this Series like object and return True if and only if all entries in this Series are True.

Instead of check for non-null entries, we can check if all entries are greater than or equal to zero. Just replace ebola.notnull() by ebola &gt;= 0.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

</rich_text>
				<codebox char_offset="540" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Assert that there are no missing values
assert ebola.notnull().all().all()

# Assert that all values are &gt;= 0
assert (ebola &gt;= 0).all().all()

# Assert that country does not contain any missing values
assert pd.notnull(gapminder.country).all()

# Assert that year does not contain any missing values
assert pd.notnull(gapminder.year).all()

# Test if country is of type object
assert gapminder.country.dtypes == np.object

# Test if year is of type int64
assert gapminder.year.dtypes == np.int64

# Test if life_expectancy is of type float64
assert gapminder.life_expectancy.dtypes == np.float64
</codebox>
			</node>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="pandas Foundations" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543853929.51" ts_lastsave="1544201882.56" unique_id="39">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Investigating data frames" prog_lang="custom-colors" readonly="False" tags="numpy, pandas" ts_creation="1543918003.92" ts_lastsave="1544162047.62" unique_id="41">
			<rich_text scale="h1">Numpy and pandas</rich_text>
			<rich_text>

Numpy and pandas can interoperate

</rich_text>
			<rich_text justification="left"></rich_text>
			<codebox char_offset="53" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># df is a pandas data frame
# Create a numpy array of DataFrame values: np_vals - this will only affect numerical columns
# This returns a numpy array
np_vals = df.values

# Create new array of base 10 logarithm values: np_vals_log10
# This returns a numpy array
np_vals_log10 = np.log10(np_vals)

# Create array of new DataFrame by passing df to np.log10(): df_log10 - this will only affect numerical columns
# This returns a pandas data frame
df_log10 = np.log10(df)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on plotting" prog_lang="custom-colors" readonly="False" tags="matplotlib, plotting" ts_creation="1543918926.97" ts_lastsave="1544162070.63" unique_id="42">
			<rich_text>
</rich_text>
			<rich_text scale="h1">More on plotting</rich_text>
			<rich_text>

There are three different idioms for plotting. They have largely the same set of arguments, but be careful!
• df.plot(kind = “hist”)
• df.plt.hist()
• df.hist()


When .plot() is called without argument, line plot is assumed

Here are the arguments for the df.plot() function

x and y: to specify both when plotting a scatter plot
y: This is the y data, which can be a list. Applicable for hist and box.
color: For the color of the line/data point etc. Can be specified as “red”, “green”, etc.
subplots: Boolean. If set to True, all data series in the plot statement will be plotted as subplots, i.e. separately, on different graphs, separate axes.
style: for type of data marker e.g. “.-”, “.”
legend: Boolean - to show legend or not
kind: “scatter”, “box”, “hist” for various types of graphs
s: For scatter plots. This should equal the array containing the normalised sizes of the data points
alpha: transparency option when overlapping plot marks are present


plt.yscale(“log”) for logarithmic y axis


multiple plots of different types

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Previous related node on plotting </rich_text>
			<rich_text link="node 4">here...</rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="1061" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This formats the plots such that they appear on separate rows
fig, axes = plt.subplots(nrows=2, ncols=1)

# Plot the PDF
df.fraction.plot(ax=axes[0], kind='hist', normed=True, bins=30, range=(0,.3))
plt.show()

# Plot the CDF
df.fraction.plot(ax=axes[1], kind=&quot;hist&quot;, normed=True, cumulative=True, bins=30, range=(0,.3))
plt.show()</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Date and Time" prog_lang="custom-colors" readonly="False" tags="DateTimeIndex, date, time" ts_creation="1544001509.24" ts_lastsave="1544903835.77" unique_id="43">
			<rich_text scale="h1">Date and Time manipulations</rich_text>
			<rich_text>

Date and Time are described in ISO 8601 format

yyyy-mm-dd hh:mm:ss

</rich_text>
			<rich_text scale="h1">Creating a DateTimeIndex</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Referencing with loc() and without</rich_text>
			<rich_text>

Various ways of indexing using loc(), and not using loc. Partial datetime string selection is fine.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Reindexing</rich_text>
			<rich_text>

Reindexing using the index of another data frame

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that indexes are non-mutable objects, i.e. they cannot be changed one at a time in place. However, if needed, the index can be replaced entirely

More on indexing is covered </rich_text>
			<rich_text link="node 46">here</rich_text>
			<rich_text>.

</rich_text>
			<rich_text scale="h1">Downsampling</rich_text>
			<rich_text>

The aggregation function is specified at the end.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Upsampling</rich_text>
			<rich_text>

ffill() for forward fill any missing data. bfill() for backward fill.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Moving quantities</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Localising to a timezone</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text weight="heavy">Q: for resample on week, when does the week start? </rich_text>
			<rich_text>
</rich_text>
			<rich_text weight="heavy">Q: It seems that df[&quot;col&quot;][&quot;date&quot;] or df[&quot;date&quot;][&quot;col&quot;] both work. Won't there be ambiguity sometimes?</rich_text>
			<rich_text>
</rich_text>
			<rich_text weight="heavy">Q: What if the date time index is not in order?</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: Daily hours of clear sky. if .resample(&quot;D&quot;) has no aggregation method, what is the default? seems to be sum</rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="124" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># date_list is a normal python list containing date time strings in the format below

# Prepare a format string: time_format
time_format = &quot;%Y-%m-%d %H:%M&quot;

# Convert date_list into a datetime object: my_datetimes
my_datetimes = pd.to_datetime(date_list,format=time_format)  

# Construct a pandas Series using temperature_list and my_datetimes: time_series
time_series = pd.Series(temperature_list, index=my_datetimes)</codebox>
			<codebox char_offset="264" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Extract the hour from 9pm to 10pm on '2010-10-11': ts1
ts1 = ts0.loc['2010-10-11 21:00:00':'2010-10-11 22:00:00']

# Extract '2010-07-04' from ts0: ts2
ts2 = ts0.loc[&quot;2010-07-04&quot;]

# Extract data from '2010-12-15' to '2010-12-31': ts3
ts3 = ts0.loc[&quot;2010-12-15&quot;:&quot;2010-12-31&quot;]

# Extract temperature data for August: august
august = df.loc[&quot;2010-08&quot;][&quot;Temperature&quot;]

# Extract data from 2010-Aug-01 to 2010-Aug-15: unsmoothed
unsmoothed = df['Temperature'][&quot;2010-08-01&quot;:&quot;2010-08-15&quot;]
</codebox>
			<codebox char_offset="330" frame_height="505" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Reindex without fill method: ts3
ts3 = ts2.reindex(ts1.index)

# NOTE: if ts1.index contains indices that are not part of the original indices of ts2, these indices will be created in ts3 with NaN entries


# Reindex with fill method, using forward fill: ts4
ts4 = ts2.reindex(ts1.index,method=&quot;ffill&quot;)
# CAUTION! This might produce unexpected fill results if the fill index is a date but not a date time index

#----------------------------
# Reset the index of ts2 to ts1, and then use linear interpolation to fill in the NaNs: ts2_interp
ts2_interp = ts2.reindex(ts1.index).interpolate(how=&quot;linear&quot;)


#----------------------------
# Convert the 'Date' column into a collection of datetime objects: df.Date
df.Date = pd.to_datetime(df[&quot;Date&quot;])

# Set the index to be the converted 'Date' column
df.set_index(&quot;Date&quot;, inplace=True)

#----------------------------
# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate
# Sometimes merged frames may not have running indices
daily_temp_climate = daily_climate.reset_index()[&quot;Temperature&quot;]
</codebox>
			<codebox char_offset="584" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Downsample to 6 hour data and aggregate by mean: df1
df1 = df[&quot;Temperature&quot;].resample(&quot;6h&quot;).mean()

# Downsample to daily data and count the number of data points: df2
df2 = df[&quot;Temperature&quot;].resample(&quot;D&quot;).count()</codebox>
			<codebox char_offset="670" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">two_days.resample('4H').ffill()

population.resample('A').first()
population.resample('A').first().interpolate('linear')</codebox>
			<codebox char_offset="693" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Apply a rolling mean with a 24 hour window: smoothed
smoothed = unsmoothed.rolling(window=24).mean()

# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August
daily_highs_smoothed = daily_highs.rolling(window=7).mean()
</codebox>
			<codebox char_offset="723" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Combine two columns of data to create a datetime series: times_tz_none 
times_tz_none = pd.to_datetime(la[&quot;Date (MM/DD/YYYY)&quot;] + ' ' + la[&quot;Wheels-off Time&quot;] )

# Localize the time to US/Central: times_tz_central
times_tz_central = times_tz_none.dt.tz_localize(&quot;US/Central&quot;)

# Convert the datetimes from US/Central to US/Pacific
times_tz_pacific = times_tz_central.dt.tz_convert(tz=&quot;US/Pacific&quot;)

# Extracts only the hour
sales['Date'].dt.hour</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Manipulating Data Frames with pandas" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544201882.56" ts_lastsave="1544901148.28" unique_id="44">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Applying functions" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544201902.08" ts_lastsave="1544202440.64" unique_id="45">
			<rich_text scale="h1">Vectorised functions</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note: As far as possible, choose to use vectorised functions instead of map() or apply() when optimising for speed.

</rich_text>
			<rich_text scale="h1">Using map and a dictionary</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Vectorised function from another module</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<codebox char_offset="22" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># divide all numeric columns by 12 and take the floor of the quotient
df.floordiv(12)

# Does the same thing as above
np.floor_divide(df, 12)

def dozens(n):
    return n//12
df.apply(dozens)

df.apply(lambda n: n//12)

df['dozens_of_eggs'] = df.eggs.floordiv(12)</codebox>
			<codebox char_offset="170" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the dictionary: red_vs_blue
red_vs_blue = {&quot;Obama&quot;:&quot;blue&quot;,&quot;Romney&quot;:&quot;red&quot;}

# Use the dictionary to map the 'winner' column to the new column: election['color']
election['color'] = election[&quot;winner&quot;].map(red_vs_blue)
</codebox>
			<codebox char_offset="214" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import zscore from scipy.stats
from scipy.stats import zscore

# Call zscore with election['turnout'] as input: turnout_zscore
turnout_zscore = zscore(election[&quot;turnout&quot;])
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on indexing" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544342756.42" ts_lastsave="1544432587.92" unique_id="46">
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: Of what use is a multiindex?</rich_text>
			<rich_text> A: To have unique indices where this is not possible using one column
</rich_text>
			<rich_text weight="heavy">Q: Why is there no need to include slice() in Advanced Indexing - Indexing multilevels of a MultiIndex</rich_text>
			<rich_text>

Indexes are not mutable. i.e. we cannot simply set df.index[0] = var. However, we can change the entire index at once through df.index = somelist

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Indexing can be hierachical

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Referencing hierachical  indexing. Slice is still quite mysterious

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: Why is slice needed in the exercise example </rich_text>
			<rich_text family="monospace" weight="heavy">stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')), :]</rich_text>
			<rich_text weight="heavy"> but not needed here?</rich_text>
			<rich_text>

Reindexing based on date and time is briefly covered </rich_text>
			<rich_text link="node 43">here</rich_text>
			<rich_text>.
Setting the index at the time of data import is cover </rich_text>
			<rich_text link="node 5">here</rich_text>
			<rich_text>.</rich_text>
			<codebox char_offset="355" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># We can change the name of the index of the rows
# Assign the string 'MONTHS' to sales.index.name
sales.index.name = &quot;MONTHS&quot;

# We can change the name of the columns
# Assign the string 'PRODUCTS' to sales.columns.name 
sales.columns.name = &quot;PRODUCTS&quot;
</codebox>
			<codebox char_offset="387" frame_height="505" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This is the original data frame
#  state  month  eggs  salt  spam
#0    CA      1    47  12.0    17
#1    CA      2   110  50.0    31
#2    NY      1   221  89.0    72
#3    NY      2    77  87.0    20
#4    TX      1   132   NaN    52
#5    TX      2   205  60.0    55

# Now we set the multi index and sort
# Set the index to be the columns ['state', 'month']: sales
sales = sales.set_index(['state', 'month'])

# Sort the MultiIndex: sales
sales = sales.sort_index()

#                eggs  salt  spam
#    state month                  
#    CA    1        47  12.0    17
#          2       110  50.0    31
#    NY    1       221  89.0    72
#          2        77  87.0    20
#    TX    1       132   NaN    52
#          2       205  60.0    55
</codebox>
			<codebox char_offset="458" frame_height="340" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#                eggs  salt  spam
#    state month                  
#    CA    1        47  12.0    17
#          2       110  50.0    31
#    NY    1       221  89.0    72
#          2        77  87.0    20
#    TX    1       132   NaN    52
#          2       205  60.0    55

# Look up data for NY in month 1: NY_month1
NY_month1 = sales.loc[(&quot;NY&quot;,1)]

# Look up data for CA and TX in month 2: CA_TX_month2
CA_TX_month2 = sales.loc[([&quot;CA&quot;,&quot;TX&quot;],2),:]

# Look up data for all states in month 2: all_month2
all_month2 = sales.loc[(slice(None), 2 ),:]</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on Melting, Pivoting and Stacking" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544432300.47" ts_lastsave="1544435411.73" unique_id="47">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Pivoting</rich_text>
			<rich_text>

The difference between pivot and pivot_table is that for the latter, one can specify an aggregation function.

This is the original </rich_text>
			<rich_text family="monospace">users</rich_text>
			<rich_text> data frame

</rich_text>
			<rich_text family="monospace">  weekday    city  visitors  signups
0     Sun  Austin       139        7
1     Sun  Dallas       237       12
2     Mon  Austin       326        3
3     Mon  Dallas       456        5
</rich_text>
			<rich_text>
For the pivot function, usually the index, columns and values parameters are needed.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

If no values parameter is specified, both visitors and signups will be pivoted. In this case, we will see another two columns in addition to the above.

When we use pivot tables, an aggregation function can be specified through aggfunc. aggfunc=&quot;count&quot; for counting number of table entries. The margins parameter enables us to calculate the column total below

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text scale="h1">Stacking and unstacking</rich_text>
			<rich_text>

The concept of stacking is to put a columns into the rows, so that the data frame becomes “taller” (like stacking blocks).

</rich_text>
			<rich_text weight="heavy">Q: how is this different from melting?</rich_text>
			<rich_text>

This is the original </rich_text>
			<rich_text family="monospace">users</rich_text>
			<rich_text> data frame.

</rich_text>
			<rich_text family="monospace">                visitors  signups
city   weekday                   
Austin Mon           326        3
       Sun           139        7
Dallas Mon           456        5
       Sun           237       12
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

If we unstack city instead, we would get the cities under visitors and signup hierachy.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The level parameter can be numeric - this allows us to specify which level in the hierachy to stack or unstack.

The levels of the multiindex can be swapped. They need to be sorted so as to return them to a more appealing form.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Melting</rich_text>
			<rich_text>

Melting only seems to act on columns. It does not make any change to indices. New columns may be created.

This is the original data frame

</rich_text>
			<rich_text family="monospace">city     Austin  Dallas
weekday                
Mon         326     456
Sun         139     237
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Now let's act on this again

</rich_text>
			<rich_text family="monospace">  weekday    city  visitors  signups
0     Sun  Austin       139        7
1     Sun  Dallas       237       12
2     Mon  Austin       326        3
3     Mon  Dallas       456        5
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

var_name and value_name parameters can be specified so that we do not get variable and value column names.

We can use melting to obtain key value pairs

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



Melting and pivoting was briefly covered </rich_text>
			<rich_text link="node 31">here</rich_text>
			<rich_text>.</rich_text>
			<codebox char_offset="433" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pivot the users DataFrame: visitors_pivot
visitors_pivot = users.pivot(index=&quot;weekday&quot;,columns=&quot;city&quot;,values=&quot;visitors&quot;)

# This gives
#city     Austin  Dallas
#weekday                
#Mon         326     456
#Sun         139     237</codebox>
			<codebox char_offset="797" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the DataFrame with the appropriate pivot table: signups_and_visitors
signups_and_visitors = users.pivot_table(index=&quot;weekday&quot;,values=[&quot;signups&quot;,&quot;visitors&quot;], aggfunc=sum)


# Add in the margins: signups_and_visitors_total 
signups_and_visitors_total = users.pivot_table(index=&quot;weekday&quot;,values=[&quot;signups&quot;,&quot;visitors&quot;], aggfunc=sum, margins=True)
</codebox>
			<codebox char_offset="1236" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unstack users by 'weekday': byweekday
byweekday = users.unstack(level=&quot;weekday&quot;)

# We get this after running unstack
#        visitors      signups    
#weekday      Mon  Sun     Mon Sun
#city                             
#Austin       326  139       3   7
#Dallas       456  237       5  12

# Calling stack will return us to the original data frame</codebox>
			<codebox char_offset="1328" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unstack users by 'city': bycity
bycity = users.unstack(level=&quot;city&quot;)

#        visitors        signups       
#city      Austin Dallas  Austin Dallas
#weekday                               
#Mon          326    456       3      5
#Sun          139    237       7     12
</codebox>
			<codebox char_offset="1560" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Swap the levels of the index of newusers: newusers
newusers = newusers.swaplevel(0,1)

# Sort the index of newusers: newusers
newusers = newusers.sort_index()
</codebox>
			<codebox char_offset="1810" frame_height="325" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Reset the index: visitors_by_city_weekday
visitors_by_city_weekday = visitors_by_city_weekday.reset_index() 

#city weekday  Austin  Dallas
#0        Mon     326     456
#1        Sun     139     237

# Melt visitors_by_city_weekday: visitors
visitors = pd.melt(visitors_by_city_weekday, id_vars=&quot;weekday&quot;, value_vars=[&quot;Austin&quot;,&quot;Dallas&quot;],value_name=&quot;visitors&quot;)

#  weekday    city  visitors
#0     Mon  Austin       326
#1     Sun  Austin       139
#2     Mon  Dallas       456
#3     Sun  Dallas       237</codebox>
			<codebox char_offset="2028" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Melt users: skinny
skinny = pd.melt(users, id_vars=[&quot;weekday&quot;,&quot;city&quot;],value_vars=[&quot;visitors&quot;,&quot;signups&quot;])

#  weekday    city  variable  value
#0     Sun  Austin  visitors    139
#1     Sun  Dallas  visitors    237
#2     Mon  Austin  visitors    326
#3     Mon  Dallas  visitors    456
#4     Sun  Austin   signups      7
#5     Sun  Dallas   signups     12
#6     Mon  Austin   signups      3
#7     Mon  Dallas   signups      5
</codebox>
			<codebox char_offset="2185" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set the new index: users_idx
users_idx = users.set_index([&quot;city&quot;,&quot;weekday&quot;])

# Obtain the key-value pairs: kv_pairs
kv_pairs = pd.melt(users_idx,col_level=0)

# Print the key-value pairs
print(kv_pairs)</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on grouping and aggregations" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544606915.13" ts_lastsave="1544901131.31" unique_id="48">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Basic grouping</rich_text>
			<rich_text>

The sequence of application is typically this:

df.groupby(...)[ columns...].aggregationfunction

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Other aggregation functions we can use include sum(), mean(), std(), first(), last(), min(), max().
This is group all aggregations by pclass, reporting only the survived column. This counts the number of survived entries in each pclass. Multiple groups and multiple columns are possible.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can even group by category from a different data frame

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

sum(axis=&quot;columns&quot;) work as well, by automatically summing all rows in data frame

Aggregating different columns with different functions

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

In the example above, max works on age and median works on fare.

Functions must be enclosed in quotes. There are sum, mean and count functions.

</rich_text>
			<rich_text scale="h1">Custom aggregation</rich_text>
			<rich_text>

We can also define our own aggregation function. The input has to take in a series.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

agg allows us to be even more flexible about aggregation by specifying different aggregators and columns in a dict

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Transformations

Transforms is different from aggregation in that it returns one value for each entry, but the computation has some dependence on the group

We use the automobile dataset and define this zscore function

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

One use of transform is to fill in missing values. See lecture notes for more detail.

Apply makes for more complex computations. It can return a data frame

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Grouping and filtering</rich_text>
			<rich_text>

The groupby object is in fact a dictionary that can be iterated over

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can filter out groups of interest

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Using map to filter

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Others

Use of idxmax to find the index where the maximum of a series occurs

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

There is idxmin() as well.

</rich_text>
			<rich_text weight="heavy">Q: What is the difference between .agg, .transform and .apply?</rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="115" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group titanic by 'pclass'
by_class = titanic.groupby(&quot;pclass&quot;)

# Aggregate 'survived' column of by_class by count
count_by_class = by_class[&quot;survived&quot;].count()
</codebox>
			<codebox char_offset="407" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group titanic by 'embarked' and 'pclass'
by_mult = titanic.groupby([&quot;embarked&quot;,&quot;pclass&quot;])

# Aggregate both the bread and butter columns
sales.groupby('weekday')[['bread','butter']].sum()

#--------------------------------------
# We can groupby a function of the index as well
# Read file: sales
sales = pd.read_csv(&quot;sales.csv&quot;,index_col=&quot;Date&quot;,parse_dates=True)

# Create a groupby object: by_day
by_day = sales.groupby(sales.index.strftime(&quot;%a&quot;))
</codebox>
			<codebox char_offset="469" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Read life_fname into a DataFrame: life
life = pd.read_csv(life_fname, index_col='Country')

# Read regions_fname into a DataFrame: regions
regions = pd.read_csv(regions_fname, index_col=&quot;Country&quot;)

# Group life by regions['region']: life_by_region
life_by_region = life.groupby(regions[&quot;region&quot;])

# Print the mean over the '2010' column of life_by_region
print(life_by_region[&quot;2010&quot;].mean())</codebox>
			<codebox char_offset="611" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group titanic by 'pclass': by_class
by_class = titanic.groupby(&quot;pclass&quot;)

# Select 'age' and 'fare'
by_class_sub = by_class[['age','fare']]

# Aggregate by_class_sub by 'max' and 'median': aggregated
aggregated = by_class_sub.agg([&quot;max&quot;,&quot;median&quot;])
</codebox>
			<codebox char_offset="865" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def data_range(series):
...: return series.max() - series.min()

sales.groupby('weekday')[['bread', 'butter']].agg(data_range)</codebox>
			<codebox char_offset="984" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group gapminder by 'Year' and 'region': by_year_region
by_year_region = gapminder.groupby(level=[&quot;Year&quot;,&quot;region&quot;])

# Create the dictionary: aggregator
aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}

# Aggregate by_year_region using the dictionary: aggregated
aggregated = by_year_region.agg(aggregator)
</codebox>
			<codebox char_offset="1208" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def zscore(series):
 ...: return (series - series.mean()) / series.std()

# This computes the zscore for every automobile's mpg, but groups them by year, i.e. the comparison is with respect to other automobiles in the same year
auto.groupby('yr')['mpg'].transform(zscore).head()</codebox>
			<codebox char_offset="1369" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def zscore_with_year_and_name(group):
...: df = pd.DataFrame(
...: {'mpg': zscore(group['mpg']),
...: 'year': group['yr'],
...: 'name': group['name']})
...: return df

auto.groupby('yr').apply(zscore_with_year_and_name).head()

#mpg name year
#0 0.058125 chevrolet chevelle malibu 70
#1 -0.503753 buick skylark 320 70
#2 0.058125 plymouth satellite 70
#3 -0.316460 amc rebel sst 70
#4 -0.129168 ford torino 70</codebox>
			<codebox char_offset="1466" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">splitting = auto.groupby('yr')

for group_name, group in splitting:
...: avg = group['mpg'].mean()
...: print(group_name, avg) 
</codebox>
			<codebox char_offset="1507" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group sales by 'Company': by_company
by_company = sales.groupby(&quot;Company&quot;)

# Filter 'Units' where the sum is &gt; 35: by_com_filt
by_com_filt = by_company.filter(lambda g:g[&quot;Units&quot;].sum() &gt; 35)
print(by_com_filt)</codebox>
			<codebox char_offset="1531" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">under10 = (titanic[&quot;age&quot;] &lt; 10).map({True:'under 10',False:'over 10'})

# Group by under10 and compute the survival rate
survived_mean_1 = titanic.groupby(under10)[&quot;survived&quot;].mean()
print(survived_mean_1)
</codebox>
			<codebox char_offset="1613" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#NOC        USA    URS
#Edition              
#1952     130.0  117.0
#1956     118.0  169.0
#1960     112.0  169.0
#1964     150.0  174.0
#1968     149.0  188.0
#1972     155.0  211.0
#1976     155.0  285.0
#1980       NaN  442.0
#1984     333.0    NaN
#1988     193.0  294.0

most_medals = cold_war_usa_urs_medals.idxmax(axis=&quot;columns&quot;)

#Edition
#1952    USA
#1956    URS
#1960    URS
#1964    URS
#1968    URS
#1972    URS
#1976    URS
#1980    URS
#1984    USA
1988    URS</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Merging Data Frames with pandas" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544901131.31" ts_lastsave="1544902109.54" unique_id="49">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Reading multiple files" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544902109.55" ts_lastsave="1544904028.54" unique_id="50">
			<rich_text>
The .copy() method for dataframes can be used to create a copy of the dataframe and assign it to another

</rich_text>
			<rich_text weight="heavy">Q: There is a difference between using

weather3 = weather1.reindex(year,method=&quot;ffill&quot;)
and
weather3 = weather1.reindex(year).ffill()
</rich_text>
			<rich_text>


reindexing is a way to find common elements between two data frames</rich_text>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Datacamp Usage TIPS" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543917644.25" ts_lastsave="1544680508.96" unique_id="40">
		<rich_text>
TIP: If columns are replaced by ... when using head or tail methods, we can view and configure the behaviours through the pandas get_options and set_option methods

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Simple sorting by a column

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


what is the difference between isin and contains</rich_text>
		<codebox char_offset="166" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">
pd.get_option(&quot;display.max_columns&quot;)

pd.set_option(&quot;display.max_columns&quot;,10)
</codebox>
		<codebox char_offset="197" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Sort counted by the 'totals' column
counted = counted.sort_values(ascending=False, by=&quot;totals&quot;)
</codebox>
	</node>
</cherrytree>
