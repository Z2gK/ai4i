<?xml version="1.0" ?>
<cherrytree>
	<bookmarks list="2,18,21,31,33,36,35,16,43,5,46,47,48,50,51"/>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Intro to Python for Data Science" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542451468.54" ts_lastsave="1542454833.41" unique_id="1">
		<rich_text>





</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Numpy basics" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542451543.36" ts_lastsave="1542715128.68" unique_id="2">
			<rich_text scale="h1">Creating numpy arrays</rich_text>
			<rich_text>

Lists can be converted to numpy arrays, but the numpy package has to be imported first.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Arithmetic operations</rich_text>
			<rich_text>

Arithmetic operations can be applied directly to numpy arrays.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Subsetting and Indexing</rich_text>
			<rich_text>

Numpy arrays can be subsetted and indexed.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">2D numpy arrays</rich_text>
			<rich_text>

2D numpy arrays can be created. These are just list of lists.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Basic statistical functions</rich_text>
			<rich_text>

Numpy also has basic statistical methods for objects, such as the following:
• numpy.mean()
• numpy.median()
• numpy.std()
• numpy.corrcoeff(... ,...)


</rich_text>
			<rich_text weight="heavy">Check what this numpy function does. This appeared in the Importing Data in Python Part 1 section about HDF5 data.</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>




</rich_text>
			<codebox char_offset="112" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import numpy as np
baseball = [180, 215, 210, 210, 188, 176, 209, 200]
np_baseball = np.array(baseball)</codebox>
			<codebox char_offset="202" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create array from height with correct units: np_height_m
np_height_m = np.array(height) * 0.0254

# Create array from weight with correct units: np_weight_kg
np_weight_kg = np.array(weight) * 0.453592

# Calculate the BMI: bmi
bmi = np_weight_kg / np_height_m ** 2
</codebox>
			<codebox char_offset="274" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the light array
light = bmi &lt; 21

# Print out light
print(light)

# Print out BMIs of all baseball players whose BMI is below 21
print(bmi[light])

# Print out the weight at index 50
print(np_weight[50])

# Print out sub-array of np_height: index 100 up to and including index 110
print(np_height[100:111])</codebox>
			<codebox char_offset="357" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">baseball = [[180, 78.4],
            [215, 102.7],
            [210, 98.5],
            [188, 75.2]]

# Create a 2D numpy array from baseball: np_baseball
np_baseball = np.array(baseball)
</codebox>
			<codebox char_offset="657" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set time vector
time = np.arange(0, 1, 1/num_samples)
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Intermediate Python for Data Science" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542453323.53" ts_lastsave="1542457044.44" unique_id="3">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Basic plotting with matplotlib" prog_lang="custom-colors" readonly="False" tags="matplotlib" ts_creation="1542453435.11" ts_lastsave="1543920059.23" unique_id="4">
			<rich_text scale="h1">Line and scatter plots</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Plotting using pandas module. This also seem to work, from what it seems in other courses.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The plotting function can be called directly from the data frame as well, through the plot method. This usage is seen in the Cleaning Data in Python chapter.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Adding subplots

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Other miscellaneous configuration options

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



More plotting </rich_text>
			<rich_text link="node 42">here...</rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="24" frame_height="250" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import matplotlib.pyplot as plt

# Make a line plot: year on the x-axis, pop on the y-axis
plt.plot(year,pop)

# Display the plot with plt.show() and then clean up
plt.show()
plt.clf()

# Change the line plot below to a scatter plot
plt.scatter(gdp_cap, life_exp)
</codebox>
			<codebox char_offset="120" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import pandas as pd

# Plot 'Age' variable in a histogram
# data is a data series
pd.DataFrame.hist(data)
plt.show()
</codebox>
			<codebox char_offset="282" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">

# Plot the histogram
# rot is for rotation of the x axis labels
# logx and logy are for specify if the axes need to be on the log scale
df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)

# Create the boxplot
# This is a whisker and tail plot
# rot is for rotating the x axis label
# The value plotted is in the column argument, category is by Borough
df.boxplot(column='initial_cost', by='Borough', rot=90)

# Create the scatter plot
g1800s.plot(kind=&quot;scatter&quot;, x=&quot;1800&quot;, y=&quot;1899&quot;)

# Create a histogram of life_expectancy
gapminder.life_expectancy.plot(kind=&quot;hist&quot;)
</codebox>
			<codebox char_offset="302" frame_height="490" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Add first subplot
plt.subplot(2, 1, 1) 

# Create a histogram of life_expectancy
gapminder.life_expectancy.plot(kind=&quot;hist&quot;)

# ....
# Do other things here

# Add second subplot
plt.subplot(2, 1, 2)

# Create a line plot of life expectancy per year
gapminder_agg.plot()

# Add title and specify axis labels
plt.title('Life expectancy over the years')
plt.ylabel('Life expectancy')
plt.xlabel('Year')

# Display the plots
plt.tight_layout()
plt.show()

</codebox>
			<codebox char_offset="349" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Specify axis limits
plt.xlim(20, 55)
plt.ylim(20, 55)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Pandas basics" prog_lang="custom-colors" readonly="False" tags="pandas" ts_creation="1542453803.32" ts_lastsave="1544342718.22" unique_id="5">
			<rich_text scale="h1">Building Pandas data frames</rich_text>
			<rich_text>

Pandas data frames can be built from lists with the aid of dictionaries.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Row labels can be specified.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


In an Importing Data in Python Part 2 exercise, a pandas  data frame can also be built from a list of dictionaries

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can use zip() and list() to help us build data frames - this is covered in pandas Foundation chapter

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

In fact the values for the keys do not need to be lists. Here state is a string and cities is a list. The value of state will simply be “broadcast” for all entries in the data frame.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text scale="h1">Reading CSV data into data frames</rich_text>
			<rich_text>

Reading csv files can be done through read_csv method. An optional </rich_text>
			<rich_text weight="heavy">chunksize argument </rich_text>
			<rich_text>can be added to read the data in chunks. The index_col argument is used to specify the column in the file used for the row label, if any.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Other options:
parse_dates: Boolean. Set to true to automatically identify date columns
index_col: can be a string. This column is used as the index column.



</rich_text>
			<rich_text scale="h1">Basic square bracket indexing</rich_text>
			<rich_text>

Dataframes can be indexed and selected. Single brackets give a Series. Double brackets give a DataFrame. With just simple square bracketting, we cannot select both rows and columns at once though. Note that for row data selection, the use of row labels is not supported with this method.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that this method is slightly different from the loc() method of indexing. Here, the column is in the FIRST set of square brackets.


</rich_text>
			<rich_text scale="h1">More advanced loc and iloc indexing</rich_text>
			<rich_text>

If we want 2D functionalities we need to use the loc and iloc methods. 

We can select row data using the row labels

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can query individual elements, and take their intersection. To select all values, as usual, the “:” operator is used.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

iloc simply uses numbers instead of column/row labels for subsetting
.iloc[::3,-1] refers to every third row, last column (on the right)

Seen in Importing Data in Python Part 2 exercise, a new way to reference

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

loc also takes a Series of booleans, as we can see in Cleaning Data in Python.

</rich_text>
			<rich_text scale="h1">Logical subsetting</rich_text>
			<rich_text>

We can select data based on logical criteria using bracket subsetting. These return data frames.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Think over the all() and any()s</rich_text>
			<rich_text>

Note special numpy logical functions have to be used when boolean operations are needed.
• numpy.logical_and()
• numpy.logical_or()
• numpy.logical_not()
</rich_text>
			<codebox char_offset="103" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pre-defined lists
names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']
dr =  [True, False, False, False, True, True, True]
cpc = [809, 731, 588, 18, 200, 70, 45]

# Import pandas as pd
import pandas as pd

# Create dictionary my_dict with three key:value pairs: my_dict
my_dict = {'country':names,'drives_right':dr,'cars_per_cap':cpc}

# Build a DataFrame cars from my_dict: cars
cars = pd.DataFrame(my_dict)
</codebox>
			<codebox char_offset="136" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Definition of row_labels
row_labels = ['US', 'AUS', 'JAP', 'IN', 'RU', 'MOR', 'EG']

# Specify row labels of cars
cars.index = row_labels</codebox>
			<codebox char_offset="256" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import package
import pandas as pd

# Build DataFrame of tweet texts and languages
df = pd.DataFrame(tweets_data, columns=[&quot;text&quot;,&quot;lang&quot;])

# Print head of DataFrame
print(df.head())
</codebox>
			<codebox char_offset="364" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#In [1]: list_keys
#Out[1]: ['Country', 'Total']

#In [2]: list_values
#Out[2]: [['United States', 'Soviet Union', 'United Kingdom'], [1118, 473, 273]]

# Zip the 2 lists together into one list of (key,value) tuples: zipped
zipped = list(zip(list_keys,list_values))
data = dict(zipped)
df = pd.DataFrame(data)
</codebox>
			<codebox char_offset="551" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Make a string with the value 'PA': state
state = &quot;PA&quot;
data = {'state':state, 'city':cities}
df = pd.DataFrame(data)
</codebox>
			<codebox char_offset="816" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas as pd
import pandas as pd

# Fix import by including index_col
cars = pd.read_csv('cars.csv',index_col = 0)</codebox>
			<codebox char_offset="1299" frame_height="385" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print out country column as Pandas Series
print(cars['country'])

# Print out country column as Pandas DataFrame
print(cars[['country']])

# Print out DataFrame with country and drives_right columns
print(cars[['country','drives_right']])

# Print out first 3 observations
print(cars[0:3])

# Print out fourth, fifth and sixth observation
print(cars[3:6])

 # The eggs column, 2nd to 4th row
df['eggs'][1:4]

# The eggs column, 5th row
df['eggs'][4] </codebox>
			<codebox char_offset="1595" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print out observation for Japan - this is pandas Series
print(cars.loc['JAP'])

# Print out observations for Australia and Egypt - this is a DataFrame
print(cars.loc[['AUS','EG']])</codebox>
			<codebox char_offset="1720" frame_height="250" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print out drives_right value of Morocco - this should be a value
# If &quot;MOR&quot; is replaced by :, this would be a Series
print(cars.loc['MOR','drives_right'])

# Print sub-DataFrame
print(cars.loc[['RU','MOR'],['country','drives_right']])

# Print out cars_per_cap and drives_right as DataFrame
print(cars.loc[:,['cars_per_cap','drives_right']])

# We can use : to specify a range of columns too
df.loc[:, 'eggs':'salt'] </codebox>
			<codebox char_offset="1935" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df.ix[:, 0:1]</codebox>
			<codebox char_offset="2136" frame_height="700" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Convert code to a one-liner - the argument is a True/False Series
sel = cars[cars['drives_right']]


# Create car_maniac: observations that have a cars_per_cap over 500
cpc = cars['cars_per_cap']
many_cars = cpc &gt; 500
car_maniac = cars[many_cars]

# Need to import numpy and use the logical operators for more complex logic
import numpy as np
between = np.logical_and(cpc &gt; 100, cpc &lt; 500)
medium = cars[between]

# We can use AND
df[(df.salt &gt;= 50) &amp; (df.eggs &lt; 200)] # Both conditions

# We can use OR
df[(df.salt &gt;= 50) | (df.eggs &lt; 200)] # Either condition

# Select columns with ALL nonzeros
df2.loc[:, df2.all()]

# Select columns with ANY nonzeros
df2.loc[:, df2.any()]

# Select columns with ANY NaNs
df.loc[:, df.isnull().any()]

# Select columns without NaNs
df.loc[:, df.notnull().all()]
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Python Data Science Toolbox Part 1" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542457044.44" ts_lastsave="1542458096.74" unique_id="6">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Lambda functions and its uses" prog_lang="custom-colors" readonly="False" tags="lambda, map, reduce, filter" ts_creation="1542457086.4" ts_lastsave="1542458233.21" unique_id="7">
			<rich_text scale="h1">Lambda functions</rich_text>
			<rich_text>

Lambda functions need to be enclosed in round brackets when being defined. They are defined in the format:

lambda &lt;inputs&gt;: &lt;actions on inputs and value to return&gt;

Only one “line” is permitted for the lambda function definition.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Map</rich_text>
			<rich_text>

Map function takes a function and a sequence as its arguments and applies the function to every member of the sequence, and returns the resulting sequence.

Syntax: map(func, seq)
Returns: a sequence

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Filter</rich_text>
			<rich_text>

Filter function takes a function and a sequence as its input, and returns a sequence of members from the original sequence. Filter takes the function and evaluates it on every member of the input sequence. Those that evaluate to True will be returned in the output sequence.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text> 

</rich_text>
			<rich_text scale="h1">Reduce</rich_text>
			<rich_text>

Reduce function takes a function and a sequence as its arguments.

The input function needs to take two inputs and return return one output. Reduce takes the first two elements of the sequence and applies the function. Using the output of the function as one of the input, and the third element and the other input, the function is applied on these two elements. This is repeated until all the elements of the sequence are exhausted.

Note that reduce needs to be imported from functools.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="250" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Define echo_word as a lambda function: echo_word
echo_word = (lambda word1, echo: word1 * echo)

# Call echo_word: result
result = echo_word(&quot;hey&quot;,5)
</codebox>
			<codebox char_offset="460" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: spells
spells = [&quot;protego&quot;, &quot;accio&quot;, &quot;expecto patronum&quot;, &quot;legilimens&quot;]

# Use map() to apply a lambda function over spells: shout_spells
shout_spells = map(lambda s: s+&quot;!!!&quot;, spells)
</codebox>
			<codebox char_offset="747" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']

# Use filter() to apply a lambda function over fellowship: result
result  = filter(lambda s: len(s)&gt;6, fellowship)
</codebox>
			<codebox char_offset="1249" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import reduce from functools
from functools import reduce

# Create a list of strings: stark
stark = ['robb', 'sansa', 'arya', 'brandon', 'rickon']

# Use reduce() to apply a lambda function over stark: result
result = reduce(lambda item1, item2: item1+item2,stark)
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Python Data Science Toolbox Part 2" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542458096.74" ts_lastsave="1543853929.51" unique_id="8">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Iterators" prog_lang="custom-colors" readonly="False" tags="iterators, iterables" ts_creation="1542458105.24" ts_lastsave="1542459579.8" unique_id="9">
			<rich_text scale="h1">Iterators and iterables</rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h2">Iterables</rich_text>
			<rich_text>

Examples of iterables include 
• lists 
• strings 
• dictionaries
• file connections, etc.

Iterables can be iterated by directly using them in a for loop

for item in &lt;iterable&gt;:
    &lt;do things on item&gt;

They can be explicitly turned into a iterator object by applying iter() method. Under the hood, this is what the for loop is doing to iterables.


</rich_text>
			<rich_text scale="h2">Iterators</rich_text>
			<rich_text>

Iterators are objects that can be created from iterables via the iter method. Iterators have an associated next() method.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This works on file connections as well, though we normally just use “for line in file:”

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Calling next() when there are no values left to return would give us a StopIteration error.

The splat operator * would return all the elements in the iterator all at once.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

An iterator can be created from a range object as well. The range function does not actually create a list in memory, which is good when we need to iterate over a large range.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="522" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: flash
flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']
# Create an iterator for flash: superspeed
superspeed = iter(flash)

# Print each item from the iterator
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))
</codebox>
			<codebox char_offset="614" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">file = open(&quot;file.txt&quot;)
it = iter(file)
print(next(it))
print(next(it))
...</codebox>
			<codebox char_offset="791" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">word = &quot;Data&quot;
it = iter(word)
print(*it)
# Output:
# D a t a

# Doing this again would return an error
print(*it)</codebox>
			<codebox char_offset="971" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create an iterator for range(10 ** 100): googol
googol = iter(range(10 ** 100))

# Print the first 5 values from googol - no errors here
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Enumerate" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542458136.12" ts_lastsave="1542460224.71" unique_id="10">
			<rich_text scale="h1">Enumerate</rich_text>
			<rich_text>

The enumerate function takes any iterable as its argument and returns a special enumerate object. The elements of this object consists of pairs with the index of the original member of the iterable, and the iterable itself.

This special enumerate object can be converted into a list.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


This enumerate object </rich_text>
			<rich_text weight="heavy">itself is an iterable</rich_text>
			<rich_text> and can be iterated over. The start index can be changed as well

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="297" frame_height="340" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: mutants
mutants = ['charles xavier', 
            'bobby drake', 
            'kurt wagner', 
            'max eisenhardt', 
            'kitty pryde']

# Create a list of tuples: mutant_list
mutant_list = list(enumerate(mutants))

# Print the list of tuples
print(mutant_list)

# Output
# [(0, 'charles xavier'), (1, 'bobby drake'), (2, 'kurt wagner'), (3, 'max eisenhardt'), (4, 'kitty pryde')]
</codebox>
			<codebox char_offset="411" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unpack and print the tuple pairs
for index1, value1 in enumerate(mutants):
    print(index1, value1)
# Output
# 0 charles xavier
# 1 bobby drake
# 2 kurt wagner
# 3 max eisenhardt
# 4 kitty pryde

# Change the start index
for index2, value2 in enumerate(mutants, start=1):
    print(index2, value2)


</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Zip" prog_lang="custom-colors" readonly="False" tags="zip" ts_creation="1542458149.6" ts_lastsave="1543918226.66" unique_id="11">
			<rich_text scale="h1">Zip</rich_text>
			<rich_text>

Zip accepts an arbitrary number of iterables as its argument and returns a special zip object, which is also </rich_text>
			<rich_text weight="heavy">an iterator of tuples</rich_text>
			<rich_text>. Zip itself </rich_text>
			<rich_text weight="heavy">is an iterable</rich_text>
			<rich_text>.

Zip objects can be convert to a list of tuples.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

They can also be iterated over in a for loop.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The zip object can be unpacked using the splat operator *.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

But this will exhaust the zip object and we have to create it again.

</rich_text>
			<codebox char_offset="214" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">mutants = ['charles xavier', 'bobby drake' ...]
aliases = ['prof x', 'iceman', ...]
powers = ['telepathy','thermokinesis',...]

# Create a list of tuples: mutant_data
mutant_data = list(zip(mutants, aliases, powers))

# Print the list of tuples
print(mutant_data)

# [('charles xavier', 'prof x', 'telepathy'), ('bobby drake', 'iceman', 'thermokinesis'), ('kurt wagner', 'nightcrawler', 'teleportation'), ('max eisenhardt', 'magneto', 'magnetokinesis'), ('kitty pryde', 'shadowcat', 'intangibility')]</codebox>
			<codebox char_offset="264" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unpack the zip object and print the tuple values
for value1,value2,value3 in mutant_zip:
    print(value1, value2, value3)
</codebox>
			<codebox char_offset="327" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a zip object from mutants and powers: z1
z1 = zip(mutants,powers)

# Print the tuples in z1 by unpacking with *
print(*z1)

# ('charles xavier', 'telepathy') ('bobby drake', 'thermokinesis') ('kurt wagner', 'teleportation') ('max eisenhardt', 'magnetokinesis') ('kitty pryde', 'intangibility')
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="List Comprehension" prog_lang="custom-colors" readonly="False" tags="list comprehension" ts_creation="1542458171.07" ts_lastsave="1542461873.74" unique_id="12">
			<rich_text scale="h1">Basic list comprehension</rich_text>
			<rich_text>

The list comprehension syntax generates a list </rich_text>
			<rich_text weight="heavy">from an iterable object</rich_text>
			<rich_text>. It applies a certain action to each member of the list. The output of that action would be the members of the new list.

List comprehension </rich_text>
			<rich_text weight="heavy">constructs the entire list and stores it in memory</rich_text>
			<rich_text>.

[ &lt;output expression&gt; for &lt;iterator variable&gt; in &lt;iterable&gt; ]

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

List comprehension are more efficient than for loops.

List comprehensions can replace nested for loops as well.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Advanced list comprehension</rich_text>
			<rich_text>

Conditionals can be added on at the iterable side.

[ &lt;output expression&gt; for &lt;iterator variable&gt; in &lt;iterable&gt; </rich_text>
			<rich_text weight="heavy">if &lt;condition&gt;</rich_text>
			<rich_text>]

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Or they can be added at the output expression side.

[ &lt;output expression&gt; </rich_text>
			<rich_text weight="heavy">if &lt;condition&gt;</rich_text>
			<rich_text> for &lt;iterator variable&gt; in &lt;iterable&gt;]

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="354" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create list comprehension: squares
squares = [i*i for i in range(0,10)]</codebox>
			<codebox char_offset="471" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">pairs_2 = [(num1, num2) for num1 in range(0,2) for num2 in range(6,8)
print(pairs_2)
# Output
# [(0, 6), (0, 7), (1, 6), (1, 7)]
</codebox>
			<codebox char_offset="632" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

# Create list comprehension: new_fellowship
new_fellowship = [member for member in fellowship if len(member)&gt;= 7]
</codebox>
			<codebox char_offset="765" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create list comprehension: new_fellowship
new_fellowship = [member if len(member)&gt;=7 else &quot;&quot; for member in fellowship]
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Generators" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542458177.9" ts_lastsave="1542462803.86" unique_id="13">
			<rich_text scale="h1">Generators from list comprehension syntax</rich_text>
			<rich_text> 

A basic generator can be obtained using the same syntax as list comprehension, except that we use () instead of [].

This creates a generator object which can be iterated over in a for loop. 

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The next() function can also be applied to generator objects. Lazy evaluation - helps when working with large sequences and we don't want to store the entire sequence memory.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Conditional expressions that work for list comprehension also apply for generators.


</rich_text>
			<rich_text scale="h1">Generators from generator functions</rich_text>
			<rich_text>

Generator functions produce generator objects and are defined using def just like functions. instead of using return, we used yield in a generator function.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>




</rich_text>
			<codebox char_offset="237" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings: lannister
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Create a generator object: lengths
lengths = (len(person) for person in lannister)

# Iterate over and print the values in lengths
for value in lengths:
    print(value)
</codebox>
			<codebox char_offset="416" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create generator object: result
result = (num for num in range(0,31))

# Print the first 5 values
print(next(result))
print(next(result))
print(next(result))
print(next(result))
print(next(result))

# Print the rest of the values
for value in result:
    print(value)
</codebox>
			<codebox char_offset="700" frame_height="325" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of strings
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Define generator function get_lengths
def get_lengths(input_list):
    &quot;&quot;&quot;Generator function that yields the
    length of the strings in input_list.&quot;&quot;&quot;

    # Yield the length of a string
    for person in input_list:
        yield(len(person))

# Print the values generated by get_lengths()
for value in get_lengths(lannister):
    print(value)
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Importing Data in Python Part 1" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542554806.19" ts_lastsave="1542987986.82" unique_id="14">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Numpy" prog_lang="custom-colors" readonly="False" tags="numpy data import" ts_creation="1542554816.8" ts_lastsave="1542555952.79" unique_id="15">
			<rich_text scale="h1">Basic numpy data import</rich_text>
			<rich_text>

Importing as a numpy array. The object data is of type numpy.ndarray

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

delimiter: what delimiter is used for the data. Tab is \t
skiprows: how many header rows to skip at the start
usecols: which columns to import - in the example above, we are importing the first and third columns
dtype: what data type to use for all the data - in  the example above everything is imported as strings

Numpy is fine if all data is of the same type, but loadtxt breaks down if data is of mixed type.

</rich_text>
			<rich_text scale="h1">genfromtxt function</rich_text>
			<rich_text>

The numpy.genfromtxt() function is preferable for mixed type data.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

names: whether there is a header row.
dtype: if set to None, genfromtxt will figure out what each column should be

Here the object data is a structured array, where each element of this 1D array represents a row of the data

data[&lt;number&gt;] accesses a row
data[&lt;column label&gt;] accesses a column

</rich_text>
			<rich_text scale="h1">recfromcsv function</rich_text>
			<rich_text>

There is another function numpy.recfromcsv() that behaves similar to genfromtxt(). Its default arguments are as above in the genfromtxt() example, so we do not need to pass recfromcsv() any further arguments. The default dtype is None.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="95" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import numpy as np
filename = &quot;file.txt&quot;
data = np.loadtxt(file, delimiter=&quot;,&quot;, skiprows=1, usecols=[0,2], dtype=str)</codebox>
			<codebox char_offset="602" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)</codebox>
			<codebox char_offset="1159" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">file = 'titanic.csv'
d=np.recfromcsv(file)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Pandas" prog_lang="custom-colors" readonly="False" tags="pandas flat file import" ts_creation="1542554823.12" ts_lastsave="1543919794.11" unique_id="16">
			<rich_text scale="h1">Basic pandas data import</rich_text>
			<rich_text>

Flat files can be imported in pandas.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

nrows: number of rows to import
header: if set to None, there is no header row in the flat file. If set to 0, there is a header row and its column names are taken as default. The default behaviour is 0. Otherwise, the column names need to be specified in an argument names, a list of column names to use in the resulting data frame.
sep: field separator if not comma
comment: comment symbol, if used in the data frame
na_values: string used in flat file to indicate NA or NaN

names: this is a list of strings for the column names, if we should decide to change the default ones provided in the csv file
delimiter: this is a string that specifies the delimiter string..
</rich_text>
			<rich_text weight="heavy">index_col: when set to 0, use zero indexing????</rich_text>
			<rich_text>
</rich_text>
			<rich_text weight="heavy">na_values: specifies what string represents na values. Q: what if different columns have different NA values?</rich_text>
			<rich_text> solved see text
parse_dates: which columns for the year, month, day information. can be set to Boolean as well

</rich_text>
			<rich_text weight="heavy">Q: what's the difference between delimiter and sep?</rich_text>
			<rich_text> A: stackoverflow says it's the same, but we should stick to sep for backward compatibility.


</rich_text>
			<rich_text scale="h1">Saving data</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

index: boolean value. Set to False to avoid saving the index column</rich_text>
			<codebox char_offset="65" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas as pd
import pandas as pd

# Assign the filename: file
file = 'titanic.csv'

# Read the file into a DataFrame: df
df = pd.read_csv(file, nrows=5, header=None, sep=&quot;\t&quot;, comment=&quot;#&quot;, na_values=&quot;Nothing&quot;)

</codebox>
			<codebox char_offset="1167" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Save both DataFrames to csv files
gapminder.to_csv(&quot;gapminder.csv&quot;)
gapminder_agg.to_csv(&quot;gapminder_agg.csv&quot;)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="pickle" prog_lang="custom-colors" readonly="False" tags="pickle" ts_creation="1542713565.79" ts_lastsave="1542715341.59" unique_id="18">
			<rich_text scale="h1">Pickle</rich_text>
			<rich_text>

Loading data with pickle

The pickle module allows us to save Python objects that cannot usually be easily saved in text format.


</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Question: How to save objects in pickle format?</rich_text>
			<codebox char_offset="139" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pickle package
import pickle

# Open pickle file and load data: d
with open('data.pkl', &quot;rb&quot;) as file:
    d = pickle.load(file)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Excel" prog_lang="custom-colors" readonly="False" tags="excel data import" ts_creation="1542713007.11" ts_lastsave="1543918892.59" unique_id="17">
			<rich_text scale="h1">Excel</rich_text>
			<rich_text>

Loading data from excel files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Alternative way to read excel files using pandas from Importing Data in Python Part 2

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Saving data</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

index: boolean value. Set to False to avoid saving the index column


</rich_text>
			<codebox char_offset="38" frame_height="520" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas
import pandas as pd

# Load spreadsheet: xl
xl = pd.ExcelFile(&quot;battledeath.xlsx&quot;)

# Print sheet names - this will return a list of strings
print(xl.sheet_names)

# Load a sheet into a DataFrame by name: df1 - this assumes that there is a sheet named &quot;2004&quot;
# skiprows is a list of indices of the rows to skip
# names is a list of the names we assign to the columns imported
df1 = xl.parse(&quot;2004&quot;, skiprows=[0], names=[&quot;Country&quot;,&quot;AAM due to War (2004)&quot;])

# Print the head of the DataFrame df1
print(df1.head())

# Parse the first column of the second sheet and rename the column: df2
# parse_cols is a list of the indices of the columns to parse. Here names give the name assigned to this column
df2 = xl.parse(&quot;2004&quot;, parse_cols=[0], skiprows=[0], names=[&quot;Country&quot;])

</codebox>
			<codebox char_offset="128" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import pandas as pd
url = 'http://my.url/data.xls'
xl = pd.read_excel(url,sheetname=None)
print(xl.keys()) # prints the sheetnames - individual sheets can be referenced as if xl was a dictionary</codebox>
			<codebox char_offset="145" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Save the cleaned up DataFrame to an excel file without the index
df2.to_excel('file_clean.xlsx', index=False)</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="SAS and Stata" prog_lang="custom-colors" readonly="False" tags="SAS Stata" ts_creation="1542713903.03" ts_lastsave="1542715304.68" unique_id="19">
			<rich_text scale="h1">SAS and Stata</rich_text>
			<rich_text>

Here's how we import SAS files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



Here's how we import Stata files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="47" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pandas has already been imported

# Import sas7bdat package
from sas7bdat import SAS7BDAT

# Save file to a DataFrame: df_sas - this is a pandas data frame
with SAS7BDAT('sales.sas7bdat') as file:
    df_sas=file.to_data_frame()
</codebox>
			<codebox char_offset="86" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import pandas
import pandas as pd

# Load Stata file into a pandas DataFrame: df
df = pd.read_stata(&quot;disarea.dta&quot;)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="HDF5" prog_lang="custom-colors" readonly="False" tags="HDF5" ts_creation="1542714323.61" ts_lastsave="1542800518.61" unique_id="20">
			<rich_text scale="h1">HDF5</rich_text>
			<rich_text>

Here's how we import HDF5 files

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Data is stored in separate sections and we can view them as if it were a dictionary. Certain sections may contain a “table of content” of sorts for the data set.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Once we have identified the data we want, we can access it through its label. This subsection will have sub-keys.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="39" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import the h5py module
import h5py

# Load file: data
data = h5py.File(&quot;LIGO_data.hdf5&quot;,&quot;r&quot;)
</codebox>
			<codebox char_offset="206" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print the keys of the file
for key in data.keys():
    print(key)

</codebox>
			<codebox char_offset="325" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">group=data[&quot;strain&quot;]

# Check out keys of group
for key in group.keys():
    print(key)

# Set variable equal to time series data: strain
# strain can be plotted
strain=data['strain'][&quot;Strain&quot;].value
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="SQL" prog_lang="custom-colors" readonly="False" tags="sql" ts_creation="1542800512.04" ts_lastsave="1543084039.51" unique_id="21">
			<rich_text scale="h1">SQL</rich_text>
			<rich_text>

Common DB types: Postgresql, MySQL, SQLite, 

This is how we open the database, run a basic query, and import it into a pandas data frame.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Fetch only a certain number of rows.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Running queries from pandas</rich_text>
			<rich_text>

We can alternatively run the query from pandas

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>







WHat is the pro and cons of using the pandas way to execute a SQL query?

Revise INNER JOIN, OUTER JOIN and different types of joins</rich_text>
			<codebox char_offset="145" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import necessary module
from sqlalchemy import create_engine
from pandas import pd

# Create engine: engine
engine = create_engine(&quot;sqlite:///Chinook.sqlite&quot;)

# a list of table names in the DB can be accessed through the table_names() method
table_names = engine.table_names()

con = engine.connect()
# We can replace the above line with a context manager with statement
# with engine.connect() as con:
#     ....
rs = con.execute(&quot;SELECT * FROM Table_Name&quot;)
df = pd.DataFrame(rs.fetchall())
df.columns = rs.keys() # Set the column names to be the same as that in the DB
con.close()</codebox>
			<codebox char_offset="187" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df = pd.DataFrame(rs.fetchmany(size=5))</codebox>
			<codebox char_offset="267" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df = pd.read_sql_query(&quot;SELECT * FROM Table&quot;, engine)
</codebox>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Basic SQL Syntax" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543082608.26" ts_lastsave="1543084021.69" unique_id="28">
				<rich_text scale="h1">Basic SQL Syntax</rich_text>
				<rich_text>

</rich_text>
				<rich_text justification="left"></rich_text>
				<codebox char_offset="18" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">-- Selects all records in all columns from table
SELECT * FROM Table_Name;

-- Selects only certain columns from the table
SELECT col1, col2, col3 FROM Table_Name;

-- Display only those records which satisfies a certain condition on a certain column
SELECT * FROM Table_Name WHERE col1 &gt;= 6;

-- Select all records but sort them by a certain column
SELECT * FROM Employee ORDER BY BirthDate;

-- Performs inner join. Displays only certain columns selected from two tables. Specifies which column to use as the joining key
SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID;

-- Another example of multiple table query
SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId=Track.TrackID WHERE Milliseconds &lt; 250000;</codebox>
			</node>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Importing Data in Python Part 2" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542988073.09" ts_lastsave="1543084212.47" unique_id="22">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="urllib" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542988172.07" ts_lastsave="1543082459.11" unique_id="23">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Importing flat/data files directly from the web</rich_text>
			<rich_text>

Using urllib
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Or directly using pandas

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Importing excel files can be done as well

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="63" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">from urllib.requests import urlretrieve
url = &quot;http://some.url/data.csv&quot;
urlretrieve(url, &quot;localdata.csv&quot;)
</codebox>
			<codebox char_offset="92" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">df=pd.read_csv(url,sep=&quot;;&quot;)</codebox>
			<codebox char_offset="138" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import pandas as pd
url = 'http://my.url/data.xls' # this can be other supported protocols such as ftp. Local file can be referenced with file:///
xl = pd.read_excel(url,sheetname=None)
print(xl.keys()) # prints the sheetnames - individual sheets can be referenced as if xl was a dictionary</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="HTML requests" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542989430.82" ts_lastsave="1543082474.39" unique_id="24">
			<rich_text scale="h1">HTTP GET requests</rich_text>
			<rich_text>

Using urllib

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Using requests module

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The response object also has a json() method to parse any API JSON response.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="33" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">from urllib.request import urlopen, Request
url = &quot;http://my.url&quot;
request = Request(url)
response = urlopen(request)
html = response.read()
response.close()</codebox>
			<codebox char_offset="59" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import requests
url = &quot;http://my.url&quot;
r = requests.get(url)
text = r.text</codebox>
			<codebox char_offset="140" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">json_data = r.json()
#json_data is a dictionary and can be accessed like key-value pairs
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="BeautifulSoup" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1542990129.82" ts_lastsave="1543082491.31" unique_id="25">
			<rich_text scale="h1">Using BeautifulSoup</rich_text>
			<rich_text>

Starting </rich_text>
			<rich_text link="node 25">BeautifulSoup</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Then we can do things to the soup.

Prettify it

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Get the title

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Get the text

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Find tags

</rich_text>
			<rich_text justification="left"></rich_text>
			<codebox char_offset="45" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">from bs4 import BeautifulSoup
import requests
url = &quot;http://my.url&quot;
r = requests.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc)</codebox>
			<codebox char_offset="97" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># pretty soup is a properly indented soup object
pretty_soup = soup.prettify()</codebox>
			<codebox char_offset="115" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># this string contains the &lt;title&gt; tags
title = soup.title</codebox>
			<codebox char_offset="132" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This string contains no tags, only the textual part of the page
text = soup.text</codebox>
			<codebox char_offset="146" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">a_tags = soup.find_all('a') # finds all a tags -usually these will have urls
# a_tags is of the type ResultSet - this is an iterable
# link is a Tag object. Printing it gives the results together with the tags
# calling the method get('href') filters the result to give only those with href
for link in a_tags:
    print(link.get('href'))</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="JSON" prog_lang="custom-colors" readonly="False" tags="json" ts_creation="1543079116.75" ts_lastsave="1543082504.53" unique_id="26">
			<rich_text scale="h1">Importing data from JSON objects</rich_text>
			<rich_text>

JSON  stands for Javascript Object Notation

Has a key value pair structure and is text readable. The value themselves can in turn be json objects.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The requests module also has a method for the response object to  turn JSON responses into key value pairs. See the </rich_text>
			<rich_text link="node 24">HTML requests node</rich_text>
			<rich_text>.

</rich_text>
			<codebox char_offset="183" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import json
# json_data is a dictionary
with open('file.json', 'r') as json_file:
    json_data = json.load(json_file)

</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="APIs" prog_lang="custom-colors" readonly="False" tags="API" ts_creation="1543080225.62" ts_lastsave="1543084190.58" unique_id="27">
			<rich_text scale="h1">Miscellaneous information about APIs</rich_text>
			<rich_text>

REST stands for Representational State Transfer


Authenticating to Twitter API

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="119" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import tweepy, json
access_token = &quot;...&quot;
access_token_secret = &quot;...&quot;
consumer_key = &quot;...&quot;
consumer_secret = &quot;...&quot;
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Cleaning Data in Python" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543084190.59" ts_lastsave="1543853953.06" unique_id="29">
		<rich_text>


</rich_text>
		<rich_text underline="single" weight="heavy">Common problems of unclean data</rich_text>
		<rich_text>

• Inconsistent column names
• Missing data - either drop them, fill them up or leave as it is
• Outliers
• Duplicate rows
• Need to process columns
• Column types signal unexpected values

</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Useful methods for data inspection" prog_lang="custom-colors" readonly="False" tags="inspecting data" ts_creation="1543084457.13" ts_lastsave="1544000921.04" unique_id="30">
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="2" frame_height="1075" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># assuming df is a pandas data frame

# Check out the first five rows of data
print(df.head())

# Check out the last five rows of data
print(df.tail())

# print the number of rows and columns of this data set in (rows, cols) format - note that this is not a method
print(df.shape)

# print the names of the columns - note that this is not a method
print(df.columns)

# print out info about the types of objects in each column, how many nulls there are etc
print(df.info())

# print out summary statistics such as count, mean, median, std, max, quartiles for numeric columns
df.describe()

# print the count of occurences for various categories, including NA
# applies to categorical data only
print(df['Column Name'].value_counts(dropna=False))

# Prints the row indices of the data frame
print(df.index)

# Counts the number of entries - returns a series
# Can have a list of column names in the square brackets as well
df[&quot;colname&quot;].count())

# Calculates the mean/median/std dev/min/max
# if [] is left out, this computes the mean/median/std dev/min/max of all numeric columns
# there is an axis option on how these should be computed - axis=&quot;column&quot; for calculating across the row
df[&quot;colname&quot;].mean()
df[&quot;colname&quot;].median()
df[&quot;colname&quot;].std()
df[&quot;colname&quot;].min()
df[&quot;colname&quot;].max()

# Computes the 25% quantile for all numeric columns
# Computes the 25% and 75% quantile for all numeric columns
df.quantile(0.25)
df.quantile([0.25, 0.75])

# prints the unique entries in this column
# counts the number of times each unique entry occurs
df.[&quot;colname&quot;].unique()
df.[&quot;colname&quot;].nunique()

</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Melting, Pivoting and Splitting" prog_lang="custom-colors" readonly="False" tags="melting, pivoting, splitting" ts_creation="1543420088.28" ts_lastsave="1544432356.71" unique_id="31">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Melting</rich_text>
			<rich_text>

Melting - doing away with certain chosen columns and putting the values associated with those attributes into rows.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can assign names to the new columns of the melted data frame through var_name and value_name.
Or we can simply assign the names through the columns variable by passing a list.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Pivoting</rich_text>
			<rich_text>

This is the opposite of melting.
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that the above is not quite the original data frame - it's got a hierachical index aka Multiindex.

We can reset the index to get back what we want. This is covered in more depth in a later chapter.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Pivoting with aggregation function to deal  with duplicates. The default aggregation function used is np.mean if no aggfunc argument is specified.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text weight="heavy">Q: Does the function supplied to the pivot_table method need to be a numpy function?
Q: There are two ways to reference columns - either by df[&quot;colname&quot;] or df.colname. Are there situations where only one way works? Personally i prefer the former. WOn't there be ambiguity when the column name contains a dot ‘.’?
</rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Splitting strings</rich_text>
			<rich_text>

This is how we split strings and create new columns. Note how we apply the str attribute

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Split() and get() methods

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

More on melting and pivoting is covered </rich_text>
			<rich_text link="node 47">here</rich_text>
			<rich_text>.


</rich_text>
			<codebox char_offset="127" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># id_vars represents the columns that we DO NOT wish to melt
# value_vars represents the columns that we DO wish to melt
# by default, if no value_vars is defined, all columns not in id_vars will be melted
melted_frame = pd.melt(frame=unmelted_frame, id_vars = ['col1','col2'], value_vars = ['meltcol1','meltcol2'])
</codebox>
			<codebox char_offset="310" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Melt airquality: airquality_melt
airquality_melt = pd.melt(frame=airquality, id_vars=[&quot;Month&quot;,&quot;Day&quot;], var_name=&quot;measurement&quot;, value_name=&quot;reading&quot;)

airquality_melt.columns = [&quot;col1name&quot;, &quot;col2name&quot;, &quot;col3name&quot;]
</codebox>
			<codebox char_offset="356" frame_height="445" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># original data frame
#    Month  Day measurement  reading
# 0      5    1       Ozone     41.0
# 1      5    2       Ozone     36.0
# 2      5    3       Ozone     12.0
# 3      5    4       Ozone     18.0
# 4      5    5       Ozone      NaN


# Pivot airquality_melt: airquality_pivot
airquality_pivot = airquality_melt.pivot_table(index=[&quot;Month&quot;, &quot;Day&quot;], columns=&quot;measurement&quot;, values=&quot;reading&quot;)

# This transforms the data frame back into this
# measurement  Ozone  Solar.R  Temp  Wind
# Month Day                              
# 5     1       41.0    190.0  67.0   7.4
#       2       36.0    118.0  72.0   8.0
#       3       12.0    149.0  74.0  12.6
#       4       18.0    313.0  62.0  11.5
#       5        NaN      NaN  56.0  14.3
</codebox>
			<codebox char_offset="564" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Print the index of airquality_pivot
print(airquality_pivot.index)

# Reset the index of airquality_pivot: airquality_pivot_reset
airquality_pivot_reset = airquality_pivot.reset_index()

# Print the new index of airquality_pivot_reset
print(airquality_pivot_reset)
</codebox>
			<codebox char_offset="715" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pivot airquality_dup: airquality_pivot
airquality_pivot = airquality_dup.pivot_table(index=[&quot;Month&quot;,&quot;Day&quot;], columns=&quot;measurement&quot;, values=&quot;reading&quot;, aggfunc=np.mean)

# Reset the index of airquality_pivot
airquality_pivot = airquality_pivot.reset_index()
</codebox>
			<codebox char_offset="1144" frame_height="355" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Melt tb: tb_melt
tb_melt = pd.melt(frame=tb, id_vars=[&quot;country&quot;, &quot;year&quot;])

# Create the 'gender' column
tb_melt['gender'] = tb_melt.variable.str[0]

# Create the 'age_group' column
tb_melt['age_group'] = tb_melt.variable.str[1:]


# The result looks like this
#   country  year variable  value gender age_group
# 0      AD  2000     m014    0.0      m       014
# 1      AE  2000     m014    2.0      m       014
# 2      AF  2000     m014   52.0      m       014
# 3      AG  2000     m014    0.0      m       014
# 4      AL  2000     m014    2.0      m       014
</codebox>
			<codebox char_offset="1174" frame_height="430" frame_width="100" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="False"># Original data frame looks a bit like this (many columns omitted)
#         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone  \
#0    1/5/2015  289        2776.0            NaN            10030.0   
#1    1/4/2015  288        2775.0            NaN             9780.0   
#2    1/3/2015  287        2769.0         8166.0             9722.0   
#3    1/2/2015  286           NaN         8157.0                NaN   
#4  12/31/2014  284        2730.0         8115.0             9633.0   


# Melt ebola: ebola_melt
ebola_melt = pd.melt(ebola, id_vars=[&quot;Date&quot;, &quot;Day&quot;], var_name=&quot;type_country&quot;, value_name=&quot;counts&quot;)

# Create the 'str_split' column
# This also seems to work: ebola_melt['str_split'] = ebola_melt.type_country.str.split(&quot;_&quot;)
ebola_melt['str_split'] = ebola_melt[&quot;type_country&quot;].str.split(&quot;_&quot;)

# Create the 'type' column
ebola_melt['type'] = ebola_melt[&quot;str_split&quot;].str.get(0)

# Create the 'country' column
ebola_melt['country'] = ebola_melt[&quot;str_split&quot;].str.get(1)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Combining data" prog_lang="custom-colors" readonly="False" tags="merging" ts_creation="1543644784.14" ts_lastsave="1543917659.56" unique_id="32">
			<rich_text scale="h1">Concatenating rows and columns</rich_text>
			<rich_text>

Concatenating data - adding rows

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Adding columns - specify axis=1 as an argument. If omitted, axis=0, as is the case above.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Finding files</rich_text>
			<rich_text>

Finding files in the local workspace - first we find all files matching a certain pattern using glob.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Merging data frames</rich_text>
			<rich_text>

Merging data frames - this is in cases where two data frames have a common column we can use to merge the data.
left_on and right_on are the column names of the common column.
1-1, many-1, 1-many merging all use the same code. Think about different types of JOINs in SQL.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Many to one merging also uses the same syntax
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Here are the left, right and resulting data frames

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

For many-many merges, all duplicate keys will be created

</rich_text>
			<codebox char_offset="66" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># uber1, uber2 and uber3 are data frames
# pandas as been imported as pd
# Concatenate uber1, uber2, and uber3: row_concat
row_concat = pd.concat([uber1,uber2,uber3])

# To use running indices in the new data frame, add an ignore_index argument
row_concat = pd.concat([uber1,uber2,uber3], ignore_index=True)</codebox>
			<codebox char_offset="160" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Concatenate ebola_melt and status_country column-wise: ebola_tidy
ebola_tidy = pd.concat([ebola_melt,status_country],axis=1)
</codebox>
			<codebox char_offset="281" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import glob

# Write the pattern: pattern
pattern = '*.csv'

# Save all file matches: csv_files - this is a list of filenames which we can use for importing in pandas
csv_files = glob.glob(pattern)
</codebox>
			<codebox char_offset="579" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Merge the DataFrames: o2o
o2o = pd.merge(left=site, right=visited, left_on=&quot;name&quot;, right_on=&quot;site&quot;)
</codebox>
			<codebox char_offset="628" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Merge the DataFrames: m2o
m2o = pd.merge(left=site, right=visited, left_on=&quot;name&quot;, right_on=&quot;site&quot;)
</codebox>
			<codebox char_offset="683" frame_height="235" frame_width="30" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sh" width_in_pixels="False">
    name    lat    long
0   DR-1 -49.85 -128.57
1   DR-3 -47.15 -126.72
2  MSK-4 -48.87 -123.40
</codebox>
			<codebox char_offset="684" frame_height="235" frame_width="30" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="ada" width_in_pixels="False">
   ident   site       dated
0    619   DR-1  1927-02-08
1    622   DR-1  1927-02-10
2    734   DR-3  1939-01-07
3    735   DR-3  1930-01-12
4    751   DR-3  1930-02-26
5    752   DR-3         NaN
6    837  MSK-4  1932-01-14
7    844   DR-1  1932-03-22
</codebox>
			<codebox char_offset="685" frame_height="235" frame_width="39" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sh" width_in_pixels="False">    name    lat    long  ident   site       dated
0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08
1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10
2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22
3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07
4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12
5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26
6   DR-3 -47.15 -126.72    752   DR-3         NaN
7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Cleaning data" prog_lang="custom-colors" readonly="False" tags="cleaning data, converting types, NA, asserts, applying functions" ts_creation="1543678078.56" ts_lastsave="1543853250.54" unique_id="33">
			<rich_text scale="h1">Dtypes</rich_text>
			<rich_text>

We can view the types assigned to each column through the dtypes method. We can also test if the data is of a certain type.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>




</rich_text>
			<rich_text weight="heavy">Q: Categorical variables - what if there are additional variables not represented by the data frame. if we add new ones through row combine (say), would  it cause any error?</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: How to prevent NA categorical values from getting coded in their proper categories?</rich_text>
			<rich_text> Ans: have to look at data beforehand and recode them as NA

</rich_text>
			<rich_text weight="heavy">Q: For the function passed to np.apply(), what data type for the input argument is assumed? An entire row? Or individual element?</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: check_null_or_valid is row based function, but we're passing almost the entire data frame. How does this work?</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text weight="heavy">Q: Any way to specify integer or float when casting to numeric data type via pd.to_numeric? Sometimes that is useful as it helps with memory management.</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: .all() is still quite unclear</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: For previous chapter - how to extract data from websites that depend on javascript to display data? viewing the source does not show up the data, so cannot use beautifulsoup immediately. Is there any python package to help display this content?</rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="133" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">print(df.dtypes)

# Test if country is of type object
assert gapminder.country.dtypes == np.object

# Test if year is of type int64
assert gapminder.year.dtypes == np.int64

# Test if life_expectancy is of type float64
assert gapminder.life_expectancy.dtypes == np.float64
</codebox>
			<codebox char_offset="707" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Check whether the values in the row are valid
assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()
</codebox>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Casting data" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543831800.06" ts_lastsave="1543853139.63" unique_id="34">
				<rich_text scale="h1">Casting data</rich_text>
				<rich_text>

Casting columns of data as categoricals, to string and to numeric

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>
</rich_text>
				<codebox char_offset="81" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Convert the smoker column to type 'category'
tips.smoker = tips.smoker.astype(&quot;category&quot;)

#Converting to string type
df['treatment b'] = df['treatment b'].astype(str)

# converting to numeric
# coerce will return NaN when error is encountered. default behaviour is errors=&quot;raise&quot;, which is to raise an exception
 df['treatment a'] = pd.to_numeric(df['treatment a'], errors='coerce')</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Checking validity using RE and other string methods" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543831830.28" ts_lastsave="1544201844.83" unique_id="35">
				<rich_text scale="h1">Using regular expressions</rich_text>
				<rich_text>

Checking for valid strings using regular expressions.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>


We can check if the data satisfies a certain pattern.
</rich_text>
				<rich_text weight="heavy">Q: Why isn't there a need to import re?</rich_text>
				<rich_text>

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>



</rich_text>
				<rich_text scale="h1">String Methods</rich_text>
				<rich_text>

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Other methods include str.upper()

</rich_text>
				<codebox char_offset="82" frame_height="670" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import the regular expression module
import re

# Compile the pattern: prog
prog = re.compile('\d{3}-\d{3}-\d{4}')

# See if the pattern matches
result = prog.match('123-456-7890')
print(bool(result))

# -------------------------------------------------------------
# Alternatively, we can return all the matches through findall.
# matches is a list containing all the matched strings
# Find the numeric values: matches
matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana')

# Print the matches
print(matches)

# -------------------------------------------------------------
# Figuring out if the string matches a pattern
# Write the first pattern
pattern1 = bool(re.match(pattern='\d{3}-\d{3}-\d{4}', string='123-456-7890'))
print(pattern1)

# Write the second pattern
pattern2 = bool(re.match(pattern='\$\d+\.\d{2}', string='$123.45'))
print(pattern2)

# Write the third pattern
pattern3 = bool(re.match(pattern='[A-Z]\w*', string='Australia'))
print(pattern3)
</codebox>
				<codebox char_offset="181" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the series of countries: countries
countries = gapminder[&quot;country&quot;]

# Drop all the duplicates from countries
countries = countries.drop_duplicates()

# Write the regular expression: pattern
pattern = '^[A-Za-z\.\s]*$'

# Create the Boolean vector: mask
mask = countries.str.contains(pattern)

# Invert the mask: mask_inverse
mask_inverse = ~mask

# Subset countries using mask_inverse: invalid_countries
invalid_countries = countries.loc[mask_inverse]

# Print invalid_countries
print(invalid_countries)
</codebox>
				<codebox char_offset="202" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Strip extra whitespace from the column names: df.columns
df.columns = df.columns.str.strip()

# Extract data for which the destination airport is Dallas: dallas
dallas = df['Destination Airport'].str.contains(&quot;DAL&quot;)

# The index can be manipulated  as well
df.index = df.index.str.upper()
df.index = df.index.map(str.lower)
</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Using apply()" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543831862.78" ts_lastsave="1543853169.2" unique_id="36">
				<rich_text scale="h1">Using the apply() function</rich_text>
				<rich_text>

Using .apply() to recode data. We can specify a series, and pass a custom function meant to take in a single string as an argument.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

We can also use a built-in functions to apply across rows or columns

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

A more complex use of .apply, with a two-input function specified and an additional argument passed into the function.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Another complex application of apply(). 

It seems that g1800s.iloc[:, </rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>:].apply(check_null_or_valid, axis=</rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>) returns a data frame full of NaN and True. Within columns we typically see NaN mixed with Trues.
Why is it that g1800s.iloc[:, </rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>:].apply(check_null_or_valid, axis=</rich_text>
				<rich_text foreground="#ff0044">1</rich_text>
				<rich_text>).all() still give us all Trues? Are NaN not considered False?

</rich_text>
				<rich_text weight="heavy">Q: Why is there a need to drop the NAs?</rich_text>
				<rich_text>
</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>





</rich_text>
				<codebox char_offset="161" frame_height="550" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Define recode_gender()
def recode_gender(gender):

    # Return 0 if gender is 'Female'
    if gender == &quot;Female&quot;:
        return 0
    
    # Return 1 if gender is 'Male'    
    elif gender == &quot;Male&quot;:
        return 1
    
    # Return np.nan    
    else:
        return np.nan

# Apply the function to the sex column
tips['recode'] = tips.sex.apply(recode_gender)

# -------------------------------------------------------------
# Or make use of lambda functions
# Write the lambda function using replace
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))

# Write the lambda function using regular expressions
tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])
</codebox>
				<codebox char_offset="234" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Column wise
# Applies mean across all columns of data frame df - default behaviour axis=0
df.apply(np.mean, axis=0)

# Row wise
# Applies mean across all rows of data frame df - need to specify axis=1
df.apply(np.mean, axis=1)</codebox>
				<codebox char_offset="357" frame_height="505" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def diff_money(row, pattern):
 icost = row['Initial Cost']
 tef = row['Total Est. Fee']

 if bool(pattern.match(icost)) and bool(pattern.match(tef)):
 icost = icost.replace(&quot;$&quot;,
&quot;&quot;)
 tef = tef.replace(&quot;$&quot;,
&quot;&quot;)

 icost = float(icost)
 tef = float(tef)

 return icost - tef
 else:
 return(NaN)
 
df_subset['diff'] = df_subset.apply(diff_money, axis=1, pattern=pattern)

# Output data frame
# Job # Doc # Borough Initial Cost Total Est. Fee diff
#0 121577873 2 MANHATTAN $75000.00 $986.00 74014.0
#1 520129502 1 STATEN ISLAND $0.00 $1144.00 -1144.0
# 121601560 1 MANHATTAN $30000.00 $522.50 29477.5
#3 121601203 1 MANHATTAN $1500.00 $225.00 1275.0 </codebox>
				<codebox char_offset="738" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def check_null_or_valid(row_data):
    &quot;&quot;&quot;Function that takes a row of data,
    drops all missing values,
    and checks if all remaining values are greater than or equal to 0
    &quot;&quot;&quot;

    no_na = row_data.dropna()
    numeric = pd.to_numeric(no_na)
    ge0 = numeric &gt;= 0
    return ge0

# Check whether the first column is 'Life expectancy'
assert g1800s.columns[0] == &quot;Life expectancy&quot;

# Check whether the values in the row are valid
assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()

# Check that there is only one instance of each country
assert g1800s['Life expectancy'].value_counts()[0] == 1
</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Duplicates and NAs" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543832028.59" ts_lastsave="1544202194.63" unique_id="37">
				<rich_text scale="h1">Dealing with duplicates and NAs</rich_text>
				<rich_text>

Remove duplicate rows

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Dropping ANY row that has NA.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Replacing NAs in a certain column with the mean, or with our specified values

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

Dropping values

</rich_text>
				<rich_text justification="left"></rich_text>
				<codebox char_offset="56" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the new DataFrame: tracks
tracks = billboard[[&quot;year&quot;,&quot;artist&quot;,&quot;track&quot;,&quot;time&quot;]]

# Drop the duplicates: tracks_no_duplicates
tracks_no_duplicates = tracks.drop_duplicates()
</codebox>
				<codebox char_offset="90" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">tips_dropped = tips_nan.dropna()

# Drop rows that has ANY NaNs, i.e. drop as long as there is at least one NaN
df.dropna(how='any')

# use how=&quot;all&quot; for dropping rows with all NaN</codebox>
				<codebox char_offset="172" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Calculate the mean of the Ozone column: oz_mean
oz_mean = airquality[&quot;Ozone&quot;].mean()

# Replace all the missing values in the Ozone column with the mean
airquality['Ozone'] = airquality[&quot;Ozone&quot;].fillna(oz_mean)

tips_nan['sex'] = tips_nan['sex'].fillna('missing')

tips_nan[['total_bill', 'size']] = tips_nan[['total_bill','size']].fillna(0)</codebox>
				<codebox char_offset="192" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Remove the appropriate columns: df_dropped
# list_to_drop contains a list of column names to drop
df_dropped = df.drop(list_to_drop,axis = &quot;columns&quot;)

# Drop columns with less than 1000 non-missing values
titanic.dropna(thresh=1000, axis='columns')</codebox>
			</node>
			<node custom_icon_id="0" foreground="" is_bold="False" name="Using asserts to verify" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543832053.66" ts_lastsave="1543853196.83" unique_id="38">
				<rich_text scale="h1">Using asserts</rich_text>
				<rich_text>

Using asserts to test if cleaning is successful.

notnull() converts all entries in the data frame to a boolean. all() checks each column and returns True if and only if all entries in the column are True. In effect, notnull().all() returns a Series like object. The final all() takes this Series like object and return True if and only if all entries in this Series are True.

Instead of check for non-null entries, we can check if all entries are greater than or equal to zero. Just replace ebola.notnull() by ebola &gt;= 0.

</rich_text>
				<rich_text justification="left"></rich_text>
				<rich_text>

</rich_text>
				<codebox char_offset="540" frame_height="415" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Assert that there are no missing values
assert ebola.notnull().all().all()

# Assert that all values are &gt;= 0
assert (ebola &gt;= 0).all().all()

# Assert that country does not contain any missing values
assert pd.notnull(gapminder.country).all()

# Assert that year does not contain any missing values
assert pd.notnull(gapminder.year).all()

# Test if country is of type object
assert gapminder.country.dtypes == np.object

# Test if year is of type int64
assert gapminder.year.dtypes == np.int64

# Test if life_expectancy is of type float64
assert gapminder.life_expectancy.dtypes == np.float64
</codebox>
			</node>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="pandas Foundations" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543853929.51" ts_lastsave="1544201882.56" unique_id="39">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Investigating data frames" prog_lang="custom-colors" readonly="False" tags="numpy, pandas" ts_creation="1543918003.92" ts_lastsave="1544162047.62" unique_id="41">
			<rich_text scale="h1">Numpy and pandas</rich_text>
			<rich_text>

Numpy and pandas can interoperate

</rich_text>
			<rich_text justification="left"></rich_text>
			<codebox char_offset="53" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># df is a pandas data frame
# Create a numpy array of DataFrame values: np_vals - this will only affect numerical columns
# This returns a numpy array
np_vals = df.values

# Create new array of base 10 logarithm values: np_vals_log10
# This returns a numpy array
np_vals_log10 = np.log10(np_vals)

# Create array of new DataFrame by passing df to np.log10(): df_log10 - this will only affect numerical columns
# This returns a pandas data frame
df_log10 = np.log10(df)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on plotting" prog_lang="custom-colors" readonly="False" tags="matplotlib, plotting" ts_creation="1543918926.97" ts_lastsave="1545391531.1" unique_id="42">
			<rich_text>
</rich_text>
			<rich_text scale="h1">More on plotting</rich_text>
			<rich_text>

There are three different idioms for plotting. They have largely the same set of arguments, but be careful!
• df.plot(kind = “hist”)
• df.plt.hist()
• df.hist()


When .plot() is called without argument, line plot is assumed

Here are the arguments for the df.plot() function

x and y: to specify both when plotting a scatter plot
y: This is the y data, which can be a list. Applicable for hist and box.
color: For the color of the line/data point etc. Can be specified as “red”, “green”, etc.
subplots: Boolean. If set to True, all data series in the plot statement will be plotted as subplots, i.e. separately, on different graphs, separate axes.
style: for type of data marker e.g. “.-”, “.”
legend: Boolean - to show legend or not
kind: “scatter”, “box”, “hist” for various types of graphs
s: For scatter plots. This should equal the array containing the normalised sizes of the data points
alpha: transparency option when overlapping plot marks are present


plt.yscale(“log”) for logarithmic y axis


multiple plots of different types

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Previous related node on plotting </rich_text>
			<rich_text link="node 4">here...</rich_text>
			<rich_text>
Even more on plotting can be found </rich_text>
			<rich_text link="node 57">here</rich_text>
			<rich_text>.

A whole course on plotting can be found here in </rich_text>
			<rich_text link="node 60">More Data Visualisation</rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="1061" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This formats the plots such that they appear on separate rows
fig, axes = plt.subplots(nrows=2, ncols=1)

# Plot the PDF
df.fraction.plot(ax=axes[0], kind='hist', normed=True, bins=30, range=(0,.3))
plt.show()

# Plot the CDF
df.fraction.plot(ax=axes[1], kind=&quot;hist&quot;, normed=True, cumulative=True, bins=30, range=(0,.3))
plt.show()</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Date and Time" prog_lang="custom-colors" readonly="False" tags="DateTimeIndex, date, time" ts_creation="1544001509.24" ts_lastsave="1544903835.77" unique_id="43">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Date and Time manipulations</rich_text>
			<rich_text>

Date and Time are described in ISO 8601 format

yyyy-mm-dd hh:mm:ss

</rich_text>
			<rich_text scale="h1">Creating a DateTimeIndex</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Referencing with loc() and without</rich_text>
			<rich_text>

Various ways of indexing using loc(), and not using loc. Partial datetime string selection is fine.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Reindexing</rich_text>
			<rich_text>

Reindexing using the index of another data frame

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that indexes are non-mutable objects, i.e. they cannot be changed one at a time in place. However, if needed, the index can be replaced entirely

More on indexing is covered </rich_text>
			<rich_text link="node 46">here</rich_text>
			<rich_text>.

</rich_text>
			<rich_text scale="h1">Downsampling</rich_text>
			<rich_text>

The aggregation function is specified at the end.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Upsampling</rich_text>
			<rich_text>

ffill() for forward fill any missing data. bfill() for backward fill.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Moving quantities</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Localising to a timezone</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text weight="heavy">Q: for resample on week, when does the week start? </rich_text>
			<rich_text>
</rich_text>
			<rich_text weight="heavy">Q: It seems that df[&quot;col&quot;][&quot;date&quot;] or df[&quot;date&quot;][&quot;col&quot;] both work. Won't there be ambiguity sometimes?</rich_text>
			<rich_text>
</rich_text>
			<rich_text weight="heavy">Q: What if the date time index is not in order?</rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: Daily hours of clear sky. if .resample(&quot;D&quot;) has no aggregation method, what is the default? seems to be sum</rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="125" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># date_list is a normal python list containing date time strings in the format below

# Prepare a format string: time_format
time_format = &quot;%Y-%m-%d %H:%M&quot;

# Convert date_list into a datetime object: my_datetimes
my_datetimes = pd.to_datetime(date_list,format=time_format)  

# Construct a pandas Series using temperature_list and my_datetimes: time_series
time_series = pd.Series(temperature_list, index=my_datetimes)</codebox>
			<codebox char_offset="265" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Extract the hour from 9pm to 10pm on '2010-10-11': ts1
ts1 = ts0.loc['2010-10-11 21:00:00':'2010-10-11 22:00:00']

# Extract '2010-07-04' from ts0: ts2
ts2 = ts0.loc[&quot;2010-07-04&quot;]

# Extract data from '2010-12-15' to '2010-12-31': ts3
ts3 = ts0.loc[&quot;2010-12-15&quot;:&quot;2010-12-31&quot;]

# Extract temperature data for August: august
august = df.loc[&quot;2010-08&quot;][&quot;Temperature&quot;]

# Extract data from 2010-Aug-01 to 2010-Aug-15: unsmoothed
unsmoothed = df['Temperature'][&quot;2010-08-01&quot;:&quot;2010-08-15&quot;]
</codebox>
			<codebox char_offset="331" frame_height="505" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Reindex without fill method: ts3
ts3 = ts2.reindex(ts1.index)

# NOTE: if ts1.index contains indices that are not part of the original indices of ts2, these indices will be created in ts3 with NaN entries


# Reindex with fill method, using forward fill: ts4
ts4 = ts2.reindex(ts1.index,method=&quot;ffill&quot;)
# CAUTION! This might produce unexpected fill results if the fill index is a date but not a date time index

#----------------------------
# Reset the index of ts2 to ts1, and then use linear interpolation to fill in the NaNs: ts2_interp
ts2_interp = ts2.reindex(ts1.index).interpolate(how=&quot;linear&quot;)


#----------------------------
# Convert the 'Date' column into a collection of datetime objects: df.Date
df.Date = pd.to_datetime(df[&quot;Date&quot;])

# Set the index to be the converted 'Date' column
df.set_index(&quot;Date&quot;, inplace=True)

#----------------------------
# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate
# Sometimes merged frames may not have running indices
daily_temp_climate = daily_climate.reset_index()[&quot;Temperature&quot;]
</codebox>
			<codebox char_offset="585" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Downsample to 6 hour data and aggregate by mean: df1
df1 = df[&quot;Temperature&quot;].resample(&quot;6h&quot;).mean()

# Downsample to daily data and count the number of data points: df2
df2 = df[&quot;Temperature&quot;].resample(&quot;D&quot;).count()</codebox>
			<codebox char_offset="671" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">two_days.resample('4H').ffill()

population.resample('A').first()
population.resample('A').first().interpolate('linear')</codebox>
			<codebox char_offset="694" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Apply a rolling mean with a 24 hour window: smoothed
smoothed = unsmoothed.rolling(window=24).mean()

# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August
daily_highs_smoothed = daily_highs.rolling(window=7).mean()
</codebox>
			<codebox char_offset="724" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Combine two columns of data to create a datetime series: times_tz_none 
times_tz_none = pd.to_datetime(la[&quot;Date (MM/DD/YYYY)&quot;] + ' ' + la[&quot;Wheels-off Time&quot;] )

# Localize the time to US/Central: times_tz_central
times_tz_central = times_tz_none.dt.tz_localize(&quot;US/Central&quot;)

# Convert the datetimes from US/Central to US/Pacific
times_tz_pacific = times_tz_central.dt.tz_convert(tz=&quot;US/Pacific&quot;)

# Extracts only the hour
sales['Date'].dt.hour</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Manipulating Data Frames with pandas" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544201882.56" ts_lastsave="1544901148.28" unique_id="44">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Applying functions" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544201902.08" ts_lastsave="1544202440.64" unique_id="45">
			<rich_text scale="h1">Vectorised functions</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note: As far as possible, choose to use vectorised functions instead of map() or apply() when optimising for speed.

</rich_text>
			<rich_text scale="h1">Using map and a dictionary</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Vectorised function from another module</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<codebox char_offset="22" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># divide all numeric columns by 12 and take the floor of the quotient
df.floordiv(12)

# Does the same thing as above
np.floor_divide(df, 12)

def dozens(n):
    return n//12
df.apply(dozens)

df.apply(lambda n: n//12)

df['dozens_of_eggs'] = df.eggs.floordiv(12)</codebox>
			<codebox char_offset="170" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the dictionary: red_vs_blue
red_vs_blue = {&quot;Obama&quot;:&quot;blue&quot;,&quot;Romney&quot;:&quot;red&quot;}

# Use the dictionary to map the 'winner' column to the new column: election['color']
election['color'] = election[&quot;winner&quot;].map(red_vs_blue)
</codebox>
			<codebox char_offset="214" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import zscore from scipy.stats
from scipy.stats import zscore

# Call zscore with election['turnout'] as input: turnout_zscore
turnout_zscore = zscore(election[&quot;turnout&quot;])
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on indexing" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544342756.42" ts_lastsave="1544973362.13" unique_id="46">
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: Of what use is a multiindex?</rich_text>
			<rich_text> A: To have unique indices where this is not possible using one column
</rich_text>
			<rich_text weight="heavy">Q: Why is there no need to include slice() in Advanced Indexing - Indexing multilevels of a MultiIndex</rich_text>
			<rich_text>

Indexes are not mutable. i.e. we cannot simply set df.index[0] = var. However, we can change the entire index at once through df.index = somelist

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Indexing can be hierachical

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Referencing hierachical  indexing. Slice is still quite mysterious

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text weight="heavy">Q: Why is slice needed in the exercise example </rich_text>
			<rich_text family="monospace" weight="heavy">stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')), :]</rich_text>
			<rich_text weight="heavy"> but not needed here?</rich_text>
			<rich_text>

Reindexing based on date and time is briefly covered </rich_text>
			<rich_text link="node 43">here</rich_text>
			<rich_text>.
Setting the index at the time of data import is cover </rich_text>
			<rich_text link="node 5">here</rich_text>
			<rich_text>.</rich_text>
			<codebox char_offset="355" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># We can change the name of the index of the rows
# Assign the string 'MONTHS' to sales.index.name
sales.index.name = &quot;MONTHS&quot;

# We can change the name of the columns
# Assign the string 'PRODUCTS' to sales.columns.name 
sales.columns.name = &quot;PRODUCTS&quot;
</codebox>
			<codebox char_offset="387" frame_height="505" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This is the original data frame
#  state  month  eggs  salt  spam
#0    CA      1    47  12.0    17
#1    CA      2   110  50.0    31
#2    NY      1   221  89.0    72
#3    NY      2    77  87.0    20
#4    TX      1   132   NaN    52
#5    TX      2   205  60.0    55

# Now we set the multi index and sort
# Set the index to be the columns ['state', 'month']: sales
sales = sales.set_index(['state', 'month'])

# Sort the MultiIndex: sales
sales = sales.sort_index()

#                eggs  salt  spam
#    state month                  
#    CA    1        47  12.0    17
#          2       110  50.0    31
#    NY    1       221  89.0    72
#          2        77  87.0    20
#    TX    1       132   NaN    52
#          2       205  60.0    55
</codebox>
			<codebox char_offset="458" frame_height="565" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#                eggs  salt  spam
#    state month                  
#    CA    1        47  12.0    17
#          2       110  50.0    31
#    NY    1       221  89.0    72
#          2        77  87.0    20
#    TX    1       132   NaN    52
#          2       205  60.0    55

# Look up data for NY in month 1: NY_month1
NY_month1 = sales.loc[(&quot;NY&quot;,1)]

# Look up data for CA and TX in month 2: CA_TX_month2
CA_TX_month2 = sales.loc[([&quot;CA&quot;,&quot;TX&quot;],2),:]

# Look up data for all states in month 2: all_month2
all_month2 = sales.loc[(slice(None), 2 ),:]


#------------------------------------------
# We need this when we filter on an inner multiindex
#
# Create alias for pd.IndexSlice: idx
# From Merging Data Frames with pandas exercise
idx = pd.IndexSlice

# Print all the data on medals won by the United Kingdom
print(medals_sorted.loc[idx[:,&quot;United Kingdom&quot;],:])
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on Melting, Pivoting and Stacking" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544432300.47" ts_lastsave="1544435411.73" unique_id="47">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Pivoting</rich_text>
			<rich_text>

The difference between pivot and pivot_table is that for the latter, one can specify an aggregation function.

This is the original </rich_text>
			<rich_text family="monospace">users</rich_text>
			<rich_text> data frame

</rich_text>
			<rich_text family="monospace">  weekday    city  visitors  signups
0     Sun  Austin       139        7
1     Sun  Dallas       237       12
2     Mon  Austin       326        3
3     Mon  Dallas       456        5
</rich_text>
			<rich_text>
For the pivot function, usually the index, columns and values parameters are needed.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

If no values parameter is specified, both visitors and signups will be pivoted. In this case, we will see another two columns in addition to the above.

When we use pivot tables, an aggregation function can be specified through aggfunc. aggfunc=&quot;count&quot; for counting number of table entries. The margins parameter enables us to calculate the column total below

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text scale="h1">Stacking and unstacking</rich_text>
			<rich_text>

The concept of stacking is to put a columns into the rows, so that the data frame becomes “taller” (like stacking blocks).

</rich_text>
			<rich_text weight="heavy">Q: how is this different from melting?</rich_text>
			<rich_text>

This is the original </rich_text>
			<rich_text family="monospace">users</rich_text>
			<rich_text> data frame.

</rich_text>
			<rich_text family="monospace">                visitors  signups
city   weekday                   
Austin Mon           326        3
       Sun           139        7
Dallas Mon           456        5
       Sun           237       12
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

If we unstack city instead, we would get the cities under visitors and signup hierachy.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The level parameter can be numeric - this allows us to specify which level in the hierachy to stack or unstack.

The levels of the multiindex can be swapped. They need to be sorted so as to return them to a more appealing form.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Melting</rich_text>
			<rich_text>

Melting only seems to act on columns. It does not make any change to indices. New columns may be created.

This is the original data frame

</rich_text>
			<rich_text family="monospace">city     Austin  Dallas
weekday                
Mon         326     456
Sun         139     237
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Now let's act on this again

</rich_text>
			<rich_text family="monospace">  weekday    city  visitors  signups
0     Sun  Austin       139        7
1     Sun  Dallas       237       12
2     Mon  Austin       326        3
3     Mon  Dallas       456        5
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

var_name and value_name parameters can be specified so that we do not get variable and value column names.

We can use melting to obtain key value pairs

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



Melting and pivoting was briefly covered </rich_text>
			<rich_text link="node 31">here</rich_text>
			<rich_text>.</rich_text>
			<codebox char_offset="433" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Pivot the users DataFrame: visitors_pivot
visitors_pivot = users.pivot(index=&quot;weekday&quot;,columns=&quot;city&quot;,values=&quot;visitors&quot;)

# This gives
#city     Austin  Dallas
#weekday                
#Mon         326     456
#Sun         139     237</codebox>
			<codebox char_offset="797" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create the DataFrame with the appropriate pivot table: signups_and_visitors
signups_and_visitors = users.pivot_table(index=&quot;weekday&quot;,values=[&quot;signups&quot;,&quot;visitors&quot;], aggfunc=sum)


# Add in the margins: signups_and_visitors_total 
signups_and_visitors_total = users.pivot_table(index=&quot;weekday&quot;,values=[&quot;signups&quot;,&quot;visitors&quot;], aggfunc=sum, margins=True)
</codebox>
			<codebox char_offset="1236" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unstack users by 'weekday': byweekday
byweekday = users.unstack(level=&quot;weekday&quot;)

# We get this after running unstack
#        visitors      signups    
#weekday      Mon  Sun     Mon Sun
#city                             
#Austin       326  139       3   7
#Dallas       456  237       5  12

# Calling stack will return us to the original data frame</codebox>
			<codebox char_offset="1328" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Unstack users by 'city': bycity
bycity = users.unstack(level=&quot;city&quot;)

#        visitors        signups       
#city      Austin Dallas  Austin Dallas
#weekday                               
#Mon          326    456       3      5
#Sun          139    237       7     12
</codebox>
			<codebox char_offset="1560" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Swap the levels of the index of newusers: newusers
newusers = newusers.swaplevel(0,1)

# Sort the index of newusers: newusers
newusers = newusers.sort_index()
</codebox>
			<codebox char_offset="1810" frame_height="325" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Reset the index: visitors_by_city_weekday
visitors_by_city_weekday = visitors_by_city_weekday.reset_index() 

#city weekday  Austin  Dallas
#0        Mon     326     456
#1        Sun     139     237

# Melt visitors_by_city_weekday: visitors
visitors = pd.melt(visitors_by_city_weekday, id_vars=&quot;weekday&quot;, value_vars=[&quot;Austin&quot;,&quot;Dallas&quot;],value_name=&quot;visitors&quot;)

#  weekday    city  visitors
#0     Mon  Austin       326
#1     Sun  Austin       139
#2     Mon  Dallas       456
#3     Sun  Dallas       237</codebox>
			<codebox char_offset="2028" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Melt users: skinny
skinny = pd.melt(users, id_vars=[&quot;weekday&quot;,&quot;city&quot;],value_vars=[&quot;visitors&quot;,&quot;signups&quot;])

#  weekday    city  variable  value
#0     Sun  Austin  visitors    139
#1     Sun  Dallas  visitors    237
#2     Mon  Austin  visitors    326
#3     Mon  Dallas  visitors    456
#4     Sun  Austin   signups      7
#5     Sun  Dallas   signups     12
#6     Mon  Austin   signups      3
#7     Mon  Dallas   signups      5
</codebox>
			<codebox char_offset="2185" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set the new index: users_idx
users_idx = users.set_index([&quot;city&quot;,&quot;weekday&quot;])

# Obtain the key-value pairs: kv_pairs
kv_pairs = pd.melt(users_idx,col_level=0)

# Print the key-value pairs
print(kv_pairs)</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More on grouping and aggregations" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544606915.13" ts_lastsave="1545041300.92" unique_id="48">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Basic grouping</rich_text>
			<rich_text>

The sequence of application is typically this:

df.groupby(...)[ columns...].aggregationfunction

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Other aggregation functions we can use include sum(), mean(), std(), first(), last(), min(), max().
This is group all aggregations by pclass, reporting only the survived column. This counts the number of survived entries in each pclass. Multiple groups and multiple columns are possible.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can even group by category from a different data frame

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

sum(axis=&quot;columns&quot;) work as well, by automatically summing all rows in data frame

Aggregating different columns with different functions

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

In the example above, max works on age and median works on fare.

Functions must be enclosed in quotes. There are sum, mean and count functions.

</rich_text>
			<rich_text scale="h1">Custom aggregation</rich_text>
			<rich_text>

We can also define our own aggregation function. The input has to take in a series.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

agg allows us to be even more flexible about aggregation by specifying different aggregators and columns in a dict

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Transformations

Transforms is different from aggregation in that it returns one value for each entry, but the computation has some dependence on the group

We use the automobile dataset and define this zscore function

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

One use of transform is to fill in missing values. See lecture notes for more detail.

Apply makes for more complex computations. It can return a data frame

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Grouping and filtering</rich_text>
			<rich_text>

The groupby object is in fact a dictionary that can be iterated over

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can filter out groups of interest

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Using map to filter

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Others</rich_text>
			<rich_text>

Use of idxmax to find the index where the maximum of a series occurs

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

There is idxmin() as well.

</rich_text>
			<rich_text weight="heavy">Q: What is the difference between .agg, .transform and .apply?</rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="115" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group titanic by 'pclass'
by_class = titanic.groupby(&quot;pclass&quot;)

# Aggregate 'survived' column of by_class by count
count_by_class = by_class[&quot;survived&quot;].count()
</codebox>
			<codebox char_offset="407" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group titanic by 'embarked' and 'pclass'
by_mult = titanic.groupby([&quot;embarked&quot;,&quot;pclass&quot;])

# Aggregate both the bread and butter columns
sales.groupby('weekday')[['bread','butter']].sum()

#--------------------------------------
# We can groupby a function of the index as well
# Read file: sales
sales = pd.read_csv(&quot;sales.csv&quot;,index_col=&quot;Date&quot;,parse_dates=True)

# Create a groupby object: by_day
by_day = sales.groupby(sales.index.strftime(&quot;%a&quot;))
</codebox>
			<codebox char_offset="469" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Read life_fname into a DataFrame: life
life = pd.read_csv(life_fname, index_col='Country')

# Read regions_fname into a DataFrame: regions
regions = pd.read_csv(regions_fname, index_col=&quot;Country&quot;)

# Group life by regions['region']: life_by_region
life_by_region = life.groupby(regions[&quot;region&quot;])

# Print the mean over the '2010' column of life_by_region
print(life_by_region[&quot;2010&quot;].mean())</codebox>
			<codebox char_offset="611" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group titanic by 'pclass': by_class
by_class = titanic.groupby(&quot;pclass&quot;)

# Select 'age' and 'fare'
by_class_sub = by_class[['age','fare']]

# Aggregate by_class_sub by 'max' and 'median': aggregated
aggregated = by_class_sub.agg([&quot;max&quot;,&quot;median&quot;])
</codebox>
			<codebox char_offset="865" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def data_range(series):
...: return series.max() - series.min()

sales.groupby('weekday')[['bread', 'butter']].agg(data_range)</codebox>
			<codebox char_offset="984" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group gapminder by 'Year' and 'region': by_year_region
by_year_region = gapminder.groupby(level=[&quot;Year&quot;,&quot;region&quot;])

# Create the dictionary: aggregator
aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}

# Aggregate by_year_region using the dictionary: aggregated
aggregated = by_year_region.agg(aggregator)
</codebox>
			<codebox char_offset="1208" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def zscore(series):
 ...: return (series - series.mean()) / series.std()

# This computes the zscore for every automobile's mpg, but groups them by year, i.e. the comparison is with respect to other automobiles in the same year
auto.groupby('yr')['mpg'].transform(zscore).head()</codebox>
			<codebox char_offset="1369" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def zscore_with_year_and_name(group):
...: df = pd.DataFrame(
...: {'mpg': zscore(group['mpg']),
...: 'year': group['yr'],
...: 'name': group['name']})
...: return df

auto.groupby('yr').apply(zscore_with_year_and_name).head()

#mpg name year
#0 0.058125 chevrolet chevelle malibu 70
#1 -0.503753 buick skylark 320 70
#2 0.058125 plymouth satellite 70
#3 -0.316460 amc rebel sst 70
#4 -0.129168 ford torino 70</codebox>
			<codebox char_offset="1466" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">splitting = auto.groupby('yr')

for group_name, group in splitting:
...: avg = group['mpg'].mean()
...: print(group_name, avg) 
</codebox>
			<codebox char_offset="1507" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Group sales by 'Company': by_company
by_company = sales.groupby(&quot;Company&quot;)

# Filter 'Units' where the sum is &gt; 35: by_com_filt
by_com_filt = by_company.filter(lambda g:g[&quot;Units&quot;].sum() &gt; 35)
print(by_com_filt)</codebox>
			<codebox char_offset="1531" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">under10 = (titanic[&quot;age&quot;] &lt; 10).map({True:'under 10',False:'over 10'})

# Group by under10 and compute the survival rate
survived_mean_1 = titanic.groupby(under10)[&quot;survived&quot;].mean()
print(survived_mean_1)
</codebox>
			<codebox char_offset="1613" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#NOC        USA    URS
#Edition              
#1952     130.0  117.0
#1956     118.0  169.0
#1960     112.0  169.0
#1964     150.0  174.0
#1968     149.0  188.0
#1972     155.0  211.0
#1976     155.0  285.0
#1980       NaN  442.0
#1984     333.0    NaN
#1988     193.0  294.0

most_medals = cold_war_usa_urs_medals.idxmax(axis=&quot;columns&quot;)

#Edition
#1952    USA
#1956    URS
#1960    URS
#1964    URS
#1968    URS
#1972    URS
#1976    URS
#1980    URS
#1984    USA
1988    URS</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Merging Data Frames with pandas" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1544901131.31" ts_lastsave="1545034371.36" unique_id="49">
		<rich_text>
</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Dealing with multiple data frames" prog_lang="custom-colors" readonly="False" tags="merge, joins, concat" ts_creation="1544902109.55" ts_lastsave="1545041376.87" unique_id="50">
			<rich_text>
The .copy() method for dataframes can be used to create a copy of the dataframe and assign it to another

</rich_text>
			<rich_text weight="heavy">Q: There is a difference between using

weather3 = weather1.reindex(year,method=&quot;ffill&quot;)
and
weather3 = weather1.reindex(year).ffill()
</rich_text>
			<rich_text>

• Reindexing is a way to impose order to an unordered index
• Reindexing is a way to find common elements between two data frames
• df1.reindex(df2.index) is not equivalent to df2.reindex(df1.index)


</rich_text>
			<rich_text scale="h1">Simple Arithmetic</rich_text>
			<rich_text>

Arithmetics can be performed on data frames. It could be simple broadcasting.

The standard arithmetic operations would work on two data frames if they have the same column names, but if not, special methods are required

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

There is .multiply() as well

Concat can stack data frames verticallly and horizontally. Append can onlyl do vertically. The below are equivalent

</rich_text>
			<rich_text family="monospace">result1 = pd.concat([s1,s2,s3])
result2 = s1.append(s2).append(s3)
</rich_text>
			<rich_text>
Indices will be preserved. To reindex, we have to call .reset_index(drop=True) (for append) OR ignore_index=True (for concat)

</rich_text>
			<rich_text scale="h1">Using append</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

When we append two data frames with quite unlike indices and columns, the indices are combined. NAs are created.

population data frame

</rich_text>
			<rich_text family="monospace">2010            Census Population
Zip Code ZCTA
57538           322
59916           130
37660           40038
2860            45199 
</rich_text>
			<rich_text>
unemployment data frame

                </rich_text>
			<rich_text family="monospace">unemployment    participants
Zip
2860    0.11            34447
46167   0.02            4800
1097    0.33            42
80808   0.07            4310
</rich_text>
			<rich_text>
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The result is this. Note the repeated indices

</rich_text>
			<rich_text family="monospace">            2010 Census Population  participants    unemployment
57538       322.0                   NaN             NaN
59916       130.0                   NaN             NaN
37660       40038.0                 NaN             NaN
2860        45199.0                 NaN             NaN
2860        NaN                     34447.0         0.11
46167       NaN                     4800.0          0.02
1097        NaN                     42.0            0.33
80808       NaN                     4310.0          0.07
</rich_text>
			<rich_text>
Equivalently, this can be achieved by concat

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

If we had used axis=1, the repeated index 2860 will disappear. and we will have 7 rows.

This is an OUTER JOIN - this is the default join type. We could have specified </rich_text>
			<rich_text family="monospace">join=&quot;outer&quot;</rich_text>
			<rich_text>, but this is the default.

If we wanted INNER JOIN, i.e. only 2860 (common indices) would appear, we would specify </rich_text>
			<rich_text family="monospace">join=&quot;inner&quot;</rich_text>
			<rich_text>.

To summarise,
</rich_text>
			<rich_text weight="heavy">OUTER JOIN - common indices appear only once. Non-common indices also appear
INNER JOIN - only common indices appear
</rich_text>
			<rich_text>
</rich_text>
			<rich_text scale="h1">Using concat</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can also add a multilevel index during concat to tell apart different data frames. This can be added on the indices or on the columns.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can use dictionaries to concat

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Merge</rich_text>
			<rich_text>

This is INNER JOIN on column data. concat only works on indices, but merge enables us to join based on column information.

Merge does INNER JOIN by default. how=&quot;inner&quot; is the parameter.

If there are columns with repeated names, both columns will be included in the new data frame, with suffixes to disambiguate. Use suffixes=[&quot;..&quot;,&quot;..&quot;] to customise.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The column to merge on can have different names. left_on and right_on can be lists.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can merge on multiple columns

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Merge can do INNER, OUTER, LEFT and RIGHT joins by specifying the how parameter.

how: “inner”, “outer”, “left”, “right”

There are other forms of merge such as merge_ordered and merge_asof.

</rich_text>
			<rich_text weight="heavy">Q: What are these for?</rich_text>
			<rich_text>

An alternative syntax for joining is to use the .join method

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text weight="heavy">Q: There is a use of expanding() function in one of the final exercises. What is expanding().mean()? Its use is strange here. Are there other examples?</rich_text>
			<rich_text>







</rich_text>
			<codebox char_offset="686" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># This divides the numerical colums in week1_range by the numerical column in week1_mean. week1_range / week1_mean does not work here
week1_range.divide(week1_mean, axis='rows')

# Percentage change of successive rows
week1_mean.pct_change() * 100
</codebox>
			<codebox char_offset="691" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Bronze and silver do not have identical indices set. If added, we will get lots of NAs. This approach treats NAs as 0
bronze.add(silver, fill_value=0)</codebox>
			<codebox char_offset="1050" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Append feb_units and then mar_units to jan_units: quarter1
quarter1 = jan_units.append(feb_units).append(mar_units)</codebox>
			<codebox char_offset="1514" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">population.append(unemployment)</codebox>
			<codebox char_offset="2128" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">pd.concat([population, unemployment], axis=0)</codebox>
			<codebox char_offset="2588" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build the list of Series
for month in [jan, feb, mar]:
    units.append(month[&quot;Units&quot;])


# Concatenate the list: quarter1
# axis = 0 equivalently
quarter1 = pd.concat(units, axis=&quot;rows&quot;)
</codebox>
			<codebox char_offset="2730" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Concatenate medals: medals
medals = pd.concat(medals,keys = [&quot;bronze&quot;,&quot;silver&quot;,&quot;gold&quot;])

# Concatenate dataframes: february
february = pd.concat(dataframes, keys=[&quot;Hardware&quot;,&quot;Software&quot;,&quot;Service&quot;],axis=1)</codebox>
			<codebox char_offset="2768" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># rain2013 and rain2014 are data frames
rain_dict = {2013: rain2013, 2014: rain2014}
In [16]: rain1314 = pd.concat(rain_dict, axis='columns') </codebox>
			<codebox char_offset="3134" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">combined = pd.merge(revenue, managers, on='city')</codebox>
			<codebox char_offset="3222" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Merge revenue &amp; managers on 'city' &amp; 'branch': combined
combined = pd.merge(revenue,managers,left_on=&quot;city&quot;,right_on=&quot;branch&quot;)</codebox>
			<codebox char_offset="3259" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Merge revenue &amp; managers on 'branch_id', 'city', &amp; 'state': combined
combined = pd.merge(revenue,managers,on=[&quot;branch_id&quot;,&quot;city&quot;,&quot;state&quot;])
</codebox>
			<codebox char_offset="3540" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">population.join(unemployment, how= 'right')
# how can be &quot;left&quot;, &quot;right&quot;, &quot;inner&quot; or &quot;outer&quot;</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="SQL basics" prog_lang="custom-colors" readonly="False" tags="SQL" ts_creation="1545034371.37" ts_lastsave="1545065566.13" unique_id="51">
		<rich_text>
Basic SELECTing of columns from table, or multiple columns, or all columns

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Simply print something

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Get all the distinct entries in a column. Quite like .unique() function in pandas

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Count the number of rows in table, and the number of non-missing values in the column, or the number of distinct entries.

</rich_text>
		<rich_text weight="heavy">Q: Are there other syntaxes? </rich_text>
		<rich_text>

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Use of WHERE to filter. And we can also build up multiple conditions

Comparison operators include the following

</rich_text>
		<rich_text family="monospace">=, &lt;&gt;, &gt;, &lt;, &lt;=, &gt;=</rich_text>
		<rich_text>

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


Boolean operators include AND and OR. We can use parentheses to further specify our order.

field BETWEEN x AND y can be used in place of field &gt;= x AND field &lt;= y.

IN operator

Instead of this,

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

we can use IN as a shorthand for the same purpose.

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


NULL can be used in queries to represent unknown values. We can use it in WHERE field IS NULL or WHERE field IS NOT NULL.

LIKE and NOT LIKE

They can be used to match strings in WHERE clauses using _ and % characters. 
% will match zero or many characters (like unix *)
_ will match exactly one character (like unix ?)

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


There are simple aggregations we can use. This calculates and returns the average budget for all films.

Other aggregation functions include SUM, MAX, MIN, 

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

This can be combined with WHERE conditional clauses.

We can perform basic arithmetic operations.

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Aliasing

We can use aliases to disambiguate by naming the new columns we create.

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


Sorting and ordering

ORDER BY is used to sort columns according to alphabetical or numerical order. DESC can be added to the end to reverse the order.

We can also ORDER BY multiple columns.

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


Result aggregation using GROUP BY

The query here counts the number of male and female entries from the employees table. We can use aggregation functions like COUNT or MAX.

The GROUP BY operator should always come after FROM.

There can be multiple GROUP BY columns.

Warning: SQL will return an error if a field SELECTed is not in the GROUP BY clause.



</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


HAVING clause

If we want to filter on an aggregation result, we cannot use WHERE. Instead, we should use HAVING.

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Example of a super long query to show ordering of clauses

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


Performing basic INNER JOIN

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>





</rich_text>
		<codebox char_offset="77" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT name FROM people;

SELECT name, birthdate FROM people;

SELECT * FROM people</codebox>
		<codebox char_offset="104" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT 'string output' AS result;</codebox>
		<codebox char_offset="190" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT DISTINCT country FROM films;</codebox>
		<codebox char_offset="347" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT COUNT(*) FROM films;

SELECT COUNT(birthdate) FROM people;

SELECT COUNT(DISTINCT birthdate) FROM people;</codebox>
		<codebox char_offset="485" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT title FROM films WHERE release_year &gt; 2000;

SELECT title
FROM films
WHERE release_year &gt; 1994
AND release_year &lt; 2000;</codebox>
		<codebox char_offset="686" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT name
FROM kids
WHERE age = 2
OR age = 4
OR age = 6
OR age = 8
OR age = 10;</codebox>
		<codebox char_offset="741" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT name
FROM kids
WHERE age IN (2, 4, 6, 8, 10);</codebox>
		<codebox char_offset="1066" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT name
FROM companies
WHERE name LIKE 'Data%';

SELECT name
FROM companies
WHERE name LIKE 'DataC_mp';</codebox>
		<codebox char_offset="1228" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT AVG(budget)
FROM films;</codebox>
		<codebox char_offset="1330" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">-- This performs integer division
SELECT (10/3);

-- This performs floating point division
SELECT (4.0/3.0);</codebox>
		<codebox char_offset="1416" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT MAX(budget) AS max_budget,
       MAX(duration) AS max_duration
FROM films;

SELECT title, (gross - budget) AS net_profit FROM films;

-- get the count(deathdate) and multiply by 100.0
-- then divide by count(*)
SELECT COUNT(deathdate) * 100.0 / COUNT(*) AS percentage_dead FROM people;</codebox>
		<codebox char_offset="1613" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT title
FROM films
ORDER BY release_year DESC;

SELECT birthdate, name
FROM people
ORDER BY birthdate, name;
</codebox>
		<codebox char_offset="1974" frame_height="220" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT sex, count(*)
FROM employees
GROUP BY sex;

SELECT sex, count(*)
FROM employees
GROUP BY sex
ORDER BY count DESC;

SELECT release_year, country, MAX(budget) FROM films GROUP BY release_year, country ORDER BY release_year, country;</codebox>
		<codebox char_offset="2093" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT release_year
FROM films
GROUP BY release_year
HAVING COUNT(title) &gt; 10;</codebox>
		<codebox char_offset="2155" frame_height="325" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross FROM films WHERE release_year &gt; 1990 GROUP BY release_year HAVING AVG(budget) &gt; 60000000 ORDER BY avg_gross DESC;

-- select country, average budget, average gross
SELECT country, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross
-- from the films table
FROM films
-- group by country 
GROUP BY country
-- where the country has more than 10 titles
HAVING COUNT(title) &gt; 10
-- order by country
ORDER BY country
-- limit to only show 5 results
LIMIT 5;</codebox>
		<codebox char_offset="2188" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="sql" width_in_pixels="True">SELECT title, imdb_score
FROM films
JOIN reviews
ON films.id = reviews.film_id
WHERE title = 'To Kill a Mockingbird';</codebox>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Introduction to Databases in Python" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545041412.32" ts_lastsave="1545391161.59" unique_id="52">
		<rich_text>



</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Basic SQL" prog_lang="custom-colors" readonly="False" tags="sql, pandas, sqlalchemy" ts_creation="1545117902.06" ts_lastsave="1545389781.41" unique_id="53">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Starting connection to database</rich_text>
			<rich_text>

We can use SQLAlchemy

The connection isn't made until we make the first query

We can use Reflection to find out more about the tables - this makes understanding the tables easier later.

Checking out table names

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Note that connection.execute(stmt) is a ResultProxy object
connection.execute(stmt).fetchall() is a ResultSet object

ResultSet objects can be referenced

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Showing metadata</rich_text>
			<rich_text>

Idiom for showing metadata

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We initialise the metadata object like this

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text scale="h1">Pythonic queries for SQL statements</rich_text>
			<rich_text>

This is equivalent to SELECT * FROM census;

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

WHERE statement

</rich_text>
			<rich_text family="monospace">SELECT * FROM census WHERE state IN (&quot;State 1&quot;, “State2”, “State3”);</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

There are other methods including and_(), any_(), or_(), not_() ... but they need importing

</rich_text>
			<rich_text family="monospace">SELECT * FROM census WHERE state = “California” AND sex != “M”;</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text family="monospace">SELECT state FROM census ORDER BY state;</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

To use DESC just import desc and use it around the column name

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Ordering by multiple columns

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Functions and GROUP BY

Distinct count

</rich_text>
			<rich_text family="monospace">SELECT COUNT(DISTINCT state) FROM census;</rich_text>
			<rich_text>

.scalar() ensures that only the number is returned, not the entire row/column object

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text family="monospace">SELECT state, COUNT(age) FROM census GROUP BY state;</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text family="monospace">SELECT state, pop2008 AS population FROM census GROUP BY state;</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Integrating with pandas</rich_text>
			<rich_text>

Importing ResultSet into pandas

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>







</rich_text>
			<codebox char_offset="249" frame_height="265" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import create_engine
from sqlalchemy import create_engine

# Create an engine that connects to the census.sqlite file: engine
engine = create_engine(&quot;sqlite:///census.sqlite&quot;)

# Print table names
print(engine.table_names())

connection = engine.connect()

stmt = &quot;SELECT * FROM census&quot;

# Execute the statement and fetch the results: results
results = connection.execute(stmt).fetchall()

# Print results
print(results)</codebox>
			<codebox char_offset="407" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">first_row = results[0]
print(first_row)
# ('Illinois', 'M', 0, 89600, 95012)

print(first_row.keys())
#['state', 'sex', 'age', 'pop2000', 'pop2008']

print(first_row.state)
# 'Illinois'</codebox>
			<codebox char_offset="457" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import Table
from sqlalchemy import Table

# Reflect census table from the engine: census
census = Table(&quot;census&quot;, metadata, autoload=True, autoload_with=engine)

# Print census table metadata
print(repr(census))

# Print the column names
print(census.columns.keys())

# Print full table metadata
print(repr(metadata.tables[&quot;census&quot;]))</codebox>
			<codebox char_offset="505" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">metadata = MetaData()</codebox>
			<codebox char_offset="592" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">stmt = select([census])</codebox>
			<codebox char_offset="682" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a query for the census table: stmt
stmt = select([census])

# Append a where clause to match all the states in_ the list states
stmt = stmt.where(census.columns.state.in_(states))

for result in connection.execute(stmt):
    print(result.state, result.pop2000)
</codebox>
			<codebox char_offset="843" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">stmt = stmt.where(
    # The state of California with a non-male sex
    and_(census.columns.state == &quot;California&quot;,
         census.columns.sex != &quot;M&quot;
         )
)</codebox>
			<codebox char_offset="888" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a query to select the state column: stmt
stmt = select([census.columns.state])

# Order stmt by the state column
stmt = stmt.order_by(census.columns.state)

# Execute the query and store the results: results
results = connection.execute(stmt).fetchall()
</codebox>
			<codebox char_offset="955" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">rev_stmt = stmt.order_by(desc(census.columns.state))</codebox>
			<codebox char_offset="988" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a query to select state and age: stmt
stmt = select([census.columns.state, census.columns.age])

# Append order by to ascend by state and descend by age
stmt = stmt.order_by(census.columns.state, desc(census.columns.age))
</codebox>
			<codebox char_offset="1160" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a query to count the distinct states values: stmt
stmt = select([func.count(census.columns.state.distinct())])

# Execute the query and store the scalar result: distinct_state_count
distinct_state_count = connection.execute(stmt).scalar()

# Print the distinct_state_count
print(distinct_state_count)
</codebox>
			<codebox char_offset="1217" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a query to select the state and count of ages by state: stmt
stmt = select([census.columns.state, func.count(census.columns.age)])

# Group stmt by state
stmt = stmt.group_by(census.columns.state)
</codebox>
			<codebox char_offset="1285" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build an expression to calculate the sum of pop2008 labeled as population
pop2008_sum = func.sum(census.columns.pop2008).label(&quot;population&quot;)

# Build a query to select the state and sum of pop2008: stmt
stmt = select([census.columns.state, pop2008_sum])

# Group stmt by state
stmt = stmt.group_by(census.columns.state)
</codebox>
			<codebox char_offset="1347" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># import pandas
import pandas as pd

# Create a DataFrame from the results: df
df = pd.DataFrame(results)

# Set column names
df.columns = results[0].keys()
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="More SQL" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545120103.31" ts_lastsave="1545141760.15" unique_id="54">
			<rich_text>
Arithmetic operations are the same and we can just use them as in Python

+ - * / %

.limit(n) works to limit the number of returned rows to n

Complicated use of case and cast statements to calculate female population percentage

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



Joins

Simple inner join

SELECT pop2000, abbreviation FROM census JOIN state_fact ON ... = ...;

There is no need to specify joining relationship here because it is already defined in the database

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


SELECT * FROM census JOIN state_fact ON census.state = state_fact.name;

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

SELECT state, SUM(pop2000), census_division_name FROM census JOIN state_fact ON census.state = state_fact.name GROUP BY name;

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Tables can be self referential

The example given in the lecture is about an employee table with employee ids. However, each employee also has a manager who is in turn an employee. Every employee's manager is stored in a columns called manager with the manager's employee id.

It is useful to think of such situations as two tables when writing queries.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Counting the number of employees a manager has

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Fetching from large database

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="232" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build an expression to calculate female population in 2000
female_pop2000 = func.sum(
    case([
        (census.columns.sex == 'F', census.columns.pop2000)
    ], else_=0))

# Cast an expression to calculate total population in 2000 to Float
total_pop2000 = cast(func.sum(census.columns.pop2000), Float)

# Build a query to calculate the percentage of females in 2000: stmt
stmt = select([female_pop2000 / total_pop2000 * 100])

# Execute the query and store the scalar result: percent_female
percent_female = connection.execute(stmt).scalar()</codebox>
			<codebox char_offset="436" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a statement to join census and state_fact tables: stmt
stmt = select([census.columns.pop2000, state_fact.columns.abbreviation])

# Execute the statement and get the first result: result
result = connection.execute(stmt).first()</codebox>
			<codebox char_offset="513" frame_height="370" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a statement to select the census and state_fact tables: stmt
stmt = select([census, state_fact])

# Add a select_from clause that wraps a join for the census and state_fact
# tables where the census state column and state_fact name column match
stmt = stmt.select_from(
    census.join(state_fact, census.columns.state == state_fact.columns.name))

# Execute the statement and get the first result: result
result = connection.execute(stmt).first()

# Loop over the keys in the result object and print the key and value
for key in result.keys():
    print(key, getattr(result, key))
</codebox>
			<codebox char_offset="643" frame_height="370" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a statement to select the state, sum of 2008 population and census
# division name: stmt
stmt = select([
    census.columns.state,
    func.sum(census.columns.pop2000),
    state_fact.columns.census_division_name
])

# Append select_from to join the census and state_fact tables by the census state and state_fact name columns
stmt = stmt.select_from(
    census.join(state_fact, census.columns.state == state_fact.columns.name)
)

# Append a group by for the state_fact name column
stmt = stmt.group_by(state_fact.columns.name)
</codebox>
			<codebox char_offset="1001" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Make an alias of the employees table: managers
managers = employees.alias()

# Build a query to select manager's and their employees names: stmt
stmt = select(
    [managers.columns.name.label('manager'),
     employees.columns.name.label('employee')]
)

# Match managers id with employees mgr: stmt
stmt = stmt.where(managers.columns.id == employees.columns.mgr)

# Order the statement by the managers name: stmt
stmt = stmt.order_by(managers.columns.name)
</codebox>
			<codebox char_offset="1052" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a query to select managers and counts of their employees: stmt
stmt = select([managers.columns.name, func.count(employees.columns.id)])
</codebox>
			<codebox char_offset="1086" frame_height="385" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">while more_results:
    # Fetch the first 50 results from the ResultProxy: partial_results
    partial_results = results_proxy.fetchmany(50)

    # if empty list, set more_results to False
    if partial_results == []:
        more_results = False

    # Loop over the fetched records and increment the count for the state
    for row in partial_results:
        if row.state in state_count:
            state_count[row.state] += 1
        else:
            state_count[row.state] = 1

# Close the ResultProxy, and thus the connection
results_proxy.close()
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Making changes to tables" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545234838.96" ts_lastsave="1545237638.73" unique_id="55">
			<rich_text>
Creating a table with default values

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Inserting a row of data

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Inserting multiple entries

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Reading from csv and insert rows in batches

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Updating a row

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

If the where statement selects more clauses, we can update more rows

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Updating using the results from another query

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Deleting an entire table

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Deleting specific records

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Dropping tables 

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="39" frame_height="370" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import Table, Column, String, Integer, Float, Boolean from sqlalchemy
from sqlalchemy import Table, Column, String, Integer, Float, Boolean

# Define a new table with a name, count, amount, and valid column: data
data = Table('data', metadata,
             Column('name', String(255), unique=True),
             Column('count', Integer(), default=1),
             Column('amount', Float()),
             Column('valid', Boolean(), default=False)
)

# Use the metadata to create the table
metadata.create_all(engine)

# Print the table details
print(repr(metadata.tables['data']))

# Print table details
print(repr(data))</codebox>
			<codebox char_offset="68" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import insert and select from sqlalchemy
from sqlalchemy import insert, select

# Build an insert statement to insert a record into the data table: stmt
stmt = insert(data).values(name='Anna', count=1, amount=1000.00, valid=True)

# Execute the statement via the connection: results
results = connection.execute(stmt)

# Print result rowcount
print(results.rowcount)
</codebox>
			<codebox char_offset="99" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a list of dictionaries: values_list
values_list = [
    {'name': 'Anna', 'count': 1, 'amount': 1000.00, 'valid': True},
    {'name': 'Taylor', 'count': 1, 'amount': 750.00, 'valid': False}
]

# Build an insert statement for the data table: stmt
stmt = insert(data)

# Execute stmt with the values_list: results
results = connection.execute(stmt, values_list)

# Print rowcount
print(results.rowcount)</codebox>
			<codebox char_offset="147" frame_height="460" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a insert statement for census: stmt
stmt = insert(census)

# Create an empty list and zeroed row count: values_list, total_rowcount
values_list = []
total_rowcount = 0

# Enumerate the rows of csv_reader
for idx, row in enumerate(csv_reader):
    #create data and append to values_list
    data = {'state': row[0], 'sex': row[1], 'age': row[2], 'pop2000': row[3],
            'pop2008': row[4]}
    values_list.append(data)

    # Check to see if divisible by 51
    if idx % 51 == 0:
        results = connection.execute(stmt, values_list)
        total_rowcount += results.rowcount
        values_list = []

# Print total rowcount
print(total_rowcount)</codebox>
			<codebox char_offset="166" frame_height="370" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a select statement: select_stmt
select_stmt = select([state_fact]).where(state_fact.columns.name == &quot;New York&quot;)

# Print the results of executing the select_stmt
print(connection.execute(select_stmt).fetchall())

# Build a statement to update the fips_state to 36: stmt
stmt = update(state_fact).values(fips_state = 36)

# Append a where clause to limit it to records for New York state
stmt = stmt.where(state_fact.columns.name == 'New York')

# Execute the statement: results
results = connection.execute(stmt)

# Print rowcount
print(results.rowcount)
</codebox>
			<codebox char_offset="239" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a statement to update the notes to 'The Wild West': stmt
stmt = update(state_fact).values(notes='The Wild West')

# Append a where clause to match the West census region records
stmt = stmt.where(state_fact.columns.census_region_name == &quot;West&quot;)
</codebox>
			<codebox char_offset="289" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a statement to select name from state_fact: stmt
fips_stmt = select([state_fact.columns.name])

# Append a where clause to Match the fips_state to flat_census fips_code
fips_stmt = fips_stmt.where(
    state_fact.columns.fips_state == flat_census.columns.fips_code)

# Build an update statement to set the name to fips_stmt: update_stmt
update_stmt = update(flat_census).values(state_name = fips_stmt)

# Execute update_stmt: results
results = connection.execute(update_stmt)
</codebox>
			<codebox char_offset="318" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import delete, select
from sqlalchemy import delete, select

# Build a statement to empty the census table: stmt
stmt = delete(census)

# Execute the statement: results
results = connection.execute(stmt)
</codebox>
			<codebox char_offset="349" frame_height="520" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build a statement to count records using the sex column for Men ('M') age 36: stmt
stmt = select([func.count(census.columns.sex)]).where(
    and_(census.columns.sex == 'M',
         census.columns.age == 36)
)

# Execute the select statement and use the scalar() fetch method to save the record count
to_delete = connection.execute(stmt).scalar()

# Build a statement to delete records from the census table: stmt_del
stmt_del = delete(census)

# Append a where clause to target Men ('M') age 36
stmt_del = stmt_del.where(
    and_(census.columns.sex == &quot;M&quot;,
         census.columns.age == 36)
)

# Execute the statement: results
results = connection.execute(stmt_del)

# Print affected rowcount and to_delete record count, make sure they match
print(results.rowcount, to_delete)</codebox>
			<codebox char_offset="370" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Drop the state_fact table
state_fact.drop(engine)

# Check to see if state_fact exists
print(state_fact.exists(engine))

# Drop all tables
metadata.drop_all(engine)

# Check to see if census exists
print(census.exists(engine))
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Example Queries" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545391161.6" ts_lastsave="1545391265.5" unique_id="59">
			<rich_text>

Determining average age grouped by gender

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Determining percentage population which is of a certain gender grouped by state

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Determining population difference grouped by state

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>
</rich_text>
			<codebox char_offset="45" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import select
from sqlalchemy  import select

# Calculate weighted average age: stmt
stmt = select([census.columns.sex,
               (func.sum(census.columns.pop2008 * census.columns.age) /
                func.sum(census.columns.pop2008)).label('average_age')
               ])

# Group by sex
stmt = stmt.group_by(census.columns.sex)

# Execute the query and store the results: results
results = connection.execute(stmt).fetchall()

# Print the average age by sex
for sex, average_age in results:
    print(sex, average_age)</codebox>
			<codebox char_offset="129" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># import case, cast and Float from sqlalchemy
from sqlalchemy import case, cast, Float

# Build a query to calculate the percentage of females in 2000: stmt
stmt = select([census.columns.state,
    (func.sum(
        case([
            (census.columns.sex == 'F', census.columns.pop2000)
        ], else_=0)) /
     cast(func.sum(census.columns.pop2000), Float) * 100).label('percent_female')
])

# Group By state
stmt = stmt.group_by(census.columns.state)

# Execute the query and store the results: results
results = connection.execute(stmt).fetchall()

# Print the percentage
for result in results:
    print(result.state, result.percent_female)</codebox>
			<codebox char_offset="185" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Build query to return state name and population difference from 2008 to 2000
stmt = select([census.columns.state,
     (census.columns.pop2008-census.columns.pop2000).label('pop_change')
])

# Group by State
stmt = stmt.group_by(census.columns.state)

# Order by Population Change
stmt = stmt.order_by(desc('pop_change'))

# Limit to top 10
stmt = stmt.limit(10)

# Use connection to execute the statement and fetch all results
results = connection.execute(stmt).fetchall()

# Print the state and population change for each record
for result in results:
    print('{}:{}'.format(result.state, result.pop_change))</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="More Data Visualisation" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545391424.12" ts_lastsave="1545708455.58" unique_id="60">
		<rich_text>
Here's a </rich_text>
		<rich_text link="node 42">previous node on plotting</rich_text>
		<rich_text>

Here's </rich_text>
		<rich_text link="node 57">another node on plotting</rich_text>
		<rich_text> in the Statistical Thinking course.</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Plotting 1D data" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545391464.6" ts_lastsave="1545396477.96" unique_id="61">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Multiple plots</rich_text>
			<rich_text>

This plots things on the same axes

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Now we plot on two separate plots, but we have to specify where we want our plots to be
</rich_text>
			<rich_text family="monospace">plt.axes[xlo, ylo, xwidth, ywidth]</rich_text>
			<rich_text> - this is applicable to the subsequent plot statements until the next plt.axes statement. - the units have to be in the interval [0,1]
xlo and ylo are the coordinates of the lower left corner of the plot.
xwidth and ywidth are the lengths of the actual plot

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Using subplot for multiple plots

This is the better alternative to plt.axes. But these three plots still suffer from the problem of overcrowded x axes labels
</rich_text>
			<rich_text family="monospace">subplot(nrows, ncols, nsubplot)</rich_text>
			<rich_text> - nsubplot activates the subplot the subsequent statements are applicable to. Indexing starts from 1 (instead of 0). It goes in the sequence of row-wise beginning at the top left

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Setting axes limits</rich_text>
			<rich_text>

The range of axis values we want to display can be controlled separately using 
plt.xlim(&lt;tuple&gt;) and
plt.ylim(&lt;tuple&gt;)

where &lt;tuple&gt; is (start, end)

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Alternatively, all of that can be accomplished in one statement of plt.axis()

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The plotted figure can be saved to file using plt.savefig(&lt;filename&gt;)

</rich_text>
			<rich_text scale="h1">Legends</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

There are various other location codes. The label parameter in plt.plot will be used for the legend label.

Annotations

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


plt.annotate() allows us to point at one point and add a comment
xy: where the tip of the arrow is
xytext: where the text should be displaced wrt xy
arrowprops: dictionary of arrow properties

</rich_text>
			<rich_text scale="h1">Using a different stylesheet</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

ggplot is a popular one. ‘fivethirtyeight’ is used in the lecture notes.

We can check which styles are available using </rich_text>
			<rich_text family="monospace">plt.style.available</rich_text>
			<rich_text>.



</rich_text>
			<codebox char_offset="53" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Plot in blue the % of degrees awarded to women in the Physical Sciences
plt.plot(year, physical_sciences, color='blue')

# Plot in red the % of degrees awarded to women in Computer Science
plt.plot(year, computer_science, color='red')

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="438" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create plot axes for the first line plot
plt.axes([0.05,0.05,0.425,0.9])

# Plot in blue the % of degrees awarded to women in the Physical Sciences
plt.plot(year, physical_sciences, color='blue')

# Create plot axes for the second line plot
plt.axes([0.525,0.05,0.425,0.9])

# Plot in red the % of degrees awarded to women in Computer Science
plt.plot(year, computer_science, color='red')

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="813" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a figure with 1x2 subplot and make the left subplot active
plt.subplot(1,2,1)

# Plot in blue the % of degrees awarded to women in the Physical Sciences
plt.plot(year, physical_sciences, color='blue')
plt.title('Physical Sciences')

# Make the right subplot active in the current 1x2 subplot grid
plt.subplot(1,2,2)

# Plot in red the % of degrees awarded to women in Computer Science
plt.plot(year, computer_science, color='red')
plt.title('Computer Science')

# Use plt.tight_layout() to improve the spacing between subplots
plt.tight_layout()
plt.show()</codebox>
			<codebox char_offset="990" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set the x-axis range
plt.xlim((1990,2010))

# Set the y-axis range
plt.ylim((0,50))
</codebox>
			<codebox char_offset="1072" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set the x-axis and y-axis limits
plt.axis((1990,2010,0,50))
</codebox>
			<codebox char_offset="1155" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Specify the label 'Computer Science'
plt.plot(year, computer_science, color='red', label='Computer Science') 

# Specify the label 'Physical Sciences' 
plt.plot(year, physical_sciences, color='blue', label='Physical Sciences')

# Add a legend at the lower center
plt.legend(loc='lower center')

# Add axis labels and title
plt.xlabel('Year')
plt.ylabel('Enrollment (%)')
plt.title('Undergraduate enrollment of women')
plt.show()
</codebox>
			<codebox char_offset="1279" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Plot with legend as before
plt.plot(year, computer_science, color='red', label='Computer Science') 
plt.plot(year, physical_sciences, color='blue', label='Physical Sciences')
plt.legend(loc='lower right')

# Compute the maximum enrollment of women in Computer Science: cs_max
cs_max = max(computer_science)

# Calculate the year in which there was maximum enrollment of women in Computer Science: yr_max
yr_max = year[computer_science.argmax()]

# Add a black arrow annotation
plt.annotate('Maximum',xy=(yr_max,cs_max),xytext=(yr_max+5,cs_max+5), arrowprops=dict(facecolor='black'))

# Add axis labels and title
plt.xlabel('Year')
plt.ylabel('Enrollment (%)')
plt.title('Undergraduate enrollment of women')
plt.show()</codebox>
			<codebox char_offset="1506" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set the style to 'ggplot'
plt.style.use('ggplot')
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Plotting 2D data" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545397906.08" ts_lastsave="1545672170.27" unique_id="62">
			<rich_text>

</rich_text>
			<rich_text scale="h1">Meshgrids</rich_text>
			<rich_text>

These are rather like heatmaps

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text family="monospace">np.linspace(-2,2,41)</rich_text>
			<rich_text> generates an array of 41 equally spaced values from -2 to 2 

np.meshgrid(u,v) generates X and Y such that
X is 
</rich_text>
			<rich_text family="monospace">[[-2, -1.9, -1.8,...., 2],
[-2, -1.9, -1.8,...., 2],
[-2, -1.9, -1.8,...., 2],
...
[-2, -1.9, -1.8,...., 2]]
</rich_text>
			<rich_text>
There are 21 rows above

Y is 
</rich_text>
			<rich_text family="monospace">[[-1, -1, -1, ...., -1],
[-0.9, -0.9,-0.9, ...-0.9],
...
[1, 1, 1, ...., 1]]
</rich_text>
			<rich_text>
plt.pcolor(Z) plots from bottom right up. Also, it does not know about the values of X and Y, so we will see only integers on the x and y axes.
On the bottom most row of the plot, we will see the values of Z (in color) for 

(-2,-1) -&gt; (-1.9, -1) -&gt; ... -&gt; (2, -1)

Then the row above this row is for the (x,y) values

(-2, -0.9) -&gt; (-1.9, -0.9) -&gt; ... (2, -0.9)

Note that this the contrary to the way we traditionally view matrices!!!

We can change the color map using the cmap parameter. Color bars can be displayed for reference

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

cmap: ‘gray’, ‘autumn’, etc.
Color schemes include 'jet', 'coolwarm', 'magma' and 'viridis'; 'Greens', 'Blues', 'Reds', and 'Purples'; or the seasons

plt.axis('tight') to get rid of the whitespace

To include the correct X and Y axes labels (instead of integers), we simply include X and Y in the pcolor argument list

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Contour plots</rich_text>
			<rich_text>

Instead of pcolor, we can use contour plots to show equi-valued points. The usage is similar.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can specify the number of contours to draw

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The function contourf produces filled contour plots

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can improve the spacing as well using this

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">2D Histogram</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

In the example here, hp and mpg are on the horizontal and vertical axes respectively. There are 20 bins on the horizontal and 20 on the vertical. the range parameter specifies the range of values we are showing - [40, 235] on the horizontal and [8,48] on the vertical.

</rich_text>
			<rich_text scale="h1">Hexbin plots</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Here, gridsize is the equivalent of bins. extent is the equivalent of range, except that it is provided in one tuple instead of two separate ones.

</rich_text>
			<rich_text scale="h1">Images</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Creating a gray scale image by collapsing an axis

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Changing aspect ratio (ratio of physical height to width of each pixel on screen)

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Rescaling image intensity values. The image array can be treated like an array and we can perform arithmetic on it. This is a grayscale image.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

More on image processing (enhancing dynamic range)

This turns the 2D array into a 1D array suitable for histograms

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This plots the histogram for the image intensity values

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This switches the grid lines off

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This makes second set of axes that shares the x-axis. This enables us to plot the CDF on the same plot as the histogram. This will cause the left and right y axes to have different sets of values.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This plots the CDF. Note that we just need to add the cumulative parameter. Note that the lecture assigns plt.hist object to cdf.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


This is how to produce an equalised image that has a linear CDF. Most of the work is done by np.interp(x, xp, fp). This function maps the array x according to the function fp evaluated on the values xp.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This extracts the RGB values from a color image

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<codebox char_offset="45" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import numpy as np
import matplotlib.pyplot as plt

# Generate two 1-D arrays: u, v
u = np.linspace(-2,2, 41)
v = np.linspace(-1,1,21)

# Generate 2-D arrays from u and v: X, Y
X,Y = np.meshgrid(u,v)

# Compute Z based on X and Y
Z = np.sin(3*np.sqrt(X**2 + Y**2)) 

# Display the resulting image with pcolor()
plt.pcolor(Z)
plt.show()</codebox>
			<codebox char_offset="936" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.pcolor(A, cmap='Blues')
In [2]: plt.colorbar()</codebox>
			<codebox char_offset="1259" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.pcolor(X,Y,Z)</codebox>
			<codebox char_offset="1372" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.contour(Z)
plt.contour(X,Y,Z)</codebox>
			<codebox char_offset="1422" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.contour(X,Y,Z, 30)</codebox>
			<codebox char_offset="1478" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.contourf(X,Y,Z, 30)</codebox>
			<codebox char_offset="1528" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.tight_layout()</codebox>
			<codebox char_offset="1546" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a 2-D histogram
plt.hist2d(hp,mpg,bins=(20,20),range=((40,235),(8,48)))
</codebox>
			<codebox char_offset="1833" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a 2d histogram with hexagonal bins
plt.hexbin(hp,mpg,gridsize=(15,12),extent=(40,235,8,48))
</codebox>
			<codebox char_offset="1992" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Load the image into an array: img
img = plt.imread('480px-Astronaut-EVA.jpg')

# Print the shape of the image
print(img.shape)

# Display the image
plt.imshow(img)

# Hide the axes
plt.axis('off')
plt.show()</codebox>
			<codebox char_offset="2046" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Compute the sum of the red, green and blue channels: intensity
intensity = img.sum(axis=2)

# Print the shape of the intensity
print(intensity.shape)
# This gives (480,480) instead of (480,480,3)

# Display the intensity with a colormap of 'gray'
plt.imshow(intensity,cmap='gray')

# Hide the axes and show the figure
plt.axis('off')
plt.show()
</codebox>
			<codebox char_offset="2132" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Load the image into an array: img
img = plt.imread('480px-Astronaut-EVA.jpg')

# Specify the extent and aspect ratio of the top left subplot
plt.subplot(2,2,1)
plt.title('extent=(-1,1,-1,1),\naspect=0.5') 
plt.xticks([-1,0,1])
plt.yticks([-1,0,1])
plt.imshow(img, extent=(-1,1,-1,1), aspect=0.5)

# ... Other code for other subplots

# Improve spacing and display the figure
plt.tight_layout()
plt.show()</codebox>
			<codebox char_offset="2279" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Load the image into an array: image
image = plt.imread('640px-Unequalized_Hawkes_Bay_NZ.jpg')

# Extract minimum and maximum values from the image: pmin, pmax
pmin, pmax = image.min(), image.max()
print(&quot;The smallest &amp; largest pixel intensities are %d &amp; %d.&quot; % (pmin, pmax))

# Rescale the pixels: rescaled_image
rescaled_image = 256*(image - pmin) / (pmax - pmin)
print(&quot;The rescaled smallest &amp; largest pixel intensities are %.1f &amp; %.1f.&quot; % 
      (rescaled_image.min(), rescaled_image.max()))

# Display the original image in the top subplot
plt.subplot(2,1,1)
plt.title('original image')
plt.axis('off')
plt.imshow(image)

# Display the rescaled image in the bottom subplot
plt.subplot(2,1,2)
plt.title('rescaled image')
plt.axis('off')
plt.imshow(rescaled_image)

plt.show()
</codebox>
			<codebox char_offset="2399" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">pixels = image.flatten()</codebox>
			<codebox char_offset="2459" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.subplot(2,1,2)
plt.xlim((0,255))
plt.title('Normalized histogram')
plt.hist(pixels,bins=64, range=(0,256), normed=True, color='red',alpha=0.4)
</codebox>
			<codebox char_offset="2496" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.grid('off')</codebox>
			<codebox char_offset="2697" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Use plt.twinx() to overlay the CDF in the bottom subplot
plt.twinx()</codebox>
			<codebox char_offset="2831" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Display a cumulative histogram of the pixels
cdf = plt.hist(pixels, bins=64, range=(0,256),
               normed=True, cumulative=True,
               color='blue', alpha=0.4)
</codebox>
			<codebox char_offset="3039" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Load the image into an array: image
image = plt.imread('640px-Unequalized_Hawkes_Bay_NZ.jpg')

# Flatten the image into 1 dimension: pixels
pixels = image.flatten()

# Generate a cumulative histogram
cdf, bins, patches = plt.hist(pixels, bins=256, range=(0,256), normed=True, cumulative=True)
new_pixels = np.interp(pixels, bins[:-1], cdf*255)

# Reshape new_pixels as a 2-D array: new_image
new_image = new_pixels.reshape(image.shape)

# Display the new image with 'gray' color map
plt.subplot(2,1,1)
plt.title('Equalized image')
plt.axis('off')
plt.imshow(new_image,cmap='gray')

# Generate a histogram of the new pixels
plt.subplot(2,1,2)
pdf = plt.hist(new_pixels, bins=64, range=(0,256), normed=False,
               color='red', alpha=0.4)
plt.grid('off')

# Use plt.twinx() to overlay the CDF in the bottom subplot
plt.twinx()
plt.xlim((0,256))
plt.grid('off')

# Add title
plt.title('PDF &amp; CDF (equalized image)')

# Generate a cumulative histogram of the new pixels
cdf = plt.hist(new_pixels, bins=64, range=(0,256),
               cumulative=True, normed=True,
               color='blue', alpha=0.4)
plt.show()
</codebox>
			<codebox char_offset="3091" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Load the image into an array: image
image = plt.imread('hs-2004-32-b-small_web.jpg')

# Extract 2-D arrays of the RGB channels: red, blue, green
red, green, blue = image[:,:,0], image[:,:,1], image[:,:,2]</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Introduction to Seaborn" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545497371.62" ts_lastsave="1545672226.62" unique_id="63">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Plotting a simple linear regression line</rich_text>
			<rich_text>

Note that the 95% confidence range shading is included in the plot

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Additional parameters:
color: string, to specify the color of the plotted points

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can use hue to group points into different sets, with different regression lines.

Alternatively, this puts the different regression lines into distinct plots.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Residual plots</rich_text>
			<rich_text>

This  is a residual plot, with similar arguments as lmplot

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Higher order regression</rich_text>
			<rich_text>

regplot() is lower level compared to lmplot(). We can specify the order of the fitted line. Presumably this has to do with the polynomial order being used.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Strip Plots</rich_text>
			<rich_text>

Strip plots are like bee swarm plots, except that the default option plots the points in a vertical line, making it impossible to see multiple plots when they are plotted on top of each other
The workaround is to use the parameter jitter=True, which jitters the plotted points so that they don't overlap. The parameter size is for the size of the points.

Note that x has to be a categorical variable, and y the continuous variable. If x is omitted, there will be no categorisation shown.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Swarm plots</rich_text>
			<rich_text>

These can be generated like strip plots, and can have a horizontal orientation as well if orient='h' is specified. Just remember to swap the x and y arrays.
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Violin plots</rich_text>
			<rich_text>

This generates a basic violin plot categorised by cyl.
</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This removes inner annotation of the violin plots and uses a different color scheme

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Visualising multivariate data</rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h2">Joint plot</rich_text>
			<rich_text>

Souped up scatter plot

By default, the below shows: correlation coefficient + p value, and histograms for both axes

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Can use kernel density, or hex, or other plot options

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

kind takes other options

• </rich_text>
			<rich_text family="monospace">kind='scatter'</rich_text>
			<rich_text> uses a scatter plot of the data points
• </rich_text>
			<rich_text family="monospace">kind='reg'</rich_text>
			<rich_text> uses a regression plot (default order 1)
• </rich_text>
			<rich_text family="monospace">kind='resid'</rich_text>
			<rich_text> uses a residual plot
• </rich_text>
			<rich_text family="monospace">kind='kde'</rich_text>
			<rich_text> uses a kernel density estimate of the joint distribution
• </rich_text>
			<rich_text family="monospace">kind='hex'</rich_text>
			<rich_text> uses a hexbin plot of the joint distribution



</rich_text>
			<rich_text scale="h2">Pair Plot </rich_text>
			<rich_text>
Pair wise plots - takes pairs of variables and plots a series of scatter plots

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

The non-categorical columns are automatically identified and plotted. Just need to pass the data frame as an argument.

Pair plot also takes the kind parameter.
kind: ‘reg’ for regression; ‘scatter’ for scatter plots (default)
hue: to group by categorical variable by color


</rich_text>
			<rich_text scale="h2">Heat map</rich_text>
			<rich_text>

For covariance matrix between two variables. This is sometimes used to visualise covariance matrices.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

It appears that the type of cov_matrix here is pandas DataFrame. 


</rich_text>
			<codebox char_offset="111" frame_height="205" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns

# Plot a linear regression between 'weight' and 'hp'
sns.lmplot(x='weight', y='hp', data=auto)

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="196" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Plot a linear regression between 'weight' and 'hp', with a hue of 'origin' and palette of 'Set1'
sns.lmplot(x='weight',y='hp',data=auto,hue='origin',palette='Set1')</codebox>
			<codebox char_offset="363" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Plot linear regressions between 'weight' and 'hp' grouped row-wise by 'origin'
sns.lmplot(x='weight',y='hp',data=auto,row='origin')</codebox>
			<codebox char_offset="442" frame_height="85" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a green residual plot of the regression between 'hp' and 'mpg'
sns.residplot(x='hp', y='mpg', data=auto, color='green')</codebox>
			<codebox char_offset="628" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Plot in green a linear regression of order 2 between 'weight' and 'mpg'
sns.regplot(x='weight', y='mpg', data=auto, scatter=None, order=2, color='green',label='order 2')</codebox>
			<codebox char_offset="1135" frame_height="160" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Make a strip plot of 'hp' grouped by 'cyl'
plt.subplot(2,1,1)
sns.stripplot(x='cyl', y='hp', data=auto)

# Make the strip plot again using jitter and a smaller point size
plt.subplot(2,1,2)
sns.stripplot(x='cyl', y='hp', data=auto, jitter=True, size=3)</codebox>
			<codebox char_offset="1309" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a swarm plot of 'hp' grouped horizontally by 'cyl'  
plt.subplot(2,1,1)
sns.swarmplot(x='cyl',y='hp',data=auto)

# Generate a swarm plot of 'hp' grouped vertically by 'cyl' with a hue of 'origin'
plt.subplot(2,1,2)
sns.swarmplot(y='cyl',x='hp',data=auto,hue='origin',orient='h')

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="1382" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a violin plot of 'hp' grouped horizontally by 'cyl'
plt.subplot(2,1,1)
sns.violinplot(x='cyl', y='hp', data=auto)
</codebox>
			<codebox char_offset="1470" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">sns.violinplot(x='cyl', y='hp', data=auto,inner=None, color='lightgray')
</codebox>
			<codebox char_offset="1634" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a joint plot of 'hp' and 'mpg'
sns.jointplot('hp','mpg',data=auto)

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="1692" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Generate a joint plot of 'hp' and 'mpg' using a hexbin plot
sns.jointplot('hp','mpg',data=auto,kind='hex')

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="2089" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Plot the pairwise joint distributions from the DataFrame 
sns.pairplot(auto)

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="2481" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Visualize the covariance matrix using a heatmap
sns.heatmap(cov_matrix)

# Display the heatmap
plt.show()
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Plotting Time Series" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545664972.78" ts_lastsave="1545666857.57" unique_id="64">
			<rich_text>
We can specify the legend

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

So can the orientation of the x axis tick marks

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This demonstrates slicing and plotting an inset within another plot.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Here is the x axis relabelling example from the lectures

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<codebox char_offset="28" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Add a legend in the top left corner of the plot
plt.legend(loc='upper left')
</codebox>
			<codebox char_offset="80" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Specify the orientation of the xticks
plt.xticks(rotation=60)
</codebox>
			<codebox char_offset="153" frame_height="310" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Slice aapl from Nov. 2007 to Apr. 2008 inclusive: view
view = aapl['2007-11':'2008-04']

# Plot the entire series 
plt.plot(aapl)
plt.xticks(rotation=45)
plt.title('AAPL: 2001-2011')

# Specify the axes
plt.axes([0.25,0.5,0.35,0.35])

# Plot the sliced series in red using the current axes
plt.plot(view,color='red')
plt.xticks(rotation=45)
plt.title('2007/11-2008/04')
plt.show()</codebox>
			<codebox char_offset="214" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">dates = jan.index[::96] # Pick every 4th day

labels = dates.strftime('%b %d')

# ...
# And then we can specify the labels
plt.xticks(dates, labels, rotation=60)</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Bokeh Plotting" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545708455.59" ts_lastsave="1545754445.05" unique_id="65">
			<rich_text>
Bokeh does not require a server to run. No javascript is required

Basic scatter plots

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

figure() has other parameters
plot_width: for the width of the plot
tools: what tools to equip the plot with. Lecture example is 'pan,box_zoom'

circle() accepts arrays. Here the list of x and y coordinates of the centres of the circles are provided. Size can be provided as an array too. One parameter can be a single integer, while the others can be arrays.

There is such a thing as an x glyph. The marks will be ‘x’ instead of circles.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

For p.circle(), we can have other parameters, e.g.
color: for the color of the marks. This can take CSS color names, RGB values or hexadecimal strings
size: for the sizes of the marks
alpha: for transparency, specified as a value between 0 and 1
fill_color: the color of the fill

Other glyph markers are available

• asterisk() 
• circle()
• circle_cross()
• circle_x()
• cross()
• diamond()
• diamond_cross()
• inverted_triangle()
• square()
• square_cross()
• square_x()
• triangle()
• x()

Plotting Lines

If we are using datetime objects, we need to specify this in the figure() method.

The line() method takes the x and y values.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Patches

These are basically filled polygons.  We provide arrays with its x and y coordinates.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


Data from numpy arrays

The circle() method can take numpy arrays as inputs.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

It  can take columns from pandas dataframes directly

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Another way is to convert pandas data frames to ColumnDataSource objects 

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Adding customisations to glyphs

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

By specifying tools='box_select', we are enabling the selection tool, which will change the colors of data points when they are selected. Additional options can be specified in circle() method.

There is also the hover tool.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

And the color mapping tool, to color each category with a different color

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>



</rich_text>
			<rich_text weight="heavy">Q: In plotting scatter diagrams, the axes don't get restricted. How do we do that? E.g. in some cases, it doesn't make sense for percentage to be negative (population literacy)</rich_text>
			<rich_text>

</rich_text>
			<codebox char_offset="89" frame_height="370" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import figure from bokeh.plotting
from bokeh.plotting import figure

# Import output_file and show from bokeh.io
from bokeh.io import output_file, show

# Create the figure: p
p = figure(x_axis_label='fertility (children per woman)', y_axis_label='female_literacy (% population)')

# Add a circle glyph to the figure p
p.circle(fertility, female_literacy)

# Call the output_file() function and specify the name of the file
output_file('fert_lit.html')

# Display the plot
show(p)
</codebox>
			<codebox char_offset="533" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Add an x glyph to the figure p
p.x(fertility_africa,female_literacy_africa)</codebox>
			<codebox char_offset="1174" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a figure with x_axis_type=&quot;datetime&quot;: p
p = figure(x_axis_type = 'datetime', x_axis_label='Date', y_axis_label='US Dollars')

# Plot date along the x axis and price along the y axis
p.line(date, price)
</codebox>
			<codebox char_offset="1273" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a list of az_lons, co_lons, nm_lons and ut_lons: x
x = [az_lons, co_lons, nm_lons, ut_lons]

# Create a list of az_lats, co_lats, nm_lats and ut_lats: y
y = [az_lats, co_lats, nm_lats, ut_lats]

# Add patches to figure p with line_color=white for x and y
p.patches(x, y, line_color='white')
</codebox>
			<codebox char_offset="1355" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import numpy as np
import numpy as np

# Create array using np.linspace: x
x = np.linspace(0,5,100)

# Create array using np.cos: y
y = np.cos(x)

# Add circles at x and y
p.circle(x,y)
</codebox>
			<codebox char_offset="1412" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Plot mpg vs hp by color
p.circle(df['hp'],df['mpg'],color=df['color'],size=10)
</codebox>
			<codebox char_offset="1490" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Import the ColumnDataSource class from bokeh.plotting
from bokeh.plotting import ColumnDataSource

# Create a ColumnDataSource from df: source
source = ColumnDataSource(df)

# Add circle glyphs to the figure p
p.circle(x='Year',y='Time',source=source,color='color',size=8)
</codebox>
			<codebox char_offset="1526" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create a figure with the &quot;box_select&quot; tool: p
p = figure(x_axis_label=&quot;Year&quot;,y_axis_label=&quot;Time&quot;,tools=&quot;box_select&quot;)

# Add circle glyphs to the figure p with the selected and non-selected properties
p.circle(x=&quot;Year&quot;,y=&quot;Time&quot;,source=source,selection_color='red',nonselection_alpha=0.1)
</codebox>
			<codebox char_offset="1755" frame_height="295" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># import the HoverTool
from bokeh.models import HoverTool

# Add circle glyphs to figure p
p.circle(x, y, size=10,
         fill_color='grey', alpha=0.1, line_color=None,
         hover_fill_color='firebrick', hover_alpha=0.5,
         hover_line_color='white')

# Create a HoverTool: hover
hover = HoverTool(tooltips=None,mode='vline')

# Add the hover tool to the figure p
p.add_tools(hover)
</codebox>
			<codebox char_offset="1833" frame_height="340" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">#Import CategoricalColorMapper from bokeh.models
from bokeh.models import CategoricalColorMapper

# Convert df to a ColumnDataSource: source
source = ColumnDataSource(df)

# Make a CategoricalColorMapper object: color_mapper
color_mapper = CategoricalColorMapper(factors=['Europe', 'Asia', 'US'],
                                      palette=['red', 'green', 'blue'])

# Add a circle glyph to the figure p
p.circle('weight', 'mpg', source=source,
            color=dict(field='origin',transform=color_mapper),
            legend='origin')
</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Statistical Thinking" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545315637.85" ts_lastsave="1545754679.34" unique_id="56">
		<rich_text></rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Basic plotting" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545315653.63" ts_lastsave="1545754946.45" unique_id="57">
			<rich_text>John Tukey: Exploratory Data Analysis .... is the foundation stone

</rich_text>
			<rich_text scale="h1">Histograms</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

hist parameters
bins: set to number for number of bins, or list for user defined bin edges
normed: Boolean - to switch the y-axis to normalised probabilities instead of the default frequency
histtype: ‘step’ to see transparent stepped bars instead of the default opaque colors

There is a ‘square root rule’ for histograms - choose the number of bins to be square root of the number of data points

</rich_text>
			<rich_text scale="h1">Bee Swarm Plots</rich_text>
			<rich_text>

Bee Swarm plots add more information to histograms without adding more complexity. These can be done in seaborn.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Empirical Cumulative Distribution Function (ECDF)</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can plot multiple ECDF on the same plot simply by calling the plt.plot function multiple times for different data sets. The legend can be set as such

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Percentiles

Note that the percentile array need to be first converted to numpy array

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Box plots</rich_text>
			<rich_text>

We use box plots when the number of samples get too much to be represented in swarm plots

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h1">Scatter plots</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Calculating covariance</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


</rich_text>
			<rich_text scale="h1">Pearson correlation coefficient</rich_text>
			<rich_text>

Note that the Pearson correlation coefficient is simply the covariance divided by the respective standard deviations of x and y.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>


This sets a 2% margin on the x and y axes

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Previous node on plotting can be found </rich_text>
			<rich_text link="node 42">here</rich_text>
			<rich_text>.
</rich_text>
			<codebox char_offset="80" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
_ = plt.hist(df_swing['dem_share'])
_ = plt.xlabel('percent of vote for Obama')
_ = plt.ylabel('number of counties')
plt.show() </codebox>
			<codebox char_offset="613" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create bee swarm plot with Seaborn's default settings
_ = sns.swarmplot(x='species',y='petal length (cm)',data=df)

# Label the axes
_ = plt.xlabel('species')
_ = plt.ylabel('petal length (cm)')

# Show the plot
plt.show()</codebox>
			<codebox char_offset="667" frame_height="280" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def ecdf(data):
    &quot;&quot;&quot;Compute ECDF for a one-dimensional array of measurements.&quot;&quot;&quot;
    # Number of data points: n
    n = len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1, (n+1)) / n

    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)

# Generate plot
_ = plt.plot(x_vers,y_vers, marker = '.', linestyle = 'none')

# Label the axes
_ = plt.xlabel('versicolor petal length')
_ = plt.ylabel('ECDF')

plt.margins(0.02) # Keeps data off plot edges

# Display the plot
plt.show()
</codebox>
			<codebox char_offset="824" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')</codebox>
			<codebox char_offset="914" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Specify array of percentiles: percentiles
percentile = [2.5,25,50,75,97.5]

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length,np.array(percentile))
</codebox>
			<codebox char_offset="1020" frame_height="145" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Create box plot with Seaborn's default settings
_ =sns.boxplot(x='species',y='petal length (cm)', data=df)

# Label the axes
_ = plt.xlabel('species')
_ = plt.ylabel('petal length (cm)')
</codebox>
			<codebox char_offset="1038" frame_height="100" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Make a scatter plot
_ = plt.plot(versicolor_petal_length,versicolor_petal_width,marker='.',linestyle='none')
</codebox>
			<codebox char_offset="1066" frame_height="175" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">covariance_matrix = np.cov(versicolor_petal_length,versicolor_petal_width)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0,1]

# Note that covariance_matrix[1,0] is the same as the above value by symmetry
</codebox>
			<codebox char_offset="1233" frame_height="235" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">def pearson_r(x, y):
    &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot;
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x,y)

    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r
r = pearson_r(versicolor_petal_length,versicolor_petal_width)
</codebox>
			<codebox char_offset="1280" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Set the margins and label axes
plt.margins(0.02)
</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Probability Distributions" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545366287.72" ts_lastsave="1545672291.66" unique_id="58">
			<rich_text>
</rich_text>
			<rich_text scale="h1">Generating random numbers</rich_text>
			<rich_text>

Seeding and generating random numbers with np.random.random()

Note that an empty array should be initialised first for efficiency.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Alternatively the array can be generated if we pass the size = n parameter to np.random.random.

size can be passed to most PDF, e.g. binomial.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Probability mass function vs Probability Distributions?

Plotting PMF in matplotlib requires a bit of trickery. We will use the histogram method to plot instead

</rich_text>
			<rich_text scale="h1">Poisson process</rich_text>
			<rich_text>

Timing of one is independent of timing of the previous
e.g. births in a hospital, hits on a website in a given hour, meteor strikes, moleculer collisionss in a gas, aviations accidents, buses in Poissonville

Poisson is the limit to Binomial for low probability of success and high number of trials, i.e. for rare events

</rich_text>
			<rich_text scale="h1">The Normal Distribution</rich_text>
			<rich_text>

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Things that you think are normally distributed are actually not
4 standard deviation data points are very very rare - when data contains such points, be very careful


</rich_text>
			<rich_text scale="h1">Exponential distribution</rich_text>
			<rich_text>

The probability of interval (time) between Poisson processes

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>













</rich_text>
			<codebox char_offset="161" frame_height="190" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Seed the random number generator
np.random.seed(42)

# Initialize random numbers: random_numbers
random_numbers = np.empty(100000)

# Generate random numbers by looping over range(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()</codebox>
			<codebox char_offset="309" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">n_defaults = np.random.binomial(n=100,p=0.05,size=10000)
</codebox>
			<codebox char_offset="838" frame_height="130" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10
samples_std1 = np.random.normal(20, 1, size=100000)
samples_std3 = np.random.normal(20, 3, size=100000)
samples_std10 = np.random.normal(20, 10, size=100000)

# Make a legend, set limits and show plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
plt.ylim(-0.01, 0.42)</codebox>
			<codebox char_offset="1097" frame_height="55" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">t1 = np.random.exponential(tau1, size=size)</codebox>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="Regression, Confidence intervals and hypothesis testing" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1545754679.34" ts_lastsave="1545801962.18" unique_id="66">
			<rich_text>
Regression

To calculate linear regression coefficients, we use polyfit()

Note that here, illiteracy and fertility are the x and y data respectively.
a is the slope, b is the intercept
deg is for the degree of the polynomial we are fitting. For linear regressions, this is 1.

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

This, empty_like(), initialises and empty array just like the one provided

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

Bootstrapping

Definition

A </rich_text>
			<rich_text weight="heavy">bootstrap sample</rich_text>
			<rich_text> is an array of length n that was drawn from the original data with replacement.

</rich_text>
			<rich_text weight="heavy">Q: Should n have the same length as the original data set?</rich_text>
			<rich_text>

A </rich_text>
			<rich_text weight="heavy">bootstrap replicate</rich_text>
			<rich_text> is single value of a statistic computed from a bootstrap sample. For example, this could be the mean or standard deviation generated many times

We can use np.random.choice() on a numpy array to help 

The confidence interval of the statistic can be generate using np.percentile(). For example, if bs_means is the data set, the 95% confidence interval is given by np.percentile(bs_means, [2.5, 97.5]).

For pair-wise data

In the case for pair-wise data, e.g. election data such as vote share (%) for Obama vs total votes cast grouped by county. 

The arange function is useful for generating a list of values from 0, to ...

</rich_text>
			<rich_text justification="left"></rich_text>
			<rich_text>

We can generate slopes and intercepts and calculate 95% confidence intervals from this bootstrap data, for both the slope and intercept. 


Hypothesis testing

We can simulate a hypothesis my sample mixing.

In the lecture example, there we have a list of voting patterns for PA and OH, i.e. the Obama vote share for 





</rich_text>
			<codebox char_offset="279" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Perform a linear regression using np.polyfit(): a, b
a, b = np.polyfit(illiteracy,fertility,deg=1)
</codebox>
			<codebox char_offset="358" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Specify slopes to consider: a_vals
a_vals = np.linspace(0,0.1,200)

# Initialize sum of square of residuals: rss
rss = np.empty_like(a_vals)</codebox>
			<codebox char_offset="1196" frame_height="40" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">inds = np.arange(len(x))</codebox>
		</node>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="Datacamp Usage TIPS" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1543917644.25" ts_lastsave="1544680508.96" unique_id="40">
		<rich_text>
TIP: If columns are replaced by ... when using head or tail methods, we can view and configure the behaviours through the pandas get_options and set_option methods

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>

Simple sorting by a column

</rich_text>
		<rich_text justification="left"></rich_text>
		<rich_text>


what is the difference between isin and contains</rich_text>
		<codebox char_offset="166" frame_height="115" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True">
pd.get_option(&quot;display.max_columns&quot;)

pd.set_option(&quot;display.max_columns&quot;,10)
</codebox>
		<codebox char_offset="197" frame_height="70" frame_width="700" highlight_brackets="True" show_line_numbers="False" syntax_highlighting="python3" width_in_pixels="True"># Sort counted by the 'totals' column
counted = counted.sort_values(ascending=False, by=&quot;totals&quot;)
</codebox>
	</node>
</cherrytree>
